@book{abarbanelStatisticalPhysicsData2022,
  title = {The Statistical Physics of Data Assimilation and Machine Learning},
  author = {Abarbanel, H. D. I.},
  year = {2022},
  publisher = {Cambridge University Press},
  address = {Cambridge New York, NY Port Melbourne},
  abstract = {"Data assimilation is a hugely important mathematical technique, relevant in fields as diverse as geophysics, data science, and neuroscience. This modern book provides an authoritative treatment of the field as it relates to several scientific disciplines, with a particular emphasis on recent developments from machine learning and its role in the optimisation of data assimilation. Underlying theory from statistical physics, such as path integrals and Monte Carlo methods, are developed in the text as a basis for data assimilation, and the author then explores examples from current multidisciplinary research such as the modelling of shallow water systems, ocean dynamics, and neuronal dynamics in the avian brain. The theory of data assimilation and machine learning is introduced in an accessible and unified manner, and the book is suitable for undergraduate and graduate students from science and engineering without specialized experience of statistical physics"--},
  isbn = {978-1-316-51963-9},
  langid = {english},
  language = {eng}
}

@book{abarbanelStatisticalPhysicsData2022a,
  title = {The {{Statistical Physics}} of {{Data Assimilation}} and {{Machine Learning}}},
  author = {Abarbanel, Henry D. I.},
  year = {2022},
  month = jan,
  edition = {1},
  publisher = {Cambridge University Press},
  urldate = {2024-09-20},
  abstract = {Data assimilation is a hugely important mathematical technique, relevant in fields as diverse as geophysics, data science, and neuroscience. This modern book provides an authoritative treatment of the field as it relates to several scientific disciplines, with a particular emphasis on recent developments from machine learning and its role in the optimisation of data assimilation. Underlying theory from statistical physics, such as path integrals and Monte Carlo methods, are developed in the text as a basis for data assimilation, and the author then explores examples from current multidisciplinary research such as the modelling of shallow water systems, ocean dynamics, and neuronal dynamics in the avian brain. The theory of data assimilation and machine learning is introduced in an accessible and unified manner, and the book is suitable for undergraduate and graduate students from science and engineering without specialized experience of statistical physics.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-1-00-902484-6 978-1-316-51963-9}
}

@misc{abayMitigatingBiasFederated2020,
  title = {Mitigating {{Bias}} in {{Federated Learning}}},
  author = {Abay, Annie and Zhou, Yi and Baracaldo, Nathalie and Rajamoni, Shashank and Chuba, Ebube and Ludwig, Heiko},
  year = {2020},
  month = dec,
  urldate = {2024-04-08},
  abstract = {As methods to create discrimination-aware models develop, they focus on centralized ML, leaving federated learning (FL) unexplored. FL is a rising approach for collaborative ML, in which an aggregator orchestrates multiple parties to train a global model without sharing their training data. In this paper, we discuss causes of bias in FL and propose three pre-processing and in-processing methods to mitigate bias, without compromising data privacy, a key FL requirement. As data heterogeneity among parties is one of the challenging characteristics of FL, we conduct experiments over several data distributions to analyze their effects on model performance, fairness metrics, and bias learning patterns. We conduct a comprehensive analysis of our proposed techniques, the results demonstrating that these methods are effective even when parties have skewed data distributions or as little as 20\% of parties employ the methods.},
  langid = {english},
  language = {en}
}

@article{abdelhackModulationLayerIncrease2023,
  title = {A {{Modulation Layer}} to {{Increase Neural Network Robustness Against Data Quality Issues}}},
  author = {Abdelhack, Mohamed and Zhang, Jiaming and Tripathi, Sandhya and Fritz, Bradley A. and Felsky, Daniel and Avidan, Michael and Chen, Yixin and King, Christopher Ryan},
  year = {2023},
  month = jan,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2024-05-11},
  abstract = {Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high-quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low-quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of additional input. This is inspired by neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against data quality degradation, including additional missingness. These models are superior to imputation as they save on training time by entirely skipping the imputation process and further allow the introduction of other data quality measures that imputation cannot handle. Our results suggest that explicitly accounting for reduced information quality with a modulating fully connected layer can enable the deployment of artificial intelligence systems in real-time applications.},
  langid = {english},
  language = {en}
}

@book{abhishekMachineLearningImbalanced2023,
  title = {Machine Learning for Imbalanced Data: Tackle Imbalanced Datasets Using Machine Learning and Deep Learning Techniques},
  shorttitle = {Machine Learning for Imbalanced Data},
  author = {Abhishek, Kumar and Abdelaziz, Mounir},
  year = {2023},
  edition = {[First edition]},
  publisher = {Packt Publishing Ltd.},
  address = {Birmingham, UK},
  abstract = {As machine learning practitioners, we often encounter imbalanced datasets in which one class has considerably fewer instances than the other. Many machine learning algorithms assume an equilibrium between majority and minority classes, leading to suboptimal performance on imbalanced data. This comprehensive guide helps you address this class imbalance to significantly improve model performance. Machine Learning for Imbalanced Data begins by introducing you to the challenges posed by imbalanced datasets and the importance of addressing these issues. It then guides you through techniques that enhance the performance of classical machine learning models when using imbalanced data, including various sampling and cost-sensitive learning methods. As you progress, you'll delve into similar and more advanced techniques for deep learning models, employing PyTorch as the primary framework. Throughout the book, hands-on examples will provide working and reproducible code that'll demonstrate the practical implementation of each technique. By the end of this book, you'll be adept at identifying and addressing class imbalances and confidently applying various techniques, including sampling, cost-sensitive techniques, and threshold adjustment, while using traditional machine learning or deep learning models},
  langid = {english},
  language = {eng},
  annotation = {OCLC: 1411849981}
}

@article{abrehaFederatedLearningEdge2022,
  title = {Federated {{Learning}} in {{Edge Computing}}: {{A Systematic Survey}}},
  shorttitle = {Federated {{Learning}} in {{Edge Computing}}},
  author = {Abreha, Haftay Gebreslasie and Hayajneh, Mohammad and Serhani, Mohamed Adel},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {2},
  pages = {450},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  urldate = {2023-10-02},
  abstract = {Edge Computing (EC) is a new architecture that extends Cloud Computing (CC) services closer to data sources. EC combined with Deep Learning (DL) is a promising technology and is widely used in several applications. However, in conventional DL architectures with EC enabled, data producers must frequently send and share data with third parties, edge or cloud servers, to train their models. This architecture is often impractical due to the high bandwidth requirements, legalization, and privacy vulnerabilities. The Federated Learning (FL) concept has recently emerged as a promising solution for mitigating the problems of unwanted bandwidth loss, data privacy, and legalization. FL can co-train models across distributed clients, such as mobile phones, automobiles, hospitals, and more, through a centralized server, while maintaining data localization. FL can therefore be viewed as a stimulating factor in the EC paradigm as it enables collaborative learning and model optimization. Although the existing surveys have taken into account applications of FL in EC environments, there has not been any systematic survey discussing FL implementation and challenges in the EC paradigm. This paper aims to provide a systematic survey of the literature on the implementation of FL in EC environments with a taxonomy to identify advanced solutions and other open problems. In this survey, we review the fundamentals of EC and FL, then we review the existing related works in FL in EC. Furthermore, we describe the protocols, architecture, framework, and hardware requirements for FL implementation in the EC environment. Moreover, we discuss the applications, challenges, and related existing solutions in the edge FL. Finally, we detail two relevant case studies of applying FL in EC, and we identify open issues and potential directions for future research. We believe this survey will help researchers better understand the connection between FL and EC enabling technologies and concepts.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {data privacy,data security,edge AI,edge computing,federated learning,intelligent edge}
}

@book{abu-mostafaLearningDataShort2012,
  title = {Learning from Data: A Short Course},
  shorttitle = {Learning from Data},
  author = {{Abu-Mostafa}, Yaser S. and {Magdon-Ismail}, Malik and Lin, Hsuan-Tien},
  year = {2012},
  publisher = {AMLbook.com},
  address = {S.l.},
  isbn = {978-1-60049-006-4},
  langid = {english},
  language = {eng}
}

@article{abyaneUnderstandingQualityChallenges2023,
  title = {Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness},
  shorttitle = {Towards Understanding Quality Challenges of the Federated Learning for Neural Networks},
  author = {Abyane, Amin Eslami and Zhu, Derui and Souza, Roberto and Ma, Lei and Hemmati, Hadi},
  year = {2023},
  month = feb,
  journal = {Empirical Software Engineering},
  volume = {28},
  number = {2},
  pages = {44},
  issn = {1573-7616},
  urldate = {2024-03-19},
  abstract = {Federated learning (FL) is a distributed learning paradigm that preserves users' data privacy while leveraging the entire dataset of all participants. In FL, multiple models are trained independently on the clients and aggregated centrally to update a global model in an iterative process. Although this approach is excellent at preserving privacy, FL still suffers from quality issues such as attacks or byzantine faults. Recent attempts have been made to address such quality challenges on the robust aggregation techniques for FL. However, the effectiveness of state-of-the-art (SOTA) robust FL techniques is still unclear and lacks a comprehensive study. Therefore, to better understand the current quality status and challenges of these SOTA FL techniques in the presence of attacks and faults, we perform a large-scale empirical study to investigate the SOTA FL's quality from multiple angles of attacks, simulated faults (via mutation operators), and aggregation (defense) methods. In particular, we study FL's performance on the image classification tasks and use Deep Neural Networks as our model type. Furthermore, we perform our study on two generic image datasets and one real-world federated medical image dataset. We also systematically investigate the effect of the proportion of affected clients and the dataset distribution factors on the robustness of FL. After a large-scale analysis with 496 configurations, we find that most mutators on each user have a negligible effect on the final model in the generic datasets, and only one of them is effective in the medical dataset. Furthermore, we show that model poisoning attacks are more effective than data poisoning attacks. Moreover, choosing the most robust FL aggregator depends on the attacks and datasets. Finally, we illustrate that a simple ensemble of aggregators achieves a more robust solution than any single aggregator and is the best choice in 75\% of the cases. The data that support the findings of this study are available in our repository at https://github.com/aminesi/federated.},
  langid = {english},
  language = {en},
  keywords = {Byzantine attacks,Defense methods,Federated learning,Mutation testing,Robustness}
}

@article{adeoyeDatacentricArtificialIntelligence2023,
  title = {Data-Centric Artificial Intelligence in Oncology: A Systematic Review Assessing Data Quality in Machine Learning Models for Head and Neck Cancer},
  shorttitle = {Data-Centric Artificial Intelligence in Oncology},
  author = {Adeoye, John and Hui, Liuling and Su, Yu-Xiong},
  year = {2023},
  month = mar,
  journal = {Journal of Big Data},
  volume = {10},
  number = {1},
  pages = {28},
  issn = {2196-1115},
  urldate = {2023-11-12},
  abstract = {Machine learning models have been increasingly considered to model head and neck cancer outcomes for improved screening, diagnosis, treatment, and prognostication of the disease. As the concept of data-centric artificial intelligence is still incipient in healthcare systems, little is known about the data quality of the models proposed for clinical utility. This is important as it supports the generalizability of the models and data standardization. Therefore, this study overviews the quality of structured and unstructured data used for machine learning model construction in head and neck cancer. Relevant studies reporting on the use of machine learning models based on structured and unstructured custom datasets between January 2016 and June 2022 were sourced from PubMed, EMBASE, Scopus, and Web of Science electronic databases. Prediction model Risk of Bias Assessment (PROBAST) tool was used to assess the quality of individual studies before comprehensive data quality parameters were assessed according to the type of dataset used for model construction. A total of 159 studies were included in the review; 106 utilized structured datasets while 53 utilized unstructured datasets. Data quality assessments were deliberately performed for 14.2\% of structured datasets and 11.3\% of unstructured datasets~before model construction. Class imbalance and data fairness were the most common limitations in data quality for both types of datasets while outlier detection and lack of representative outcome classes were common in structured and unstructured datasets respectively. Furthermore, this review found that class imbalance reduced the~discriminatory performance for models based on structured datasets while higher image resolution and good class overlap resulted in better model performance using unstructured datasets~during internal validation. Overall, data quality was infrequently assessed before the construction of ML models in head and neck cancer irrespective of the use of structured or unstructured datasets. To improve model generalizability, the assessments discussed in this study should be introduced during model construction to achieve data-centric intelligent~systems for head and neck cancer management.},
  keywords = {Artificial intelligence,Data quality,Data-centric AI,Head and neck cancer,Machine learning,Review}
}

@article{advaniStatisticalMechanicsComplex2013,
  title = {Statistical Mechanics of Complex Neural Systems and High Dimensional Data},
  author = {Advani, Madhu and Lahiri, Subhaneil and Ganguli, Surya},
  year = {2013},
  month = mar,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2013},
  number = {03},
  pages = {P03014},
  publisher = {{IOP Publishing and SISSA}},
  issn = {1742-5468},
  urldate = {2024-05-18},
  abstract = {Recent experimental advances in neuroscience have opened new vistas into the immense complexity of neuronal networks. This proliferation of data challenges us on two parallel fronts. First, how can we form adequate theoretical frameworks for understanding how dynamical network processes cooperate across widely disparate spatiotemporal scales to solve important computational problems? Second, how can we extract meaningful models of neuronal systems from high dimensional datasets? To aid in these challenges, we give a pedagogical review of a collection of ideas and theoretical methods arising at the intersection of statistical physics, computer science and neurobiology. We introduce the interrelated replica and cavity methods, which originated in statistical physics as powerful ways to quantitatively analyze large highly heterogeneous systems of many interacting degrees of freedom. We also introduce the closely related notion of message passing in graphical models, which originated in computer science as a distributed algorithm capable of solving large inference and optimization problems involving many coupled variables. We then show how both the statistical physics and computer science perspectives can be applied in a wide diversity of contexts to problems arising in theoretical neuroscience and data analysis. Along the way we discuss spin glasses, learning theory, illusions of structure in noise, random matrices, dimensionality reduction and compressed sensing, all within the unified formalism of the replica method. Moreover, we review recent conceptual connections between message passing in graphical models, and neural computation and learning. Overall, these ideas illustrate how statistical physics and computer science might provide a lens through which we can uncover emergent computational functions buried deep within the dynamical complexities of neuronal networks.},
  langid = {english},
  language = {en}
}

@inproceedings{agarwalaDeepEquilibriumNetworks2022,
  title = {Deep Equilibrium Networks Are Sensitive to Initialization Statistics},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Agarwala, Atish and Schoenholz, Samuel S.},
  year = {2022},
  month = jun,
  pages = {136--160},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-25},
  abstract = {Deep equilibrium networks (DEQs) are a promising way to construct models which trade off memory for compute. However, theoretical understanding of these models is still lacking compared to traditional networks, in part because of the repeated application of a single set of weights. We show that DEQs are sensitive to the higher order statistics of the matrix families from which they are initialized. In particular, initializing with orthogonal or symmetric matrices allows for greater stability in training. This gives us a practical prescription for initializations which allow for training with a broader range of initial weight scales.},
  langid = {english},
  language = {en}
}

@inproceedings{agarwalFairRegressionQuantitative2019,
  title = {Fair {{Regression}}: {{Quantitative Definitions}} and {{Reduction-Based Algorithms}}},
  shorttitle = {Fair {{Regression}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Agarwal, Alekh and Dudik, Miroslav and Wu, Zhiwei Steven},
  year = {2019},
  month = may,
  pages = {120--129},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-01-06},
  abstract = {In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness--accuracy frontiers on several standard datasets.},
  langid = {english},
  language = {en}
}

@inproceedings{agarwalReductionsApproachFair2018,
  title = {A {{Reductions Approach}} to {{Fair Classification}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
  year = {2018},
  month = jul,
  pages = {60--69},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-01-06},
  abstract = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.},
  langid = {english},
  language = {en}
}

@inproceedings{agarwalSkellamMechanismDifferentially2021,
  title = {The {{Skellam Mechanism}} for {{Differentially Private Federated Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Agarwal, Naman and Kairouz, Peter and Liu, Ziyu},
  year = {2021},
  volume = {34},
  pages = {5052--5064},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-09-28},
  abstract = {We introduce the multi-dimensional Skellam mechanism, a discrete differential privacy mechanism based on the difference of two independent Poisson random variables. To quantify its privacy guarantees, we analyze the privacy loss distribution via a numerical evaluation and provide a sharp bound on the R{\'e}nyi divergence between two shifted Skellam distributions. While useful in both centralized and distributed privacy applications, we investigate how it can be applied in the context of federated learning with secure aggregation under communication constraints. Our theoretical findings and extensive experimental evaluations demonstrate that the Skellam mechanism provides the same privacy-accuracy trade-offs as the continuous Gaussian mechanism, even when the precision is low. More importantly, Skellam is closed under summation and sampling from it only requires sampling from a Poisson distribution -- an efficient routine that ships with all machine learning and data analysis software packages. These features, along with its discrete nature and competitive privacy-accuracy trade-offs, make it an attractive practical alternative to the newly introduced discrete Gaussian mechanism.}
}

@inproceedings{ahujaInvariantRiskMinimization2020,
  title = {Invariant {{Risk Minimization Games}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Ahuja, Kartik and Shanmugam, Karthikeyan and Varshney, Kush and Dhurandhar, Amit},
  year = {2020},
  month = nov,
  pages = {145--155},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-12-03},
  abstract = {The standard risk minimization paradigm of machine learning is brittle when operating in environments whose test distributions are different from the training distribution due to spurious correlations. Training on data from many environments and finding invariant predictors reduces the effect of spurious features by concentrating models on features that have a causal relationship with the outcome. In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a simple training algorithm that uses best response dynamics and, in our experiments, yields similar or better empirical accuracy with much lower variance than the challenging bi-level optimization problem of Arjovsky et al. (2019). One key theoretical contribution is showing that the set of Nash equilibria for the proposed game are equivalent to the set of invariant predictors for any finite number of environments, even with nonlinear classifiers and transformations. As a result, our method also retains the generalization guarantees to a large set of environments shown in Arjovsky et al. (2019). The proposed algorithm adds to the collection of successful game-theoretic machine learning algorithms such as generative adversarial networks.},
  langid = {english},
  language = {en}
}

@article{alabadlaSystematicReviewUsing2022,
  title = {Systematic {{Review}} of {{Using Machine Learning}} in {{Imputing Missing Values}}},
  author = {Alabadla, Mustafa and Sidi, Fatimah and Ishak, Iskandar and Ibrahim, Hamidah and Affendey, Lilly Suriani and Ani, Zafienas Che and Jabar, Marzanah A. and Bukar, Umar Ali and Devaraj, Navin Kumar and Muda, Ahmad Sobri and Tharek, Anas and Omar, Noritah and Jaya, M. Izham Mohd},
  year = {2022},
  month = jan,
  journal = {IEEE Access},
  volume = {10},
  pages = {44483--44502},
  publisher = {IEEE},
  issn = {2169-3536},
  urldate = {2023-12-01},
  abstract = {Missing data are a universal data quality problem in many domains, leading to misleading analysis and inaccurate decisions. Much research has been done to...},
  langid = {english},
  language = {en}
}

@article{alamaStudyTopologyGeometry,
  title = {On the {{Study}} of the {{Topology}}, {{Geometry}}, and {{Second Order Propertes}} of the {{Loss Surface}} of {{Deep Neural Networks}}},
  author = {Alama, Yvonne B and Brandenberger, Anna},
  langid = {english},
  language = {en}
}

@inproceedings{albertAddressingOutofDistributionLabel2022,
  title = {Addressing {{Out-of-Distribution Label Noise}} in {{Webly-Labelled Data}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Albert, Paul and Ortego, Diego and Arazo, Eric and O'Connor, Noel E. and McGuinness, Kevin},
  year = {2022},
  pages = {392--401},
  urldate = {2024-01-24},
  langid = {english},
  language = {en}
}

@article{alganImageClassificationDeep2021,
  title = {Image Classification with Deep Learning in the Presence of Noisy Labels: {{A}} Survey},
  shorttitle = {Image Classification with Deep Learning in the Presence of Noisy Labels},
  author = {Algan, G{\"o}rkem and Ulusoy, Ilkay},
  year = {2021},
  month = mar,
  journal = {Knowledge-Based Systems},
  volume = {215},
  pages = {106771},
  issn = {0950-7051},
  urldate = {2024-01-10},
  abstract = {Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.},
  keywords = {Classification with noise,Deep learning,Label noise,Noise robust,Noise tolerant}
}

@misc{alganLabelNoiseTypes2020,
  title = {Label {{Noise Types}} and {{Their Effects}} on {{Deep Learning}}},
  author = {Algan, G{\"o}rkem and Ulusoy, {\.I}lkay},
  year = {2020},
  month = mar,
  number = {arXiv:2003.10471},
  eprint = {2003.10471},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-24},
  abstract = {The recent success of deep learning is mostly due to the availability of big datasets with clean annotations. However, gathering a cleanly annotated dataset is not always feasible due to practical challenges. As a result, label noise is a common problem in datasets, and numerous methods to train deep neural networks in the presence of noisy labels are proposed in the literature. These methods commonly use benchmark datasets with synthetic label noise on the training set. However, there are multiple types of label noise, and each of them has its own characteristic impact on learning. Since each work generates a different kind of label noise, it is problematic to test and compare those algorithms in the literature fairly. In this work, we provide a detailed analysis of the effects of different kinds of label noise on learning. Moreover, we propose a generic framework to generate feature-dependent label noise, which we show to be the most challenging case for learning. Our proposed method aims to emphasize similarities among data instances by sparsely distributing them in the feature domain. By this approach, samples that are more likely to be mislabeled are detected from their softmax probabilities, and their labels are flipped to the corresponding class. The proposed method can be applied to any clean dataset to synthesize feature-dependent noisy labels. For the ease of other researchers to test their algorithms with noisy labels, we share corrupted labels for the most commonly used benchmark datasets. Our code and generated noisy synthetic labels are available online.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{amariInformationGeometryIts2016,
  title = {Information Geometry and Its Applications},
  author = {Amari, Shun'ichi},
  year = {2016},
  series = {Applied Mathematical Sciences},
  number = {volume 194},
  publisher = {Springer},
  address = {Japan},
  isbn = {978-4-431-55977-1},
  lccn = {QA641 .A5974 2016},
  keywords = {Geometrie differentielle,Geometry Differential,Information Theorie de l',Information theory,Information theory in mathematics,Mathematical statistics,Mathematics,Statistique mathematique},
  annotation = {OCLC: ocn930997330}
}

@book{amariMethodsInformationGeometry2000,
  title = {Methods of Information Geometry},
  author = {Amari, Shun'ichi and Nagaoka, Hiroshi and Harada, Daishi},
  year = {2000},
  series = {Translations of Mathematical Monographs},
  number = {191},
  publisher = {American mathematical society},
  address = {Providence (R. I.)},
  isbn = {978-0-8218-0531-2},
  langid = {english},
  language = {eng},
  lccn = {519.5}
}

@article{aminEdgeIntelligenceInternet2021,
  title = {Edge {{Intelligence}} and {{Internet}} of {{Things}} in {{Healthcare}}: {{A Survey}}},
  shorttitle = {Edge {{Intelligence}} and {{Internet}} of {{Things}} in {{Healthcare}}},
  author = {Amin, Syed Umar and Hossain, M. Shamim},
  year = {2021},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {9},
  pages = {45--59},
  issn = {2169-3536},
  abstract = {With the advent of new technologies and the fast pace of human life, patients today require a sophisticated and advanced smart healthcare framework that is tailored to suit their individual health requirements. Along with 5G and state-of-the-art smart Internet of Things (IoT) sensors, edge computing provides intelligent, real-time healthcare solutions that satisfy energy consumption and latency criteria. Earlier surveys on smart healthcare systems were centered on cloud and fog computing architectures, security, and authentication, and the types of sensors and devices used in edge computing frameworks. They did not focus on the healthcare IoT applications deployed within edge computing architectures. The first purpose of this study is to analyze the existing and evolving edge computing architectures and techniques for smart healthcare and recognize the demands and challenges of different application scenarios. We examine edge intelligence that targets health data classification with the tracking and identification of vital signs using state-of-the-art deep learning techniques. This study also presents a comprehensive analysis of the use of cutting-edge artificial intelligence-based classification and prediction techniques employed for edge intelligence. Even with its many advantages, edge intelligence poses challenges related to computational complexity and security. To offer a higher quality of life to patients, potential research recommendations for improving edge computing services for healthcare are identified in this study. This study also offers a brief overview of the general usage of IoT solutions in edge platforms for medical treatment and healthcare.},
  keywords = {artificial intelligence,Cloud computing,Computer architecture,edge computing,Edge computing,fog computing,Intelligent sensors,Internet of Things,Medical services,Sensors,smart healthcare}
}

@inproceedings{andriushchenkoModernLookRelationship2023,
  title = {A {{Modern Look}} at the {{Relationship}} between {{Sharpness}} and {{Generalization}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Andriushchenko, Maksym and Croce, Francesco and M{\"u}ller, Maximilian and Hein, Matthias and Flammarion, Nicolas},
  year = {2023},
  month = jul,
  pages = {840--902},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-12},
  abstract = {Sharpness of minima is a promising quantity that can correlate with generalization in deep networks and, when optimized during training, can improve generalization. However, standard sharpness is not invariant under reparametrizations of neural networks, and, to fix this, reparametrization-invariant sharpness definitions have been proposed, most prominently adaptive sharpness (Kwon et al., 2021). But does it really capture generalization in modern practical settings? We comprehensively explore this question in a detailed study of various definitions of adaptive sharpness in settings ranging from training from scratch on ImageNet and CIFAR-10 to fine-tuning CLIP on ImageNet and BERT on MNLI. We focus mostly on transformers for which little is known in terms of sharpness despite their widespread usage. Overall, we observe that sharpness does not correlate well with generalization but rather with some training parameters like the learning rate that can be positively or negatively correlated with generalization depending on the setup. Interestingly, in multiple cases, we observe a consistent negative correlation of sharpness with OOD generalization implying that sharper minima can generalize better. Finally, we illustrate on a simple model that the right sharpness measure is highly data-dependent, and that we do not understand well this aspect for realistic data distributions.},
  langid = {english},
  language = {en}
}

@inproceedings{andriushchenkoUnderstandingSharpnessAwareMinimization2022,
  title = {Towards {{Understanding Sharpness-Aware Minimization}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Andriushchenko, Maksym and Flammarion, Nicolas},
  year = {2022},
  month = jun,
  pages = {639--668},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-12},
  abstract = {Sharpness-Aware Minimization (SAM) is a recent training method that relies on worst-case weight perturbations which significantly improves generalization in various settings. We argue that the existing justifications for the success of SAM which are based on a PAC-Bayes generalization bound and the idea of convergence to flat minima are incomplete. Moreover, there are no explanations for the success of using m-sharpness in SAM which has been shown as essential for generalization. To better understand this aspect of SAM, we theoretically analyze its implicit bias for diagonal linear networks. We prove that SAM always chooses a solution that enjoys better generalization properties than standard gradient descent for a certain class of problems, and this effect is amplified by using m-sharpness. We further study the properties of the implicit bias on non-linear networks empirically, where we show that fine-tuning a standard model with SAM can lead to significant generalization improvements. Finally, we provide convergence results of SAM for non-convex objectives when used with stochastic gradients. We illustrate these results empirically for deep networks and discuss their relation to the generalization behavior of SAM. The code of our experiments is available at https://github.com/tml-epfl/understanding-sam.},
  langid = {english},
  language = {en}
}

@misc{angRobustFederatedLearning2019,
  title = {Robust {{Federated Learning}} with {{Noisy Communication}}},
  author = {Ang, Fan and Chen, Li and Zhao, Nan and Chen, Yunfei and Wang, Weidong and Yu, F. Richard},
  year = {2019},
  month = nov,
  number = {arXiv:1911.00251},
  eprint = {1911.00251},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-12},
  abstract = {Federated learning is a communication-efficient training process that alternates between local training at the edge devices and averaging the updated local model at the central server. Nevertheless, it is impractical to achieve a perfect acquisition of the local models in wireless communication due to noise, which also brings serious effects on federated learning. To tackle this challenge, we propose a robust design for federated learning to alleviate the effects of noise in this paper. Considering noise in the two aforementioned steps, we first formulate the training problem as a parallel optimization for each node under the expectation-based model and the worst-case model. Due to the non-convexity of the problem, a regularization for the loss function approximation method is proposed to make it tractable. Regarding the worst-case model, we develop a feasible training scheme which utilizes the sampling-based successive convex approximation algorithm to tackle the unavailable maxima or minima noise condition and the non-convex issue of the objective function. Furthermore, the convergence rates of both new designs are analyzed from a theoretical point of view. Finally, the improvement of prediction accuracy and the reduction of loss function are demonstrated via simulations for the proposed designs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{arjovskyInvariantRiskMinimization2020,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2020},
  month = mar,
  number = {arXiv:1907.02893},
  eprint = {1907.02893},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-03},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{aroraStudyingRoleData2021,
  title = {Studying the {{Role}} of {{Data Quality}} on {{Statistical}} and {{Neural Machine Translation}}},
  booktitle = {2021 10th {{IEEE International Conference}} on {{Communication Systems}} and {{Network Technologies}} ({{CSNT}})},
  author = {Arora, Karunesh Kumar and Tomar, Geetam S and Agrawal, Shyam S},
  year = {2021},
  month = jun,
  pages = {199--204},
  issn = {2329-7182},
  urldate = {2024-02-25},
  abstract = {Statistical and Neural Machine translation techniques are based on the parallel data used for training models. The general belief is that more training data would result in better models. We studied the available corpora and ambient noises present in them. It revealed that the available data is highly noisy. The paper describes various types of noises present in there and how these are identified. Different types of noise filters are developed and normalization processes have been applied on the corpora. Statistical and neural machine translation models are trained to study the impact of cleaning of noisy data. We performed experiments with noisy data and with cleaned data after discarding noisy data from the training corpus. Standard test set WMT-14 has been used for performing evaluation. The quality of machine translation has been measured through BLEU scores. It was observed that even after discarding a significant volume of noisy data, the models without noisy data performed better than the corpus containing noises. It proves that quality of data has significant impact and mere having huge piles of uncleaned data in not a good choice. The test case presented here is for English-Hindi language pair. It also shows a path that for low resource language pairs, paying attention to the quality of data would bring returns in form of better translation performance. As the noises discussed in paper are general in nature, the findings should be true for any other Indian language pair also, due to inherent similarity among Indian languages.},
  keywords = {Communication systems,component,Conferences,Corpus quality,Data integrity,Data models,Neural Machine Translation,Solid modeling,Statistical Machine Translation,Training,Training data}
}

@misc{azadiAuxiliaryImageRegularization2016,
  title = {Auxiliary {{Image Regularization}} for {{Deep CNNs}} with {{Noisy Labels}}},
  author = {Azadi, Samaneh and Feng, Jiashi and Jegelka, Stefanie and Darrell, Trevor},
  year = {2016},
  month = mar,
  number = {arXiv:1511.07069},
  eprint = {1511.07069},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-13},
  abstract = {Precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classification with mislabeled training samples - an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{baccourPervasiveAIIoT2022,
  title = {Pervasive {{AI}} for {{IoT}} Applications: {{A Survey}} on {{Resource-efficient Distributed Artificial Intelligence}}},
  shorttitle = {Pervasive {{AI}} for {{IoT}} Applications},
  author = {Baccour, Emna and Mhaisen, Naram and Abdellatif, Alaa Awad and Erbad, Aiman and Mohamed, Amr and Hamdi, Mounir and Guizani, Mohsen},
  year = {2022},
  journal = {IEEE Communications Surveys \& Tutorials},
  volume = {24},
  number = {4},
  eprint = {2105.01798},
  primaryclass = {cs},
  pages = {2366--2418},
  issn = {1553-877X, 2373-745X},
  urldate = {2023-07-28},
  abstract = {Artificial intelligence (AI) has witnessed a substantial breakthrough in a variety of Internet of Things (IoT) applications and services, spanning from recommendation systems to robotics control and military surveillance. This is driven by the easier access to sensory data and the enormous scale of pervasive/ubiquitous devices that generate zettabytes (ZB) of real-time data streams. Designing accurate models using such data streams, to predict future insights and revolutionize the decision-taking process, inaugurates pervasive systems as a worthy paradigm for a better quality-of-life. The confluence of pervasive computing and artificial intelligence, Pervasive AI, expanded the role of ubiquitous IoT systems from mainly data collection to executing distributed computations with a promising alternative to centralized learning, presenting various challenges. In this context, a wise cooperation and resource scheduling should be envisaged among IoT devices (e.g., smartphones, smart vehicles) and infrastructure (e.g. edge nodes, and base stations) to avoid communication and computation overheads and ensure maximum performance. In this paper, we conduct a comprehensive survey of the recent techniques developed to overcome these resource challenges in pervasive AI systems. Specifically, we first present an overview of the pervasive computing, its architecture, and its intersection with artificial intelligence. We then review the background, applications and performance metrics of AI, particularly Deep Learning (DL) and online learning, running in a ubiquitous system. Next, we provide a deep literature review of communication-efficient techniques, from both algorithmic and system perspectives, of distributed inference, training and online learning tasks across the combination of IoT devices, edge devices and cloud servers. Finally, we discuss our future vision and research challenges.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing}
}

@inproceedings{bagdasaryanHowBackdoorFederated2020,
  title = {How {{To Backdoor Federated Learning}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  year = {2020},
  month = jun,
  pages = {2938--2948},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-03-25},
  abstract = {Federated models are created by aggregating model updates submittedby participants.  To protect confidentiality of the training data,the aggregator by design has no visibility into how these updates aregenerated.  We show that this makes federated learning vulnerable to amodel-poisoning attack that is significantly more powerful than poisoningattacks that target only the training data.A single or multiple malicious participants can use modelreplacement to introduce backdoor functionality into the joint model,e.g., modify an image classifier so that it assigns an attacker-chosenlabel to images with certain features, or force a word predictor tocomplete certain sentences with an attacker-chosen word.  We evaluatemodel replacement under different assumptions for the standardfederated-learning tasks and show that it greatly outperformstraining-data poisoning.Federated learning employs secure aggregation to protect confidentialityof participants' local models and thus cannot detect anomalies inparticipants' contributions to the joint model.  To demonstrate thatanomaly detection would not have been effective in any case, we alsodevelop and evaluate a generic constrain-and-scale technique thatincorporates the evasion of defenses into the attacker's loss functionduring training.},
  langid = {english},
  language = {en}
}

@misc{bainLossPlotBetterWay2021,
  title = {{{LossPlot}}: {{A Better Way}} to {{Visualize Loss Landscapes}}},
  shorttitle = {{{LossPlot}}},
  author = {Bain, Robert and Tokarev, Mikhail and Kothari, Harsh and Damineni, Rahul},
  year = {2021},
  month = nov,
  number = {arXiv:2111.15133},
  eprint = {2111.15133},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-21},
  abstract = {Investigations into the loss landscapes of deep neural networks are often laborious. This work documents our user-driven approach to create a platform for semi-automating this process. LossPlot accepts data in the form of a csv, and allows multiple trained minimizers of the loss function to be manipulated in sync. Other features include a simple yet intuitive checkbox UI, summary statistics, and the ability to control clipping which other methods do not offer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@misc{bainVisualizingLossLandscape2021,
  title = {Visualizing the {{Loss Landscape}} of {{Winning Lottery Tickets}}},
  author = {Bain, Robert},
  year = {2021},
  month = dec,
  number = {arXiv:2112.08538},
  eprint = {2112.08538},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-26},
  abstract = {The underlying loss landscapes of deep neural networks have a great impact on their training, but they have mainly been studied theoretically due to computational constraints. This work vastly reduces the time required to compute such loss landscapes, and uses them to study winning lottery tickets found via iterative magnitude pruning. We also share results that contradict previously claimed correlations between certain loss landscape projection methods and model trainability and generalization error.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{baity-jesiComparingDynamicsDeep2018,
  title = {Comparing {{Dynamics}}: {{Deep Neural Networks}} versus {{Glassy Systems}}},
  shorttitle = {Comparing {{Dynamics}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {{Baity-Jesi}, Marco and Sagun, Levent and Geiger, Mario and Spigler, Stefano and Arous, Gerard Ben and Cammarota, Chiara and LeCun, Yann and Wyart, Matthieu and Biroli, Giulio},
  year = {2018},
  month = jul,
  pages = {314--323},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-19},
  abstract = {We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are the complexity of the loss-landscape and of the dynamics within it, and to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and data-sets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, thus showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.},
  langid = {english},
  language = {en}
}

@inproceedings{balleReconstructingTrainingData2022,
  title = {Reconstructing {{Training Data}} with {{Informed Adversaries}}},
  booktitle = {2022 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Balle, Borja and Cherubin, Giovanni and Hayes, Jamie},
  year = {2022},
  month = may,
  pages = {1138--1156},
  publisher = {IEEE},
  address = {San Francisco, CA, USA},
  urldate = {2024-02-27},
  isbn = {978-1-66541-316-9}
}

@inproceedings{bar-noyQualityofinformationAwareNetworking2011,
  title = {Quality-of-Information Aware Networking for Tactical Military Networks},
  booktitle = {2011 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications Workshops}} ({{PERCOM Workshops}})},
  author = {{Bar-Noy}, A. and Cirincione, G. and Govindan, R. and Krishnamurthy, S. and LaPorta, T. F. and Mohapatra, P. and Neely, M. and Yener, A.},
  year = {2011},
  month = mar,
  pages = {2--7},
  urldate = {2023-11-25},
  abstract = {In tactical military networks, decisions must often be made quickly based on information at hand. It is a challenge to provide decision makers with a notion of the quality of the information they have, or to provide a method by which decision makers can specify a required quality of information. It is a further challenge to honor requests for a required quality of information when selecting information sources, transporting information through a highly-dynamic network, and perhaps performing processing on that information. In this paper we motivate the need for a general, but formal, definition of quality-of-information so that this metric may be specified and potentially optimized by algorithms that operate a tactical network. Furthermore, we define a new notion, the operational information content capacity, to capture the amount and quality of information that a network can deliver.}
}

@book{barbarescoGeometricStructuresStatistical2021,
  title = {Geometric Structures of Statistical Physics, Information Geometry, and Learning: {{SPIGL}}'20, {{Les Houches}}, {{France}}, {{July}} 27--31},
  shorttitle = {Geometric Structures of Statistical Physics, Information Geometry, and Learning},
  editor = {Barbaresco, Fr{\'e}d{\'e}ric and Nielsen, Frank},
  year = {2021},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}},
  number = {volume 361},
  publisher = {Springer Nature},
  address = {Cham, Switzerland},
  abstract = {Machine learning and artificial intelligence increasingly use methodological tools rooted in statistical physics. Conversely, limitations and pitfalls encountered in AI question the very foundations of statistical physics. This interplay between AI and statistical physics has been attested since the birth of AI, and principles underpinning statistical physics can shed new light on the conceptual basis of AI. During the last fifty years, statistical physics has been investigated through new geometric structures allowing covariant formalization of the thermodynamics. Inference methods in machine learning have begun to adapt these new geometric structures to process data in more abstract representation spaces. This volume collects selected contributions on the interplay of statistical physics and artificial intelligence. The aim is to provide a constructive dialogue around a common foundation to allow the establishment of new principles and laws governing these two disciplines in a unified manner. The contributions were presented at the workshop on the Joint Structures and Common Foundation of Statistical Physics, Information Geometry and Inference for Learning which was held in Les Houches in July 2020. The various theoretical approaches are discussed in the context of potential applications in cognitive systems, machine learning, signal processing},
  isbn = {978-3-030-77957-3 978-3-030-77956-6},
  langid = {english},
  language = {eng}
}

@inproceedings{barrettImprovingMLTraining2019,
  title = {Improving {{ML Training Data}} with {{Gold-Standard Quality Metrics}}},
  booktitle = {{{KDD}} '19: 25th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}, {{Workshop}} on {{Data Collection}}, {{Curation}}, and {{Labeling}} ({{DCCL}}) for {{Mining}} and {{Learning}}, {{August}} 05, 2019, {{Anchorage}}, {{AK}}.},
  author = {Barrett, Leslie and Sherman, Michael W.},
  year = {2019}
}

@inproceedings{barryImpactDataQuality2023,
  title = {On the {{Impact}} of {{Data Quality}} on {{Image Classification Fairness}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Barry, Aki and Han, Lei and Demartini, Gianluca},
  year = {2023},
  month = jul,
  series = {{{SIGIR}} '23},
  pages = {2225--2229},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-11-16},
  abstract = {With the proliferation of algorithmic decision-making, increased scrutiny has been placed on these systems. This paper explores the relationship between the quality of the training data and the overall fairness of the models trained with such data in the context of supervised classification. We measure key fairness metrics across a range of algorithms over multiple image classification datasets that have a varying level of noise in both the labels and the training data itself. We describe noise in the labels as inaccuracies in the labelling of the data in the training set and noise in the data as distortions in the data, also in the training set. By adding noise to the original datasets, we can explore the relationship between the quality of the training data and the fairness of the output of the models trained on that data.},
  isbn = {978-1-4503-9408-6},
  keywords = {data quality,fairness,machine learning}
}

@article{baskervilleLossSurfacesNeural2021,
  title = {The Loss Surfaces of Neural Networks with General Activation Functions},
  author = {Baskerville, Nicholas P. and Keating, Jonathan P. and Mezzadri, Francesco and Najnudel, Joseph},
  year = {2021},
  month = jun,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2021},
  number = {6},
  pages = {064001},
  publisher = {{IOP Publishing and SISSA}},
  issn = {1742-5468},
  urldate = {2024-05-18},
  abstract = {The loss surfaces of deep neural networks have been the subject of several studies, theoretical and experimental, over the last few years. One strand of work considers the complexity, in the sense of local optima, of high dimensional random functions with the aim of informing how local optimisation methods may perform in such complicated settings. Prior work of Choromanska et al (2015) established a direct link between the training loss surfaces of deep multi-layer perceptron networks and spherical multi-spin glass models under some very strong assumptions on the network and its data. In this work, we test the validity of this approach by removing the undesirable restriction to ReLU activation functions. In doing so, we chart a new path through the spin glass complexity calculations using supersymmetric methods in random matrix theory which may prove useful in other contexts. Our results shed new light on both the strengths and the weaknesses of spin glass models in this context.},
  langid = {english},
  language = {en}
}

@phdthesis{baskervilleRandomMatrixTheory2023,
  title = {Random Matrix Theory and the Loss Surfaces of Neural Networks},
  author = {Baskerville, Nicholas P},
  year = {2023},
  month = jun,
  abstract = {Neural network models are one of the most successful approaches to machine learning, enjoying an enormous amount of development and research over recent years and finding concrete real-world applications in almost any conceivable area of science, engineering and modern life in general. The theoretical understanding of neural networks trails significantly behind their practical success and the engineering heuristics that have grown up around them. Random matrix theory provides a rich framework of tools with which aspects of neural network phenomenology can be explored theoretically. In this thesis, we establish significant extensions of prior work using random matrix theory to understand and describe the loss surfaces of large neural networks, particularly generalising to different architectures. Informed by the historical applications of random matrix theory in physics and elsewhere, we establish the presence of local random matrix universality in real neural networks and then utilise this as a modeling assumption to derive powerful and novel results about the Hessians of neural network loss surfaces and their spectra. In addition to these major contributions, we make use of random matrix models for neural network loss surfaces to shed light on modern neural network training approaches and even to derive a novel and effective variant of a popular optimisation algorithm.},
  langid = {english},
  language = {en},
  school = {University of Bristol}
}

@misc{belferSpectralAnalysisNeural2021,
  title = {Spectral {{Analysis}} of the {{Neural Tangent Kernel}} for {{Deep Residual Networks}}},
  author = {Belfer, Yuval and Geifman, Amnon and Galun, Meirav and Basri, Ronen},
  year = {2021},
  month = apr,
  urldate = {2024-05-18},
  abstract = {Deep residual network architectures have been shown to achieve superior accuracy over classical feed-forward networks, yet their success is still not fully understood. Focusing on massively over-parameterized, fully connected residual networks with ReLU activation through their respective neural tangent kernels (ResNTK), we provide here a spectral analysis of these kernels. Specifically, we show that, much like NTK for fully connected networks (FC-NTK), for input distributed uniformly on the hypersphere \${\textbackslash}mathbb\{S\}{\textasciicircum}\{d-1\}\$, the eigenfunctions of ResNTK are the spherical harmonics and the eigenvalues decay polynomially with frequency \$k\$ as \$k{\textasciicircum}\{-d\}\$. These in turn imply that the set of functions in their Reproducing Kernel Hilbert Space are identical to those of FC-NTK, and consequently also to those of the Laplace kernel. We further show, by drawing on the analogy to the Laplace kernel, that depending on the choice of a hyper-parameter that balances between the skip and residual connections ResNTK can either become spiky with depth, as with FC-NTK, or maintain a stable shape.},
  langid = {english},
  language = {en}
}

@misc{belgoumriDataQualityEdge2024,
  title = {Data {{Quality}} in {{Edge Machine Learning}}: {{A State-of-the-Art Survey}}},
  shorttitle = {Data {{Quality}} in {{Edge Machine Learning}}},
  author = {Belgoumri, Mohammed Djameleddine and Bouadjenek, Mohamed Reda and Aryal, Sunil and Hacid, Hakim},
  year = {2024},
  month = jun,
  number = {arXiv:2406.02600},
  eprint = {2406.02600},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-18},
  abstract = {Data-driven Artificial Intelligence (AI) systems trained using Machine Learning (ML) are shaping an ever-increasing (in size and importance) portion of our lives, including, but not limited to, recommendation systems, autonomous driving technologies, healthcare diagnostics, financial services, and personalized marketing. On the one hand, the outsized influence of these systems imposes a high standard of quality, particularly in the data used to train them. On the other hand, establishing and maintaining standards of Data Quality (DQ) becomes more challenging due to the proliferation of Edge Computing and Internet of Things devices, along with their increasing adoption for training and deploying ML models. The nature of the edge environment -- characterized by limited resources, decentralized data storage, and processing -- exacerbates data-related issues, making them more frequent, severe, and difficult to detect and mitigate. From these observations, it follows that DQ research for edge ML is a critical and urgent exploration track for the safety and robust usefulness of present and future AI systems. Despite this fact, DQ research for edge ML is still in its infancy. The literature on this subject remains fragmented and scattered across different research communities, with no comprehensive survey to date. Hence, this paper aims to fill this gap by providing a global view of the existing literature from multiple disciplines that can be grouped under the umbrella of DQ for edge ML. Specifically, we present a tentative definition of data quality in Edge computing, which we use to establish a set of DQ dimensions. We explore each dimension in detail, including existing solutions for mitigation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{belkinFitFearRemarkable2021,
  title = {Fit without Fear: Remarkable Mathematical Phenomena of Deep Learning through the Prism of Interpolation},
  shorttitle = {Fit without Fear},
  author = {Belkin, Mikhail},
  year = {2021},
  month = may,
  journal = {Acta Numerica},
  volume = {30},
  pages = {203--248},
  issn = {0962-4929, 1474-0508},
  urldate = {2024-04-25},
  abstract = {In the past decade the mathematical theory of machine learning has lagged far behind the triumphs of deep neural networks on practical challenges. However, the gap between theory and practice is gradually starting to close. In this paper I will attempt to assemble some pieces of the remarkable and still incomplete mathematical mosaic emerging from the efforts to understand the foundations of deep learning. The two key themes will be interpolation and its sibling over-parametrization. Interpolation corresponds to fitting data, even noisy data, exactly. Over-parametrization enables interpolation and provides flexibility to select a suitable interpolating model.As we will see, just as a physical prism separates colours mixed within a ray of light, the figurative prism of interpolation helps to disentangle generalization and optimization properties within the complex picture of modern machine learning. This article is written in the belief and hope that clearer understanding of these issues will bring us a step closer towards a general theory of deep learning and machine learning.},
  langid = {english},
  language = {en}
}

@article{belkinReconcilingModernMachinelearning2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias--Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {116},
  number = {32},
  pages = {15849--15854},
  issn = {0027-8424},
  urldate = {2024-02-27},
  abstract = {While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms., Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias--variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias--variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ``double-descent'' curve subsumes the textbook U-shaped bias--variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  pmcid = {PMC6689936},
  pmid = {31341078}
}

@misc{bellamyAIFairness3602018,
  title = {{{AI Fairness}} 360: {{An Extensible Toolkit}} for {{Detecting}}, {{Understanding}}, and {{Mitigating Unwanted Algorithmic Bias}}},
  shorttitle = {{{AI Fairness}} 360},
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  year = {2018},
  month = oct,
  number = {arXiv:1810.01943},
  eprint = {1810.01943},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-10},
  abstract = {Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license \{https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.\vphantom\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@article{bemaniAggregationStrategyFederated2022,
  title = {Aggregation {{Strategy}} on {{Federated Machine Learning Algorithm}} for {{Collaborative Predictive Maintenance}}},
  author = {Bemani, Ali and Bj{\"o}rsell, Niclas},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {16},
  pages = {6252},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  urldate = {2023-10-02},
  abstract = {Industry 4.0 lets the industry build compact, precise, and connected assets and also has made modern industrial assets a massive source of data that can be used in process optimization, defining product quality, and predictive maintenance (PM). Large amounts of data are collected from machines, processed, and analyzed by different machine learning (ML) algorithms to achieve effective PM. These machines, assumed as edge devices, transmit their data readings to the cloud for processing and modeling. Transmitting massive amounts of data between edge and cloud is costly, increases latency, and causes privacy concerns. To address this issue, efforts have been made to use edge computing in PM applications., reducing data transmission costs and increasing processing speed. Federated learning (FL) has been proposed a mechanism that provides the ability to create a model from distributed data in edge, fog, and cloud layers without violating privacy and offers new opportunities for a collaborative approach to PM applications. However, FL has challenges in confronting with asset management in the industry, especially in the PM applications, which need to be considered in order to be fully compatible with these applications. This study describes distributed ML for PM applications and proposes two federated algorithms: Federated support vector machine (FedSVM) with memory for anomaly detection and federated long-short term memory (FedLSTM) for remaining useful life (RUL) estimation that enables factories at the fog level to maximize their PM models' accuracy without compromising their privacy. A global model at the cloud level has also been generated based on these algorithms. We have evaluated the approach using the Commercial Modular Aero-Propulsion System Simulation (CMAPSS) dataset to predict engines' RUL Experimental results demonstrate the advantage of FedSVM and FedLSTM in terms of model accuracy, model convergence time, and network usage resources.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {aggregation strategy,distributed machine learning algorithm,edge and fog computing,federated learning,resource allocation}
}

@misc{benderPositionBasedSimulationMethods2015,
  title = {Position-{{Based Simulation Methods}} in {{Computer Graphics}}},
  author = {Bender, Jan and M{\"u}ller, Matthias and Macklin, Miles},
  year = {2015},
  journal = {EG 2015 - Tutorials},
  publisher = {The Eurographics Association},
  issn = {1017-4656},
  urldate = {2024-08-26},
  abstract = {The physically-based simulation of mechanical effects has been an important research topic in computer graphics for more than two decades. Classical methods in this field discretize Newton's second law and determine different forces to simulate various effects like stretching, shearing, and bending of deformable bodies or pressure and viscosity of fluids, to mention just a few. Given these forces, velocities and finally positions are determined by a numerical integration of the resulting accelerations.},
  archiveprefix = {The Eurographics Association},
  langid = {english},
  language = {en},
  keywords = {Animation,based animation,based dynamics,deformable solids,fluids Computer Graphics [I.3.7],Keywords,physically,position,rigid bodies,Three Dimensional Graphics and Realism}
}

@article{berkFairnessCriminalJustice2021,
  title = {Fairness in {{Criminal Justice Risk Assessments}}: {{The State}} of the {{Art}}},
  shorttitle = {Fairness in {{Criminal Justice Risk Assessments}}},
  author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
  year = {2021},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {50},
  number = {1},
  pages = {3--44},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  urldate = {2023-12-30},
  abstract = {Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this article, we seek to clarify the trade-offs between different kinds of fairness and between fairness and accuracy. Methods: We draw on the existing literatures in criminology, computer science, and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments. Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy. Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging trade-offs. These lessons apply to applications well beyond criminology where assessments of risk can be used by decision makers. Examples include mortgage lending, employment, college admissions, child welfare, and medical diagnoses.},
  langid = {english},
  language = {en}
}

@article{berti-equilleDiscoveryGenuineFunctional2018,
  title = {Discovery of Genuine Functional Dependencies from Relational Data with Missing Values},
  author = {{Berti-{\'E}quille}, Laure and Harmouch, Hazar and Naumann, Felix and Novelli, No{\"e}l and Thirumuruganathan, Saravanan},
  year = {2018},
  month = apr,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {8},
  pages = {880--892},
  issn = {2150-8097},
  urldate = {2023-11-25},
  abstract = {Functional dependencies (FDs) play an important role in maintaining data quality. They can be used to enforce data consistency and to guide repairs over a database. In this work, we investigate the problem of missing values and its impact on FD discovery. When using existing FD discovery algorithms, some genuine FDs could not be detected precisely due to missing values or some non-genuine FDs can be discovered even though they are caused by missing values with a certain NULL semantics. We define a notion of genuineness and propose algorithms to compute the genuineness score of a discovered FD. This can be used to identify the genuine FDs among the set of all valid dependencies that hold on the data. We evaluate the quality of our method over various real-world and semi-synthetic datasets with extensive experiments. The results show that our method performs well for relatively large FD sets and is able to accurately capture genuine FDs.}
}

@article{bertossiDataQualityExplainable2020,
  title = {Data {{Quality}} and {{Explainable AI}}},
  author = {Bertossi, Leopoldo and Geerts, Floris},
  year = {2020},
  month = jun,
  journal = {Journal of Data and Information Quality},
  volume = {12},
  number = {2},
  pages = {1--9},
  issn = {1936-1955, 1936-1963},
  urldate = {2023-12-11},
  abstract = {In this work, we provide some insights and develop some ideas, with few technical details, about the role of explanations in Data Quality in the context of data-based machine learning models (ML). In this direction, there are, as expected, roles for causality, and               explainable artificial intelligence               . The latter area not only sheds light on the models, but also on the data that support model construction. There is also room for defining, identifying, and explaining errors in data, in particular, in ML, and also for suggesting repair actions. More generally, explanations can be used as a basis for defining dirty data in the context of ML, and measuring or quantifying them. We think dirtiness as relative to the ML task at hand, e.g., classification.},
  langid = {english},
  language = {en}
}

@article{bhatiaDecentralizedDataEvaluation2023,
  title = {A Decentralized Data Evaluation Framework in Federated Learning},
  author = {Bhatia, Laveen and Samet, Saeed},
  year = {2023},
  month = dec,
  journal = {Blockchain: Research and Applications},
  volume = {4},
  number = {4},
  pages = {100152},
  issn = {20967209},
  urldate = {2023-12-16},
  abstract = {Federated Learning (FL) is a type of distributed deep learning framework in which multiple devices train a local model using local data, and the gradients of the local model are then sent to a central server that aggregates them to create a global model. This type of framework is ideal where data privacy is of utmost importance because the data never leave the local device. However, a major concern in FL is ensuring the data quality of local training data. Since there is no control over the local training data, ensuring that the local model is trained on clean data becomes challenging. A model trained on poor-quality data can have a significant impact on its accuracy. In this paper, we propose a decentralized approach using blockchain to ensure local model data quality. We use miners to validate each local model by checking its accuracy against a secret testing dataset. This is done using a smart contract that the miners invoke during the mining process. The local model is aggregated with the global model only if it passes a preset accuracy threshold. We test our proposed method on two datasets: the Brain Tumor Classification dataset from Kaggle, comprised of 7000 MRI images divided into two classes (Tumor/No Tumor), and the Medical MNIST dataset, which includes 58,954 images classified into six different classes: AbdomenCT, BreastMRI, ChestCT, Chest X-ray, Hand X-ray, and HeadCT. Our results show that our method outperforms the original FL approach in all experiments.},
  langid = {english},
  language = {en}
}

@misc{bianFederatedEmpiricalRisk2023,
  title = {Federated {{Empirical Risk Minimization}} via {{Second-Order Method}}},
  author = {Bian, Song and Song, Zhao and Yin, Junze},
  year = {2023},
  month = may,
  number = {arXiv:2305.17482},
  eprint = {2305.17482},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Many convex optimization problems with important applications in machine learning are formulated as empirical risk minimization (ERM). There are several examples: linear and logistic regression, LASSO, kernel regression, quantile regression, \$p\$-norm regression, support vector machines (SVM), and mean-field variational inference. To improve data privacy, federated learning is proposed in machine learning as a framework for training deep learning models on the network edge without sharing data between participating nodes. In this work, we present an interior point method (IPM) to solve a general ERM problem under the federated learning setting. We show that the communication complexity of each iteration of our IPM is \${\textbackslash}tilde\{O\}(d{\textasciicircum}\{3/2\})\$, where \$d\$ is the dimension (i.e., number of features) of the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning}
}

@misc{bijralDataDependenceDistributed2016,
  title = {On {{Data Dependence}} in {{Distributed Stochastic Optimization}}},
  author = {Bijral, Avleen S. and Sarwate, Anand D. and Srebro, Nathan},
  year = {2016},
  month = aug,
  number = {arXiv:1603.04379},
  eprint = {1603.04379},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2024-05-07},
  abstract = {We study a distributed consensus-based stochastic gradient descent (SGD) algorithm and show that the rate of convergence involves the spectral properties of two matrices: the standard spectral gap of a weight matrix from the network topology and a new term depending on the spectral norm of the sample covariance matrix of the data. This data-dependent convergence rate shows that distributed SGD algorithms perform better on datasets with small spectral norm. Our analysis method also allows us to find data-dependent convergence rates as we limit the amount of communication. Spreading a fixed amount of data across more nodes slows convergence; for asymptotically growing data sets we show that adding more machines can help when minimizing twice-differentiable losses.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Mathematics - Optimization and Control}
}

@article{bijralDataDependentConvergenceConsensus2017,
  title = {Data-{{Dependent Convergence}} for {{Consensus Stochastic Optimization}}},
  author = {Bijral, Avleen S. and Sarwate, Anand D. and Srebro, Nathan},
  year = {2017},
  month = sep,
  journal = {IEEE Transactions on Automatic Control},
  volume = {62},
  number = {9},
  pages = {4483--4498},
  issn = {1558-2523},
  urldate = {2024-05-07},
  abstract = {We study a distributed consensus-based stochastic gradient descent (SGD) algorithm and show that the rate of convergence involves the spectral properties of two matrices: The standard spectral gap of a weight matrix from the network topology and a new term depending on the spectral norm of the sample covariance matrix of the data. This data-dependent convergence rate shows that distributed SGD algorithms perform better on datasets with small spectral norm. Our analysis method also allows us to find data-dependent convergence rates as we limit the amount of communication. Spreading a fixed amount of data across more nodes slows convergence; for asymptotically growing datasets, we show that adding more machines can help when minimizing twice-differentiable losses.},
  keywords = {Convergence,distributed computing,machine learning,Manganese,minimization,optimization,Silicon}
}

@inproceedings{binnsFairnessMachineLearning2018,
  title = {Fairness in {{Machine Learning}}: {{Lessons}} from {{Political Philosophy}}},
  shorttitle = {Fairness in {{Machine Learning}}},
  booktitle = {Proceedings of the 1st {{Conference}} on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  author = {Binns, Reuben},
  year = {2018},
  month = jan,
  pages = {149--159},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-12-30},
  abstract = {What does it mean for a machine learning model to be `fair', in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise `fairness' in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.},
  langid = {english},
  language = {en}
}

@inproceedings{blanchardMachineLearningAdversaries2017,
  title = {Machine {{Learning}} with {{Adversaries}}: {{Byzantine Tolerant Gradient Descent}}},
  shorttitle = {Machine {{Learning}} with {{Adversaries}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-10-22}
}

@misc{boeschEdgeIntelligenceEdge2023,
  title = {Edge {{Intelligence}}: {{Edge Computing}} and {{Machine Learning}} (2023 {{Guide}})},
  shorttitle = {Edge {{Intelligence}}},
  author = {Boesch, Gaudenz},
  year = {2023},
  month = jan,
  urldate = {2023-09-20},
  abstract = {Edge Intelligence or Edge AI moves AI computing from the cloud to edge devices, where data is generated. This is a key to AI democratization.},
  langid = {american},
  language = {american},
  keywords = {figure}
}

@inproceedings{bolukbasiManComputerProgrammer2016,
  title = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  shorttitle = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  year = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-01-30},
  abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female.  Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.}
}

@inproceedings{bonawitzPracticalSecureAggregation2017,
  title = {Practical {{Secure Aggregation}} for {{Privacy-Preserving Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  year = {2017},
  month = oct,
  pages = {1175--1191},
  publisher = {ACM},
  address = {Dallas Texas USA},
  urldate = {2024-02-24},
  isbn = {978-1-4503-4946-8},
  langid = {english},
  language = {en}
}

@article{borsVisualInteractiveCreation2018,
  title = {Visual {{Interactive Creation}}, {{Customization}}, and {{Analysis}} of {{Data Quality Metrics}}},
  author = {Bors, Christian and Gschwandtner, Theresia and Kriglstein, Simone and Miksch, Silvia and Pohl, Margit},
  year = {2018},
  month = may,
  journal = {Journal of Data and Information Quality},
  volume = {10},
  number = {1},
  pages = {3:1--3:26},
  issn = {1936-1955},
  urldate = {2023-11-15},
  abstract = {During data preprocessing, analysts spend a significant part of their time and effort profiling the quality of the data along with cleansing and transforming the data for further analysis. While quality metrics---ranging from general to domain-specific measures---support assessment of the quality of a dataset, there are hardly any approaches to visually support the analyst in customizing and applying such metrics. Yet, visual approaches could facilitate users' involvement in data quality assessment. We present MetricDoc, an interactive environment for assessing data quality that provides customizable, reusable quality metrics in combination with immediate visual feedback. Moreover, we provide an overview visualization of these quality metrics along with error visualizations that facilitate interactive navigation of the data to determine the causes of quality issues present in the data. In this article, we describe the architecture, design, and evaluation of MetricDoc, which underwent several design cycles, including heuristic evaluation and expert reviews as well as a focus group with data quality, human-computer interaction, and visual analytics experts.},
  keywords = {Data profiling,data quality metrics,visual exploration}
}

@inproceedings{bosmanEmpiricalLossLandscape2023,
  title = {Empirical {{Loss Landscape Analysis}} of {{Neural Network Activation Functions}}},
  booktitle = {Proceedings of the {{Companion Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Bosman, Anna Sergeevna and Engelbrecht, Andries and Helbig, Marde},
  year = {2023},
  month = jul,
  series = {{{GECCO}} '23 {{Companion}}},
  pages = {2029--2037},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-05-18},
  abstract = {Activation functions play a significant role in neural network design by enabling non-linearity. The choice of activation function was previously shown to influence the properties of the resulting loss landscape. Understanding the relationship between activation functions and loss landscape properties is important for neural architecture and training algorithm design. This study empirically investigates neural network loss landscapes associated with hyperbolic tangent, rectified linear unit, and exponential linear unit activation functions. Rectified linear unit is shown to yield the most convex loss landscape, and exponential linear unit is shown to yield the least flat loss landscape, and to exhibit superior generalisation performance. The presence of wide and narrow valleys in the loss landscape is established for all activation functions, and the narrow valleys are shown to correlate with saturated neurons and implicitly regularised network configurations.},
  isbn = {9798400701207},
  keywords = {activation functions,fitness landscape analysis,loss landscape,neural networks}
}

@misc{bosmanLossSurfaceModality2020,
  title = {Loss {{Surface Modality}} of {{Feed-Forward Neural Network Architectures}}},
  author = {Bosman, Anna Sergeevna and Engelbrecht, Andries and Helbig, Mard{\'e}},
  year = {2020},
  month = jan,
  number = {arXiv:1905.10268},
  eprint = {1905.10268},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-13},
  abstract = {It has been argued in the past that high-dimensional neural networks do not exhibit local minima capable of trapping an optimisation algorithm. However, the relationship between loss surface modality and the neural architecture parameters, such as the number of hidden neurons per layer and the number of hidden layers, remains poorly understood. This study employs fitness landscape analysis to study the modality of neural network loss surfaces under various feed-forward architecture settings. An increase in the problem dimensionality is shown to yield a more searchable and more exploitable loss surface. An increase in the hidden layer width is shown to effectively reduce the number of local minima, and simplify the shape of the global attractor. An increase in the architecture depth is shown to sharpen the global attractor, thus making it more exploitable.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{bosmanVisualisingBasinsAttraction2020,
  title = {Visualising Basins of Attraction for the Cross-Entropy and the Squared Error Neural Network Loss Functions},
  author = {Bosman, Anna Sergeevna and Engelbrecht, Andries and Helbig, Mard{\'e}},
  year = {2020},
  month = aug,
  journal = {Neurocomputing},
  volume = {400},
  pages = {113--136},
  issn = {0925-2312},
  urldate = {2024-06-22},
  abstract = {Quantification of the stationary points and the associated basins of attraction of neural network loss surfaces is an important step towards a better understanding of neural network loss surfaces at large. This work proposes a novel method to visualise basins of attraction together with the associated stationary points via gradient-based stochastic sampling. The proposed technique is used to perform an empirical study of the loss surfaces generated by two different error metrics: quadratic loss and entropic loss. The empirical observations confirm the theoretical hypothesis regarding the nature of neural network attraction basins. Entropic loss is shown to exhibit stronger gradients and fewer stationary points than quadratic loss, indicating that entropic loss has a more searchable landscape. Quadratic loss is shown to be more resilient to overfitting than entropic loss. Both losses are shown to exhibit local minima, but the number of local minima is shown to decrease with an increase in dimensionality. Thus, the proposed visualisation technique successfully captures the local minima properties exhibited by the neural network loss surfaces, and can be used for the purpose of fitness landscape analysis of neural networks.},
  keywords = {Cross-entropy,Fitness landscape analysis,Local minima,Loss functions,Neural networks,Squared error}
}

@article{bottcherVisualizingHighdimensionalLoss2024,
  title = {Visualizing High-Dimensional Loss Landscapes with {{Hessian}} Directions},
  author = {B{\"o}ttcher, Lucas and Wheeler, Gregory},
  year = {2024},
  month = feb,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2024},
  number = {2},
  pages = {023401},
  publisher = {IOP Publishing},
  issn = {1742-5468},
  urldate = {2024-05-19},
  abstract = {Analyzing the geometric properties of high-dimensional loss functions, such as local curvature and the existence of other optima around a certain point in loss space, can help provide a better understanding of the interplay between neural-network structure, implementation attributes, and learning performance. In this paper, we combine concepts from high-dimensional probability and differential geometry to study how curvature properties in lower-dimensional loss representations depend on those in the original loss space. We show that saddle points in the original space are rarely correctly identified as such in the expected lower-dimensional representations if random projections are used. The principal curvature in the expected lower-dimensional representation is proportional to the mean curvature in the original loss space. Hence, the mean curvature in the original loss space determines if saddle points appear, on average, as either minima, maxima, or almost flat regions. We use the connection between expected curvature in random projections and mean curvature in the original space (i.e. the normalized Hessian trace) to compute Hutchinson-type trace estimates without calculating Hessian-vector products as in the original Hutchinson method. Because random projections are not suitable for correctly identifying saddle information, we propose to study projections along the dominant Hessian directions that are associated with the largest and smallest principal curvatures. We connect our findings to the ongoing debate on loss landscape flatness and generalizability. Finally, for different common image classifiers and a function approximator, we show and compare random and Hessian projections of loss landscapes with up to approximately parameters.},
  langid = {english},
  language = {en}
}

@article{bouazizProjectiveDynamicsFusing2014,
  title = {Projective Dynamics: Fusing Constraint Projections for Fast Simulation},
  shorttitle = {Projective Dynamics},
  author = {Bouaziz, Sofien and Martin, Sebastian and Liu, Tiantian and Kavan, Ladislav and Pauly, Mark},
  year = {2014},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {33},
  number = {4},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  urldate = {2024-08-22},
  abstract = {We present a new method for implicit time integration of physical systems. Our approach builds a bridge between nodal Finite Element methods and Position Based Dynamics, leading to a simple, efficient, robust, yet accurate solver that supports many different types of constraints. We propose specially designed energy potentials that can be solved efficiently using an alternating optimization approach. Inspired by continuum mechanics, we derive a set of continuum-based potentials that can be efficiently incorporated within our solver. We demonstrate the generality and robustness of our approach in many different applications ranging from the simulation of solids, cloths, and shells, to example-based simulation. Comparisons to Newton-based and Position Based Dynamics solvers highlight the benefits of our formulation.},
  langid = {english},
  language = {en}
}

@article{brodleyIdentifyingMislabeledTraining1999,
  title = {Identifying {{Mislabeled Training Data}}},
  author = {Brodley, C. E. and Friedl, M. A.},
  year = {1999},
  month = aug,
  journal = {Journal of Artificial Intelligence Research},
  volume = {11},
  pages = {131--167},
  issn = {1076-9757},
  urldate = {2024-04-13},
  abstract = {This paper presents a new approach to identifying and    eliminating mislabeled training instances for supervised learning. The    goal of this approach is to improve classification accuracies produced    by learning algorithms by improving the quality of the training data.    Our approach uses a set of learning algorithms to create classifiers    that serve as noise filters for the training data.  We evaluate single    algorithm, majority vote and consensus filters on five datasets that    are prone to labeling errors.  Our experiments illustrate that    filtering significantly improves classification accuracy for noise    levels up to 30 percent.  An analytical and empirical evaluation of    the precision of our approach shows that consensus filters are    conservative at throwing away good data at the expense of retaining    bad data and that majority filters are better at detecting bad data at    the expense of throwing away good data.  This suggests that for    situations in which there is a paucity of data, consensus filters are    preferable, whereas majority vote filters are preferable for    situations with an abundance of data.},
  copyright = {Copyright (c)},
  langid = {english},
  language = {en}
}

@inproceedings{brunetUnderstandingOriginsBias2019,
  title = {Understanding the {{Origins}} of {{Bias}} in {{Word Embeddings}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Brunet, Marc-Etienne and {Alkalay-Houlihan}, Colleen and Anderson, Ashton and Zemel, Richard},
  year = {2019},
  month = may,
  pages = {803--811},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-08},
  abstract = {Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems can amplify stereotypes in important contexts. Although some methods have been developed to mitigate this problem, how word embedding biases arise during training is poorly understood. In this work we develop a technique to address this question. Given a word embedding, our method reveals how perturbing the training corpus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikipedia and New York Times corpora, and find it to be very accurate.},
  langid = {english},
  language = {en}
}

@misc{bucarelliTopologicalDescriptionLoss2024,
  title = {A Topological Description of Loss Surfaces Based on {{Betti Numbers}}},
  author = {Bucarelli, Maria Sofia and D'Inverno, Giuseppe Alessio and Bianchini, Monica and Scarselli, Franco and Silvestri, Fabrizio},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03824},
  eprint = {2401.03824},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-28},
  abstract = {In the context of deep learning models, attention has recently been paid to studying the surface of the loss function in order to better understand training with methods based on gradient descent. This search for an appropriate description, both analytical and topological, has led to numerous efforts to identify spurious minima and characterize gradient dynamics. Our work aims to contribute to this field by providing a topological measure to evaluate loss complexity in the case of multilayer neural networks. We compare deep and shallow architectures with common sigmoidal activation functions by deriving upper and lower bounds on the complexity of their loss function and revealing how that complexity is influenced by the number of hidden units, training models, and the activation function used. Additionally, we found that certain variations in the loss function or model architecture, such as adding an {$\ell$}2 regularization term or implementing skip connections in a feedforward network, do not affect loss topology in specific cases.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{budachEffectsDataQuality2022,
  title = {The {{Effects}} of {{Data Quality}} on {{Machine Learning Performance}}},
  author = {Budach, Lukas and Feuerpfeil, Moritz and Ihde, Nina and Nathansen, Andrea and Noack, Nele and Patzlaff, Hendrik and Naumann, Felix and Harmouch, Hazar},
  year = {2022},
  month = nov,
  number = {arXiv:2207.14529},
  eprint = {2207.14529},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-25},
  abstract = {Modern artificial intelligence (AI) applications require large quantities of training and test data. This need creates critical challenges not only concerning the availability of such data, but also regarding its quality. For example, incomplete, erroneous or inappropriate training data can lead to unreliable models that produce ultimately poor decisions. Trustworthy AI applications require high-quality training and test data along many dimensions, such as accuracy, completeness, consistency, and uniformity. We explore empirically the relationship between six of the traditional data quality dimensions and the performance of fifteen widely used machine learning (ML) algorithms covering the tasks of classification, regression, and clustering, with the goal of explaining their performance in terms of data quality. Our experiments distinguish three scenarios based on the AI pipeline steps that were fed with polluted data: polluted training data, test data, or both. We conclude the paper with an extensive discussion of our observations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases}
}

@misc{bushnaqUsingDegeneracyLoss2024,
  title = {Using {{Degeneracy}} in the {{Loss Landscape}} for {{Mechanistic Interpretability}}},
  author = {Bushnaq, Lucius and Mendel, Jake and Heimersheim, Stefan and Braun, Dan and {Goldowsky-Dill}, Nicholas and H{\"a}nni, Kaarel and Wu, Cindy and Hobbhahn, Marius},
  year = {2024},
  month = may,
  number = {arXiv:2405.10927},
  eprint = {2405.10927},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-04},
  abstract = {Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{buzagloDeconstructingDataReconstruction2023,
  title = {Deconstructing {{Data Reconstruction}}: {{Multiclass}}, {{Weight Decay}} and {{General Losses}}},
  shorttitle = {Deconstructing {{Data Reconstruction}}},
  author = {Buzaglo, Gon and Haim, Niv and Yehudai, Gilad and Vardi, Gal and Oz, Yakir and Nikankin, Yaniv and Irani, Michal},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {51515--51535},
  urldate = {2024-03-17},
  langid = {english},
  language = {en}
}

@inproceedings{byabazaireUsingTrustMeasure2020,
  title = {Using {{Trust}} as a {{Measure}} to {{Derive Data Quality}} in {{Data Shared IoT Deployments}}},
  booktitle = {2020 29th {{International Conference}} on {{Computer Communications}} and {{Networks}} ({{ICCCN}})},
  author = {Byabazaire, John and O'Hare, Gregory and Delaney, Declan},
  year = {2020},
  month = aug,
  pages = {1--9},
  issn = {2637-9430},
  urldate = {2023-12-01},
  abstract = {Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.}
}

@article{caiChallengesDataQuality2015,
  title = {The {{Challenges}} of {{Data Quality}} and {{Data Quality Assessment}} in the {{Big Data Era}}},
  author = {Cai, Li and Zhu, Yangyong},
  year = {2015},
  month = may,
  volume = {14},
  number = {0},
  pages = {2},
  publisher = {Ubiquity Press},
  issn = {1683-1470},
  urldate = {2023-11-25},
  abstract = {High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms.},
  langid = {american},
  language = {en-US}
}

@inproceedings{caldarolaImprovingGeneralizationFederated2022,
  title = {Improving {{Generalization}} in~{{Federated Learning}} by~{{Seeking Flat Minima}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Caldarola, Debora and Caputo, Barbara and Ciccone, Marco},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  pages = {654--672},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  abstract = {Models trained in federated settings often suffer from degraded performances and fail at generalizing, especially when facing heterogeneous scenarios. In this work, we investigate such behavior through the lens of geometry of the loss and Hessian eigenspectrum, linking the model's lack of generalization capacity to the sharpness of the solution. Motivated by prior studies connecting the sharpness of the loss surface and the generalization gap, we show that i) training clients locally with Sharpness-Aware Minimization (SAM) or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on the server-side can substantially improve generalization in Federated Learning and help bridging the gap with centralized models. By seeking parameters in neighborhoods having uniform low loss, the model converges towards flatter minima and its generalization significantly improves in both homogeneous and heterogeneous scenarios. Empirical results demonstrate the effectiveness of those optimizers across a variety of benchmark vision datasets (e.g. Cifar10/100, Landmarks-User-160k, Idda) and tasks (large scale classification, semantic segmentation, domain generalization).},
  isbn = {978-3-031-20050-2},
  langid = {english},
  language = {en}
}

@inproceedings{calmonOptimizedPreProcessingDiscrimination2017,
  title = {Optimized {{Pre-Processing}} for {{Discrimination Prevention}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-19},
  abstract = {Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.}
}

@inproceedings{camachoQualityQualityOut2023,
  title = {Quality {{In}} / {{Quality Out}}: {{Data}} Quality More Relevant than Model Choice in Anomaly Detection with the {{UGR}}'16},
  shorttitle = {Quality {{In}} / {{Quality Out}}},
  booktitle = {{{NOMS}} 2023-2023 {{IEEE}}/{{IFIP Network Operations}} and {{Management Symposium}}},
  author = {Camacho, Jos{\'e} and Wasielewska, Katarzyna and Espinosa, Pablo and {Fuentes-Garc{\'i}a}, Marta},
  year = {2023},
  month = may,
  pages = {1--5},
  issn = {2374-9709},
  urldate = {2024-01-21},
  abstract = {Autonomous or self-driving networks are expected to provide a solution to the myriad of extremely demanding new applications with minimal human supervision. For this purpose, the community relies on the development of new Machine Learning (ML) models and techniques. However, ML can only be as good as the data it is fitted with, and data quality is an elusive concept difficult to assess. In this paper, we show that relatively minor modifications on a benchmark dataset (UGR'16, a flow-based real-traffic dataset for anomaly detection) cause significantly more impact on model performance than the specific ML technique considered. We also show that the measured model performance is uncertain, as a result of labelling inaccuracies. Our findings illustrate that the widely adopted approach of comparing a set of models in terms of performance results (e.g., in terms of accuracy or ROC curves) may lead to incorrect conclusions when done without a proper understanding of dataset biases and sensitivity. We contribute a methodology to interpret a model response that can be useful for this understanding.}
}

@inproceedings{canonacoAdaptiveFederatedLearning2021,
  title = {Adaptive {{Federated Learning}} in {{Presence}} of {{Concept Drift}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Canonaco, Giuseppe and Bergamasco, Alex and Mongelluzzo, Alessio and Roveri, Manuel},
  year = {2021},
  month = jul,
  pages = {1--7},
  issn = {2161-4407},
  urldate = {2024-04-25},
  abstract = {Federated Learning (FL) is a promising research area in the machine learning field. Techniques and solutions belonging to this area operate in distributed scenarios, comprising a server and pervasively distributed clients, aiming at learning a single central model without sending (possibly sensitive) data from the clients to the server. Such an approach allows mitigating the privacy concerns that are nowadays perceived as relevant in distributed machine learning solutions leveraging data belonging to different users or companies. The literature in the field of FL is wide and many state-of-the-art solutions are available. Unfortunately, all these solutions assume (implicitly or explicitly) that the process generating the data is stationary (hence not changing its statistical behavior over time); an assumption that rarely holds in real-world conditions where concept drift occurs due to, e.g., seasonality or periodicity effects, faults in sensors or actuators or changes in the users' behaviour. In this paper, we introduce, for the first time in the literature, a novel FL algorithm called Adaptive-FedAVG, able to operate with nonstationary data generating processes affected by concept drifts. Following a passive approach, Adaptive-FedAVG is able to increase the accuracy in stationary conditions and promptly react to concept drift by adapting the learning rate to increase the plasticity of the learning phase. A wide experimental campaign shows the effectiveness of the proposed Adaptive-FedAVG algorithm by comparing it with a state-of-the-art FL algorithm present in the literature both in stationary and non-stationary conditions.},
  keywords = {Actuators,Collaborative work,Companies,Data privacy,Distributed databases,Machine learning,Neural networks}
}

@inproceedings{caoUnderstandingDistributedPoisoning2019,
  title = {Understanding {{Distributed Poisoning Attack}} in {{Federated Learning}}},
  booktitle = {2019 {{IEEE}} 25th {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{ICPADS}})},
  author = {Cao, Di and Chang, Shan and Lin, Zhijian and Liu, Guohua and Sun, Donghong},
  year = {2019},
  month = dec,
  pages = {233--239},
  publisher = {IEEE},
  address = {Tianjin, China},
  urldate = {2024-04-06},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-72812-583-1}
}

@inproceedings{carliniExtractingTrainingData2021,
  title = {Extracting {{Training Data}} from {{Large Language Models}}},
  booktitle = {30th {{USENIX Security Symposium}} ({{USENIX Security}} 21)},
  author = {Carlini, Nicholas and Tram{\`e}r, Florian and Wallace, Eric and Jagielski, Matthew and {Herbert-Voss}, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, {\'U}lfar and Oprea, Alina and Raffel, Colin},
  year = {2021},
  pages = {2633--2650},
  urldate = {2024-02-24},
  isbn = {978-1-939133-24-3},
  langid = {english},
  language = {en}
}

@inproceedings{carliniExtractingTrainingData2023,
  title = {Extracting {{Training Data}} from {{Diffusion Models}}},
  booktitle = {32nd {{USENIX Security Symposium}} ({{USENIX Security}} 23)},
  author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tram{\`e}r, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
  year = {2023},
  pages = {5253--5270},
  abstract = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
  langid = {english},
  language = {en}
}

@article{castelnovoClarificationNuancesFairness2022,
  title = {A Clarification of the Nuances in the Fairness Metrics Landscape},
  author = {Castelnovo, Alessandro and Crupi, Riccardo and Greco, Greta and Regoli, Daniele and Penco, Ilaria Giuseppina and Cosentini, Andrea Claudio},
  year = {2022},
  month = mar,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {4209},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  urldate = {2024-04-28},
  abstract = {In recent years, the problem of addressing fairness in machine learning (ML) and automatic decision making has attracted a lot of attention in the scientific communities dealing with artificial intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a ``fair decision'' in situations impacting individuals in the population. The precise differences, implications and ``orthogonality'' between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions.},
  copyright = {2022 The Author(s)},
  langid = {english},
  language = {en},
  keywords = {Computer science,Statistics}
}

@article{chaiDataManagementMachine2023,
  title = {Data {{Management}} for {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Data {{Management}} for {{Machine Learning}}},
  author = {Chai, Chengliang and Wang, Jiayi and Luo, Yuyu and Niu, Zeping and Li, Guoliang},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {5},
  pages = {4646--4667},
  issn = {1558-2191},
  urldate = {2023-11-30},
  abstract = {Machine learning (ML) has widespread applications and has revolutionized many industries, but suffers from several challenges. First, sufficient high-quality training data is inevitable for producing a well-performed model, but the data is always human expensive to acquire. Second, a large amount of training data and complicated model structures lead to the inefficiency of training and inference. Third, given an ML task, one always needs to train lots of models, which are hard to manage in real applications. Fortunately, database techniques can benefit ML by addressing the above three challenges. In this paper, we review existing studies from the following three aspects along with the pipeline highly related to ML. (1) Data preparation (Pre-ML): it focuses on preparing high-quality training data that can improve the performance of the ML model, where we review data discovery, data cleaning and data labeling. (2) Model training \& inference (In-ML): researchers in ML community focus on improving the model performance during training, while in this survey we mainly study how to accelerate the entire training process, also including feature selection and model selection. (3) Model management (Post-ML): in this part, we survey how to store, query, deploy and debug the models after training. Finally, we provide research challenges and future directions.}
}

@article{chaiDeepLearningIrregularly2020,
  title = {Deep Learning for Irregularly and Regularly Missing Data Reconstruction},
  author = {Chai, Xintao and Gu, Hanming and Li, Feng and Duan, Hongyou and Hu, Xiaobo and Lin, Kai},
  year = {2020},
  month = feb,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {3302},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  urldate = {2023-10-21},
  abstract = {Deep learning (DL) is a powerful tool for mining features from data, which can theoretically avoid assumptions (e.g., linear events) constraining conventional interpolation methods. Motivated by this and inspired by image-to-image translation, we applied DL to irregularly and regularly missing data reconstruction with the aim of transforming incomplete data into corresponding complete data. To accomplish this, we established a model architecture with randomly sampled data as input and corresponding complete data as output, which was based on an encoder-decoder-style U-Net convolutional neural network. We carefully prepared the training data using synthetic and field seismic data. We used a mean-squared-error loss function and an Adam optimizer to train the network. We displayed the feature maps for a randomly sampled data set going through the trained model with the aim of explaining how the missing data are reconstructed. We benchmarked the method on several typical datasets for irregularly missing data reconstruction, which achieved better performances compared with a peer-reviewed Fourier transform interpolation method, verifying the effectiveness, superiority, and generalization capability of our approach. Because regularly missing is a special case of irregularly missing, we successfully applied the model to regularly missing data reconstruction, although it was trained with irregularly sampled data only.},
  copyright = {2020 The Author(s)},
  langid = {english},
  language = {en},
  keywords = {Geophysics,Seismology}
}

@misc{changFLASHFederatedLearning2024,
  title = {{{FLASH}}: {{Federated Learning Across Simultaneous Heterogeneities}}},
  shorttitle = {{{FLASH}}},
  author = {Chang, Xiangyu and Ahmed, Sk Miraj and Krishnamurthy, Srikanth V. and Guler, Basak and Swami, Ananthram and Oymak, Samet and {Roy-Chowdhury}, Amit K.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08769},
  eprint = {2402.08769},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-11},
  abstract = {The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified manner. To do so, FLASH models the learning dynamics through contextual multi-armed bandits (CMAB) and dynamically selects the most promising clients. Through extensive experiments, we demonstrate that FLASH achieves substantial and consistent improvements over state-of-the-art baselines -- as much as 10\% in absolute accuracy -- thanks to its unified approach. Importantly, FLASH also outperforms federated aggregation methods that are designed to handle highly heterogeneous settings and even enjoys a performance boost when integrated with them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning}
}

@article{changGeneralizationBoundsRegularization2011,
  title = {Generalization Bounds of Regularization Algorithms Derived Simultaneously through Hypothesis Space Complexity, Algorithmic Stability and Data Quality},
  author = {Chang, Xiangyu and Xu, Zongben and Zou, Bin and Zhang, Hai},
  year = {2011},
  month = jul,
  journal = {International Journal of Wavelets, Multiresolution and Information Processing},
  volume = {09},
  number = {04},
  pages = {549--570},
  publisher = {World Scientific Publishing Co.},
  issn = {0219-6913},
  urldate = {2023-12-01},
  abstract = {A main issue in machine learning research is to analyze the generalization performance of a learning machine. Most classical results on the generalization performance of regularization algorithms are derived merely with the complexity of hypothesis space or the stability property of a learning algorithm. However, in practical applications, the performance of a learning algorithm is not actually affected only by an unitary factor just like the complexity of hypothesis space, stability of the algorithm and data quality. Therefore, in this paper, we develop a framework of evaluating the generalization performance of regularization algorithms combinatively in terms of hypothesis space complexity, algorithmic stability and data quality. We establish new bounds on the learning rate of regularization algorithms based on the measure of uniform stability and empirical covering number for general type of loss functions. As applications of the generic results, we evaluate the learning rates of support vector machines and regularization networks, and propose a new strategy for regularization parameter setting.},
  keywords = {algorithmic stability,hypothesis space,Learning rate,regularization algorithm,regularization error,sample error}
}

@misc{chaudhariEntropySGDBiasingGradient2017,
  title = {Entropy-{{SGD}}: {{Biasing Gradient Descent Into Wide Valleys}}},
  shorttitle = {Entropy-{{SGD}}},
  author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  year = {2017},
  month = apr,
  number = {arXiv:1611.01838},
  eprint = {1611.01838},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-15},
  abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{chaudhariEntropySGDBiasingGradient2019,
  title = {Entropy-{{SGD}}: Biasing Gradient Descent into Wide Valleys*},
  shorttitle = {Entropy-{{SGD}}},
  author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  year = {2019},
  month = dec,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2019},
  number = {12},
  pages = {124018},
  publisher = {{IOP Publishing and SISSA}},
  issn = {1742-5468},
  urldate = {2024-05-27},
  abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  langid = {english},
  language = {en}
}

@article{chenAsynchronousFederatedLearning2021,
  title = {Towards Asynchronous Federated Learning for Heterogeneous Edge-Powered Internet of Things},
  author = {Chen, Zheyi and Liao, Weixian and Hua, Kun and Lu, Chao and Yu, Wei},
  year = {2021},
  month = aug,
  journal = {Digital Communications and Networks},
  volume = {7},
  number = {3},
  pages = {317--326},
  issn = {2352-8648},
  urldate = {2023-10-02},
  abstract = {The advancement of the Internet of Things (IoT) brings new opportunities for collecting real-time data and deploying machine learning models. Nonetheless, an individual IoT device may not have adequate computing resources to train and deploy an entire learning model. At the same time, transmitting continuous real-time data to a central server with high computing resource incurs enormous communication costs and raises issues in data security and privacy. Federated learning, a distributed machine learning framework, is a promising solution to train machine learning models with resource-limited devices and edge servers. Yet, the majority of existing works assume an impractically synchronous parameter update manner with homogeneous IoT nodes under stable communication connections. In this paper, we develop an asynchronous federated learning scheme to improve training efficiency for heterogeneous IoT devices under unstable communication network. Particularly, we formulate an asynchronous federated learning model and develop a lightweight node selection algorithm to carry out learning tasks effectively. The proposed algorithm iteratively selects heterogeneous IoT nodes to participate in the global learning aggregation while considering their local computing resource and communication condition. Extensive experimental results demonstrate that our proposed asynchronous federated learning scheme outperforms the state-of-the-art schemes in various settings on independent and identically distributed (i.i.d.) and non-i.i.d. data distribution.},
  keywords = {Asynchronous federated learning,Internet of Things (IoT),Mobile edge computing}
}

@inproceedings{chenBootstrapGeneralizationAbility2023,
  title = {Bootstrap {{Generalization Ability}} from~{{Loss Landscape Perspective}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022 {{Workshops}}},
  author = {Chen, Huanran and Shao, Shitong and Wang, Ziyi and Shang, Zirui and Chen, Jin and Ji, Xiaofeng and Wu, Xinxiao},
  editor = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
  year = {2023},
  pages = {500--517},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  abstract = {Domain generalization aims to learn a model that can generalize well on the unseen test dataset, i.e., out-of-distribution data, which has different distribution from the training dataset. To address domain generalization in computer vision, we introduce the loss landscape theory into this field. Specifically, we bootstrap the generalization ability of the deep learning model from the loss landscape perspective in four aspects, including backbone, regularization, training paradigm, and learning rate. We verify the proposed theory on the NICO++, PACS, and VLCS datasets by doing extensive ablation studies as well as visualizations. In addition, we apply this theory in the ECCV 2022 NICO Challenge1 and achieve the 3rd place without using any domain invariant methods.},
  isbn = {978-3-031-25075-0},
  langid = {english},
  language = {en},
  keywords = {Adaptive learning rate scheduler,Distillation-based fine-tuning paradigm,Domain generalization,Loss landscape,Wide-PyramidNet}
}

@article{chenDataEvaluationEnhancement2021,
  title = {Data {{Evaluation}} and {{Enhancement}} for {{Quality Improvement}} of {{Machine Learning}}},
  author = {Chen, Haihua and Chen, Jiangping and Ding, Junhua},
  year = {2021},
  month = jun,
  journal = {IEEE Transactions on Reliability},
  volume = {70},
  number = {2},
  pages = {831--847},
  issn = {1558-1721},
  urldate = {2023-12-01},
  abstract = {Poor data quality has a direct impact on the performance of the machine learning system that is built on the data. As a demonstrated effective approach for data quality improvement, transfer learning has been widely used to improve machine learning quality. However, the ``quality improvement'' brought by transfer learning was rarely rigorously validated, and some of the quality improvement results were misleading. This article first exposed the hidden quality problem in the datasets used to build a machine learning system for normalizing medical concepts in social media text. The system was claimed to have achieved the best performance compared to existing work on a machine learning task. However, the results of our experiments showed that the ``best performance'' was due to the poor quality of the datasets and the defective validation process. To address the data quality issue and build a high-performance medical concept normalization system, we developed a transfer-learning-based strategy for data quality enhancement and system performance improvement. The results of the experiments showed a strong correlation between the quality of the datasets and the performance of the machine learning system. The results also demonstrated that a rigorous evaluation of data quality is necessary for guiding the quality improvement of machine learning. Therefore, we propose a data quality evaluation framework that includes the quality criteria and their corresponding evaluation approaches. The data validation process, the performance improvement strategy, and the data quality evaluation framework discussed in this article can be used for machine learning researchers and practitioners to build high-performance machine learning systems.}
}

@phdthesis{chenDataQualityEvaluation2022,
  title = {Data {{Quality Evaluation}} and {{Improvement}} for {{Machine Learning}}},
  author = {Chen, Haihua},
  year = {2022},
  address = {United States -- Texas},
  urldate = {2023-12-01},
  abstract = {In this research the focus is on data-centric AI with a specific concentration on data quality evaluation and improvement for machine learning. We first present a practical framework for data quality evaluation and improvement, using a legal domain as a case study and build a corpus for legal argument mining. We first created an initial corpus with 4,937 instances that were manually labeled. We define five data quality evaluation dimensions: comprehensiveness, correctness, variety, class imbalance, and duplication, and conducted a quantitative evaluation on these dimensions for the legal dataset and two existing datasets in the medical domain for medical concept normalization. The first group of experiments showed that class imbalance and insufficient training data are the two major data quality issues that negatively impacted the quality of the system that was built on the legal corpus. The second group of experiments showed that the overlap between the test datasets and the training datasets, which we defined as ``duplication,'' is the major data quality issue for the two medical corpora. We explore several widely used machine learning methods for data quality improvement. Compared to pseudo-labeling, co-training, and expectation-maximization (EM), generative adversarial network (GAN) is more effective for automated data augmentation, especially when a small portion of labeled data and a large amount of unlabeled data is available. The data validation process, the performance improvement strategy, and the machine learning framework for data evaluation and improvement discussed in this dissertation can be used by machine learning researchers and practitioners to build high-performance machine learning systems. All the materials including the data, code, and results will be released at: https://github.com/haihua0913/dissertation-dqei.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798352930380},
  langid = {english},
  language = {English},
  school = {University of North Texas},
  keywords = {Data augmentation,Data quality,Data-centric AI,Legal text classification,Machine learning,Medical concept normalization,Semi-supervised learning,Transfer learning}
}

@incollection{chenDealingLabelQuality2020,
  title = {Dealing with {{Label Quality Disparity}} in {{Federated Learning}}},
  booktitle = {Federated {{Learning}}: {{Privacy}} and {{Incentive}}},
  author = {Chen, Yiqiang and Yang, Xiaodong and Qin, Xin and Yu, Han and Chan, Piu and Shen, Zhiqi},
  editor = {Yang, Qiang and Fan, Lixin and Yu, Han},
  year = {2020},
  pages = {108--121},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-03-27},
  abstract = {Federated Learning (FL) is highly useful for the applications which suffer silo effect and privacy preserving, such as healthcare, finance, education, etc. Existing FL approaches generally do not account for disparities in the quality of local data labels. However, the participants tend to suffer from label noise due to annotators' varying skill-levels, biases or malicious tampering. In this chapter, we propose an alternative approach to address this challenge. It maintains a small set of benchmark samples on the FL coordinator and quantifies the credibility of the participants' local data without directly observing them by computing the mutual cross-entropy between performance of the FL model on the local datasets and that of the participant's local model on the benchmark dataset. Then, a credit-weighted orchestration is performed to adjust the weight assigned to participants in the FL model based on their credibility values. By experimentally evaluating on both synthetic data and real-world data, the results show that the proposed approach effectively identifies participants with noisy labels and reduces their impact on the FL model performance, thereby significantly outperforming existing FL approaches.},
  isbn = {978-3-030-63076-8},
  langid = {english},
  language = {en},
  keywords = {Credit-weighted,Federated Learning,Label quality}
}

@article{chenDistributionallyRobustLearning2020,
  title = {Distributionally {{Robust Learning}}},
  author = {Chen, Ruidi and Paschalidis, Ioannis Ch},
  year = {2020},
  month = dec,
  journal = {Foundations and Trends{\textregistered} in Optimization},
  volume = {4},
  number = {1-2},
  pages = {1--243},
  publisher = {Now Publishers, Inc.},
  issn = {2167-3888, 2167-3918},
  urldate = {2023-12-16},
  abstract = {Distributionally Robust Learning},
  langid = {english},
  language = {English}
}

@inproceedings{chenFederatedMultitaskLearning2020,
  title = {Federated {{Multi-task Learning}} with {{Hierarchical Attention}} for {{Sensor Data Analytics}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Chen, Yujing and Ning, Yue and Chai, Zheng and Rangwala, Huzefa},
  year = {2020},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  urldate = {2024-04-17},
  abstract = {The past decade has been marked by the rapid emergence and proliferation of a myriad of small devices, such as smartphones and wearables. There is a critical need for analysis of multivariate temporal data obtained from sensors on these devices. Given the heterogeneity of sensor data, individual devices may not have sufficient quality data to learn an effective model. Factors such as skewed/varied data distributions bring more difficulties to the sensor data analytics. In this paper, we propose to leverage multi-task learning with attention mechanism to perform inductive knowledge transfer among related devices and improve generalization performance. We design a novel federated multi-task hierarchical attention model (FATHOM) that jointly trains classification/regression models from multiple distributed devices. The attention mechanism in the proposed model seeks to extract feature representations from inputs and to learn a shared representation across multiple devices to identify key features at each time step. The underlying temporal and nonlinear relationships are modeled using a combination of attention mechanism and long short-term memory (LSTM) networks. The proposed method outperforms a wide range of competitive baselines in both classification and regression settings on three unbalanced real-world datasets. It also allows for the visual characterization of key features learned at the input task level and the global temporal level.},
  keywords = {Attention mechanism,Correlation,Data analysis,Data models,Feature extraction,Multitask learning,Performance evaluation,Predictive models,Sensor analytics,Task analysis}
}

@article{chenLearningDynamicsCoarse2023,
  title = {Learning Dynamics from Coarse/Noisy Data with Scalable Symbolic Regression},
  author = {Chen, Zhao and Wang, Nan},
  year = {2023},
  month = may,
  journal = {Mechanical Systems and Signal Processing},
  volume = {190},
  pages = {110147},
  issn = {0888-3270},
  urldate = {2024-05-11},
  abstract = {Distilling equations from data can provide insights into physics systems, helping validate theoretical modeling, infer unknown system properties, and explore indeterminate processes. Noisy or downsampled data have been a bottleneck limiting wide applications of symbolic regression, since identified equations are sensitively affected by data statistics. Coarse and noisy data can deteriorate equation reliability by magnifying errors in derivative estimation. While physics-informed surrogate models have been introduced in the literature to reconstruct augmented data that are high-resolution, less affected by noise, and more consistent with underlying physics mechanism (``physics-consistent'') for symbolic regression, dominant symbolic regression methods with data augmentation features are reluctant to consider wide function search space. This is due to the optimization burden in nontrivial function search, which is further exacerbated when dynamics exhibit chaotic or rapid oscillation. In this paper, a novel physics-informed equation learning method is proposed to address these issues. Specifically, leveraging a Fourier feature mapping enables a regular and accessible fully-connected neural network (the surrogate model) to learn dynamics with various frequency components. A neural-network-based symbolic model is improved to efficiently represent and separate function combinations in the form of polynomial series. Joint training of the surrogate model and the symbolic model enables ``physics-consistent'' data augmentation to the original low-quality data and lays the ground for a more reliable equation discovery. The proposed method is demonstrated by numerical and experimental systems parameterized by ordinary differential equations. Compared with baseline methods, such as sparse Bayesian learning and physics-informed neural network with dictionaries, it is found that the proposed method possesses evident competitive edges regarding optimization tractability for scalable function search. Codes will be publicly available upon publication.},
  keywords = {Equation discovery,Fourier feature embedding,Ordinary differential equations,Physics-informed neural network,Symbolic regression,System identification}
}

@book{chenLifelongMachineLearning2018,
  title = {Lifelong {{Machine Learning}}},
  author = {Chen, Zhiyuan and Liu, Bing},
  year = {2018},
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-04-27},
  abstract = {Its goal is to introduce this emerging ML paradigm and present a comprehensive survey and review of the important research results and ideas in the area.},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-031-00453-7 978-3-031-01581-6},
  langid = {english},
  language = {en}
}

@misc{chenPrivacyFairnessFederated2023,
  title = {Privacy and {{Fairness}} in {{Federated Learning}}: On the {{Perspective}} of {{Trade-off}}},
  shorttitle = {Privacy and {{Fairness}} in {{Federated Learning}}},
  author = {Chen, Huiqiang and Zhu, Tianqing and Zhang, Tao and Zhou, Wanlei and Yu, Philip S.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.14123},
  eprint = {2306.14123},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-27},
  abstract = {Federated learning (FL) has been a hot topic in recent years. Ever since it was introduced, researchers have endeavored to devise FL systems that protect privacy or ensure fair results, with most research focusing on one or the other. As two crucial ethical notions, the interactions between privacy and fairness are comparatively less studied. However, since privacy and fairness compete, considering each in isolation will inevitably come at the cost of the other. To provide a broad view of these two critical topics, we presented a detailed literature review of privacy and fairness issues, highlighting unique challenges posed by FL and solutions in federated settings. We further systematically surveyed different interactions between privacy and fairness, trying to reveal how privacy and fairness could affect each other and point out new research directions in fair and private FL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{chenRobustFederatedLearning2023,
  title = {Robust {{Federated Learning With Noisy Labeled Data Through Loss Function Correction}}},
  author = {Chen, Li and Ang, Fan and Chen, Yunfei and Wang, Weidong},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Network Science and Engineering},
  volume = {10},
  number = {3},
  pages = {1501--1511},
  issn = {2327-4697},
  urldate = {2024-04-12},
  abstract = {Federated learning (FL) is a communication-efficient machine learning paradigm to leverage distributed data at the network edge. Nevertheless, FL usually fails to train a high-quality model from the networks, where the edge nodes collect noisy labeled data. To tackle this challenge, this paper focuses on developing an innovative robust FL. We consider two kinds of networks with different data distribution. Firstly, we design a reweighted FL under a full-data network, where all edge nodes are equipped with both numerous noisy labeled dataset and small clean dataset. The key idea is that edge devices learn to assign the local weights of loss functions in noisy labeled dataset, and cooperate with central server to update global weights. Secondly, we consider a part-data network where some edge nodes exclude clean dataset, and can not compute the weights locally. The broadcasting of the global weights is added to help those edge nodes without clean dataset to reweight their noisy loss functions. Both designs have a convergence rate of {\textbackslash}mathcal O(1/T{\textasciicircum}2). Simulation results illustrate that the both proposed training processes improve the prediction accuracy due to the proper weights assignments of noisy loss function.},
  keywords = {Convergence,Data models,Distributed networks,federated learning,Federated learning,label noise,Loss measurement,machine learning,Noise measurement,non-convex optimization,parallel and distributed algorithms,robust design,Servers,Training}
}

@misc{chenTargetedBackdoorAttacks2017,
  title = {Targeted {{Backdoor Attacks}} on {{Deep Learning Systems Using Data Poisoning}}},
  author = {Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  year = {2017},
  month = dec,
  number = {arXiv:1712.05526},
  eprint = {1712.05526},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-25},
  abstract = {Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90\%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@book{chirikjianStochasticModelsInformation2009,
  title = {Stochastic Models, Information Theory, and Lie Groups, Volume 1: Classical Results and Geometric Methods},
  shorttitle = {Stochastic Models, Information Theory, and Lie Groups, Volume 1},
  author = {Chirikjian, Gregory S.},
  year = {2009},
  series = {Applied and {{Numerical Harmonic Analysis}}},
  publisher = {Birkh{\"a}user Boston Springer e-books},
  address = {Boston},
  abstract = {The subjects of stochastic processes, information theory, and Lie groups are usually treated separately from each other. This unique two-volume set presents these topics in a unified setting, thereby building bridges between fields that are rarely studied by the same people. Unlike the many excellent formal treatments available for each of these subjects individually, the emphasis in both of these volumes is on the use of stochastic, geometric, and group-theoretic concepts in the modeling of physical phenomena. Volume 1 establishes the geometric and statistical foundations required to understand the fundamentals of continuous-time stochastic processes, differential geometry, and the probabilistic foundations of information theory. Volume 2 delves deeper into relationships between these topics, including stochastic geometry, geometric aspects of the theory of communications and coding, multivariate statistical analysis, and error propagation on Lie groups. Key features and topics of Volume 1: * The author reviews stochastic processes and basic differential geometry in an accessible way for applied mathematicians, scientists, and engineers. * Extensive exercises and motivating examples make the work suitable as a textbook for use in courses that emphasize applied stochastic processes or differential geometry. * The concept of Lie groups as continuous sets of symmetry operations is introduced. * The Fokker-Planck Equation for diffusion processes in Euclidean space and on differentiable manifolds is derived in a way that can be understood by nonspecialists. * The concrete presentation style makes it easy for readers to obtain numerical solutions for their own problems; the emphasis is on how to calculate quantities rather than how to prove theorems. * A self-contained appendix provides a comprehensive review of concepts from linear algebra, multivariate calculus, and systems of ordinary differential equations. Stochastic Models, Information Theory, and Lie Groups will be of interest to advanced undergraduate and graduate students, researchers, and practitioners working in applied mathematics, the physical sciences, and engineering},
  isbn = {978-0-8176-4803-9},
  langid = {english},
  language = {eng},
  lccn = {519.2}
}

@book{chirikjianStochasticModelsInformation2009a,
  title = {Stochastic Models, Information Theory, and Lie Groups},
  author = {Chirikjian, Gregory S.},
  year = {2009},
  series = {Applied and Numerical Harmonic Analysis},
  publisher = {Birkh{\"a}user},
  address = {Boston},
  isbn = {978-0-8176-4802-2 978-0-8176-4943-2 978-0-8176-4944-9},
  lccn = {QA403 .C47 2009},
  keywords = {Harmonic analysis,Lie groups,Stochastic processes},
  annotation = {OCLC: ocn390725688}
}

@inproceedings{chizatLazyTrainingDifferentiable2019,
  title = {On {{Lazy Training}} in {{Differentiable Programming}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chizat, L{\'e}na{\"i}c and Oyallon, Edouard and Bach, Francis},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-09-20},
  abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this lazy training'' phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely thatlazy training'' is behind the many successes of neural networks in difficult high dimensional tasks.}
}

@misc{chizatLazyTrainingDifferentiable2020,
  title = {On {{Lazy Training}} in {{Differentiable Programming}}},
  author = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  year = {2020},
  month = jan,
  number = {arXiv:1812.07956},
  eprint = {1812.07956},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this "lazy training" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that "lazy training" is behind the many successes of neural networks in difficult high dimensional tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@article{choDataQualityMeasures2021,
  title = {Data {{Quality Measures}} and {{Efficient Evaluation Algorithms}} for {{Large-Scale High-Dimensional Data}}},
  author = {Cho, Hyeongmin and Lee, Sangkyun},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {2},
  pages = {472},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  urldate = {2023-11-14},
  abstract = {Machine learning has been proven to be effective in various application areas, such as object and speech recognition on mobile systems. Since a critical key to machine learning success is the availability of large training data, many datasets are being disclosed and published online. From a data consumer or manager point of view, measuring data quality is an important first step in the learning process. We need to determine which datasets to use, update, and maintain. However, not many practical ways to measure data quality are available today, especially when it comes to large-scale high-dimensional data, such as images and videos. This paper proposes two data quality measures that can compute class separability and in-class variability, the two important aspects of data quality, for a given dataset. Classical data quality measures tend to focus only on class separability; however, we suggest that in-class variability is another important data quality factor. We provide efficient algorithms to compute our quality measures based on random projections and bootstrapping with statistical benefits on large-scale high-dimensional data. In experiments, we show that our measures are compatible with classical measures on small-scale data and can be computed much more efficiently on large-scale high-dimensional datasets.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {bootstrapping,data quality,high-dimensionality,large-scale,linear discriminant analysis,random projection}
}

@article{choGeneralizedRunsTests2011,
  title = {Generalized Runs Tests for the {{IID}} Hypothesis},
  author = {Cho, Jin Seo and White, Halbert},
  year = {2011},
  month = jun,
  journal = {Journal of Econometrics},
  volume = {162},
  number = {2},
  pages = {326--344},
  issn = {0304-4076},
  urldate = {2023-11-25},
  abstract = {We provide a family of tests for the IID hypothesis based on generalized runs, powerful against unspecified alternatives, providing a useful complement to tests designed for specific alternatives, such as serial correlation, GARCH, or structural breaks. Our tests have appealing computational simplicity in that they do not require kernel density estimation, with the associated challenge of bandwidth selection. Simulations show levels close to nominal asymptotic levels. Our tests have power against both dependent and heterogeneous alternatives, as both theory and simulations demonstrate.},
  keywords = {Dependence,Gaussian process,Geometric distribution,IID condition,Runs test,Structural break}
}

@inproceedings{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Choromanska, Anna and Henaff, {\relax Mi}kael and Mathieu, Michael and Arous, Gerard Ben and LeCun, Yann},
  year = {2015},
  month = feb,
  pages = {192--204},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-06-22},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  langid = {english},
  language = {en}
}

@inproceedings{chuprovReputationTrustModels2020,
  title = {Reputation and {{Trust Models}} with {{Data Quality Metrics}} for {{Improving Autonomous Vehicles Traffic Security}} and {{Safety}}},
  booktitle = {2020 {{IEEE Systems Security Symposium}} ({{SSS}})},
  author = {Chuprov, Sergey and Viksnin, Ilya and Kim, Iuliia and Reznikand, Leon and Khokhlov, Igor},
  year = {2020},
  month = jul,
  pages = {1--8},
  urldate = {2023-11-15},
  abstract = {In this paper, we develop, implement, test, and analyze a novel technique that allows to improve the security and safety of intersections crossing by the group of autonomous vehicles. The proposed approach is based on augmenting the trust and reputation models with data quality evaluation and using it for initial trust value assignment. This technique allows increasing accuracy and recall rates in detecting agents that might supply incorrect data and facilitating their removal from the agent group consideration. To evaluate the proposed method, we performed the simulation study of the autonomous vehicle traffic control through an intersection. The conducted experiments showed that the employment of data quality metrics improves detecting autonomous vehicles and other agents that might transmit incorrect data.}
}

@inproceedings{coifmanHARMONICANALYTICGEOMETRY2019,
  title = {{{HARMONIC ANALYTIC GEOMETRY ON SUBSETS IN HIGH DIMENSIONS}} - {{EMPIRICAL MODELS}}},
  booktitle = {Proceedings of the {{International Congress}} of {{Mathematicians}} ({{ICM}} 2018)},
  author = {Coifman, Ronald R.},
  year = {2019},
  month = may,
  pages = {391--424},
  publisher = {WORLD SCIENTIFIC},
  address = {Rio de Janeiro, Brazil},
  urldate = {2024-05-18},
  abstract = {We describe a recent evolution of Harmonic Analysis to generate analytic tools for the joint organization of the geometry of subsets of Rn and the analysis of functions and operators on the subsets. In this analysis we establish a duality between the geometry of functions and the geometry of the space. The methods are used to automate various analytic organizations, as well as to enable informative data analysis. These tools extend to higher order tensors, to combine dynamic analysis of changing structures.},
  isbn = {978-981-327-287-3 978-981-327-288-0},
  langid = {english},
  language = {en}
}

@article{combesConvergencePropertiesDeep2018,
  title = {Convergence {{Properties}} of {{Deep Neural Networks}} on {{Separable Data}}},
  author = {des Combes, Remi Tachet and Pezeshki, Mohammad and Shabanian, Samira and Courville, Aaron and Bengio, Yoshua},
  year = {2018},
  month = sep,
  urldate = {2024-05-11},
  abstract = {While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.},
  langid = {english},
  language = {en}
}

@book{ConvexityOptimizationBanach,
  title = {Convexity and {{Optimization}} in {{Banach Spaces}}},
  urldate = {2024-04-20},
  langid = {english},
  language = {en}
}

@misc{cooperLossLandscapeOverparameterized2018,
  title = {The Loss Landscape of Overparameterized Neural Networks},
  author = {Cooper, Y.},
  year = {2018},
  month = apr,
  number = {arXiv:1804.10200},
  eprint = {1804.10200},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-07},
  abstract = {We explore some mathematical features of the loss landscape of overparameterized neural networks. A priori one might imagine that the loss function looks like a typical function from \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$ to \${\textbackslash}mathbb\{R\}\$ - in particular, nonconvex, with discrete global minima. In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function. If a neural net has \$n\$ parameters and is trained on \$d\$ data points, with \$n{$>$}d\$, we show that the locus \$M\$ of global minima of \$L\$ is usually not discrete, but rather an \$n-d\$ dimensional submanifold of \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$. In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that \$M\$ is typically a very high-dimensional subset of \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{coppensOrderedRiskMinimization2023,
  title = {Ordered {{Risk Minimization}}: {{Learning More}} from {{Less Data}}},
  shorttitle = {Ordered {{Risk Minimization}}},
  author = {Coppens, Peter and Patrinos, Panagiotis},
  year = {2023},
  month = sep,
  number = {arXiv:2303.09196},
  eprint = {2303.09196},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2023-12-03},
  abstract = {We consider the worst-case expectation of a permutation invariant ambiguity set of discrete distributions as a proxy-cost for data-driven expected risk minimization. For this framework, we coin the term ordered risk minimization to highlight how results from order statistics inspired the proxy-cost. Specifically, we show how such costs serve as point-wise high-confidence upper bounds of the expected risk. The confidence level can be determined tightly for any sample size. Conversely we also illustrate how to calibrate the size of the ambiguity set such that the high-confidence upper bound has some user specified confidence. This calibration procedure notably supports \${\textbackslash}phi\$-divergence based ambiguity sets. Numerical experiments then illustrate how the resulting scheme both generalizes better and is less sensitive to tuning parameters compared to the empirical risk minimization approach.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control}
}

@inproceedings{corbett-daviesAlgorithmicDecisionMaking2017,
  title = {Algorithmic {{Decision Making}} and the {{Cost}} of {{Fairness}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {{Corbett-Davies}, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
  year = {2017},
  month = aug,
  series = {{{KDD}} '17},
  pages = {797--806},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-01-06},
  abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
  isbn = {978-1-4503-4887-4},
  keywords = {algorithmic fairness,discrimination,disparate impact,pretrial detention,risk assessment}
}

@misc{corinziaVariationalFederatedMultiTask2021,
  title = {Variational {{Federated Multi-Task Learning}}},
  author = {Corinzia, Luca and Beuret, Ami and Buhmann, Joachim M.},
  year = {2021},
  month = feb,
  number = {arXiv:1906.06268},
  eprint = {1906.06268},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-17},
  abstract = {In federated learning, a central server coordinates the training of a single model on a massively distributed network of devices. This setting can be naturally extended to a multi-task learning framework, to handle real-world federated datasets that typically show strong statistical heterogeneity among devices. Despite federated multi-task learning being shown to be an effective paradigm for real-world datasets, it has been applied only on convex models. In this work, we introduce VIRTUAL, an algorithm for federated multi-task learning for general non-convex models. In VIRTUAL the federated network of the server and the clients is treated as a star-shaped Bayesian network, and learning is performed on the network using approximated variational inference. We show that this method is effective on real-world federated datasets, outperforming the current state-of-the-art for federated learning, and concurrently allowing sparser gradient updates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{correaSmoothnessMetricProjection2018,
  title = {Smoothness of the Metric Projection onto Nonconvex Bodies in {{Hilbert}} Spaces},
  author = {Correa, Rafael and Salas, David and Thibault, Lionel},
  year = {2018},
  month = jan,
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {457},
  number = {2},
  pages = {1307--1332},
  issn = {0022247X},
  urldate = {2024-09-28},
  langid = {english},
  language = {en},
  keywords = {Distance function,Metric projection,Nonconvex body,Normal cone,Prox-regular set,Submanifold}
}

@inproceedings{cortesLimitsLearningMachine1994,
  title = {Limits on {{Learning Machine Accuracy Imposed}} by {{Data Quality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cortes, Corinna and Jackel, L. D. and Chiang, Wan-Ping},
  year = {1994},
  volume = {7},
  publisher = {MIT Press},
  urldate = {2023-10-25},
  abstract = {Random  errors  and  insufficiencies  in  databases  limit  the  perfor(cid:173) mance  of any classifier  trained  from  and applied  to the  database.  In this paper we  propose a method to estimate the limiting perfor(cid:173) mance of classifiers imposed by the database.  We demonstrate this  technique  on  the  task  of predicting  failure  in  telecommunication  paths.}
}

@article{costaDatadrivenDistributionallyRobust2022,
  title = {Data-Driven Distributionally Robust Risk Parity Portfolio Optimization},
  author = {Costa, Giorgio and Kwon, Roy H.},
  year = {2022},
  month = sep,
  journal = {Optimization Methods and Software},
  volume = {37},
  number = {5},
  pages = {1876--1911},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  urldate = {2023-12-03},
  abstract = {We propose a distributionally robust formulation of the traditional risk parity portfolio optimization problem. Distributional robustness is introduced by targeting the discrete probabilities attached to each observation used during parameter estimation. Instead of assuming that all observations are equally likely, we consider an ambiguity set that provides us with the flexibility to find the most adversarial probability distribution based on the investor's desired degree of robustness. This allows us to derive robust estimates to parametrize the distribution of asset returns without having to impose any particular structure on the data. The resulting distributionally robust optimization problem is a constrained convex--concave minimax problem. Our approach is financially meaningful and attempts to attain full risk diversification with respect to the worst-case instance of the portfolio risk measure. We propose a novel algorithmic approach to solve this minimax problem, which blends projected gradient ascent with sequential convex programming. This algorithm is highly flexible and allows the user to choose among alternative statistical distance measures to define the ambiguity set. Moreover, the algorithm is highly tractable and scalable. Our numerical experiments suggest that a distributionally robust risk parity portfolio can yield a higher risk-adjusted rate of return when compared against the nominal portfolio.},
  keywords = {distributionally robust optimization,gradient descent,Portfolio selection,risk parity,saddle-point problem,statistical ambiguity}
}

@book{cramerSecureMultipartyComputation2015,
  title = {Secure Multiparty Computation and Secret Sharing},
  author = {Cramer, Ronald and Damg{\aa}rd, I. B. and Nielsen, Jesper Buus},
  year = {2015},
  publisher = {Cambridge University Press},
  address = {Cambridge ; New York},
  isbn = {978-1-107-04305-3},
  lccn = {TK5105.59 .C685 2015},
  keywords = {Computer network protocols,Computer networks,Computer security,Information theory,Security measures}
}

@inproceedings{crammerLearningDataVariable2005,
  title = {Learning from {{Data}} of {{Variable Quality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Crammer, Koby and Kearns, Michael and Wortman, Jennifer},
  year = {2005},
  volume = {18},
  publisher = {MIT Press},
  urldate = {2023-11-29},
  abstract = {We initiate the study of learning from multiple sources of limited data, each of which may be corrupted at a different rate. We develop a com- plete theory of which data sources should be used for two fundamental problems: estimating the bias of a coin, and learning a classifier in the presence of label noise. In both cases, efficient algorithms are provided for computing the optimal subset of data.}
}

@misc{cummingsDetectingDatasetDrift2023,
  title = {Detecting {{Dataset Drift}} and {{Non-IID Sampling}} via k-{{Nearest Neighbors}}},
  author = {Cummings, Jesse and Snorrason, El{\'i}as and Mueller, Jonas},
  year = {2023},
  month = may,
  number = {arXiv:2305.15696},
  eprint = {2305.15696},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-25},
  abstract = {We present a straightforward statistical test to detect certain violations of the assumption that the data are Independent and Identically Distributed (IID). The specific form of violation considered is common across real-world applications: whether the examples are ordered in the dataset such that almost adjacent examples tend to have more similar feature values (e.g. due to distributional drift, or attractive interactions between datapoints). Based on a k-Nearest Neighbors estimate, our approach can be used to audit any multivariate numeric data as well as other data types (image, text, audio, etc.) that can be numerically represented, perhaps with model embeddings. Compared with existing methods to detect drift or auto-correlation, our approach is both applicable to more types of data and also able to detect a wider variety of IID violations in practice. Code: https://github.com/cleanlab/cleanlab},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{damaskinosAsynchronousByzantineMachine2018,
  title = {Asynchronous {{Byzantine Machine Learning}} (the Case of {{SGD}})},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Damaskinos, Georgios and Mhamdi, El Mahdi El and Guerraoui, Rachid and Patra, Rhicheek and Taziki, Mahsa},
  year = {2018},
  month = jul,
  pages = {1145--1154},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-06},
  abstract = {Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce Kardam, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against 1/3 Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than f/n, where f is the number of Byzantine failures tolerated and n the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.},
  langid = {english},
  language = {en}
}

@inproceedings{dandiImplicitGradientAlignment2022,
  title = {Implicit {{Gradient Alignment}} in {{Distributed}} and {{Federated Learning}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Dandi, Yatin and Barba, Luis and Jaggi, Martin},
  year = {2022},
  month = jun,
  volume = {36},
  pages = {6454--6462},
  urldate = {2024-05-12},
  abstract = {A major obstacle to achieving global convergence in distributed and federated learning is the misalignment of gradients across clients or mini-batches due to heterogeneity and stochasticity of the distributed data. In this work, we show that data heterogeneity can in fact be exploited to improve generalization performance through implicit regularization. One way to alleviate the effects of heterogeneity is to encourage the alignment of gradients across different clients throughout training. Our analysis reveals that this goal can be accomplished by utilizing the right optimization method that replicates the implicit regularization effect of SGD, leading to gradient alignment as well as improvements in test accuracies. Since the existence of this regularization in SGD completely relies on the sequential use of different mini-batches during training, it is inherently absent when training with large mini-batches. To obtain the generalization benefits of this regularization while increasing parallelism, we propose a novel GradAlign algorithm that induces the same implicit regularization while allowing the use of arbitrarily large batches in each update. We experimentally validate the benefits of our algorithm in different distributed and federated learning settings.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {Machine Learning (ML)}
}

@incollection{danilovDataQualityEstimation2023,
  title = {Data {{Quality Estimation Via Model Performance}}: {{Machine Learning}} as a {{Validation Tool}}},
  shorttitle = {Data {{Quality Estimation Via Model Performance}}},
  booktitle = {Healthcare {{Transformation}} with {{Informatics}} and {{Artificial Intelligence}}},
  author = {Danilov, Gleb and Kotik, Konstantin and Shifrin, Michael and Strunina, Yulia and Pronkina, Tatiana and Tsukanova, Tatiana and Nepomnyashiy, Vladimir and Konovalov, Nikolay and Danilov, Valeriy and Potapov, Alexander},
  year = {2023},
  pages = {369--372},
  publisher = {IOS Press},
  urldate = {2024-01-21}
}

@inproceedings{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-06-25},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.}
}

@misc{dauphinNeglectedHessianComponent2024,
  title = {Neglected {{Hessian}} Component Explains Mysteries in {{Sharpness}} Regularization},
  author = {Dauphin, Yann N. and Agarwala, Atish and Mobahi, Hossein},
  year = {2024},
  month = jan,
  number = {arXiv:2401.10809},
  eprint = {2401.10809},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-17},
  abstract = {Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties. This equivalence relies on the assumption that the NME can be ignored, which we find does not hold for modern networks since they involve significant feature learning. We find that regularizing feature exploitation but not feature exploration yields performance similar to gradient penalties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{dehghaniDistributedMachineDistributed2023,
  title = {From Distributed Machine to Distributed Deep Learning: A Comprehensive Survey},
  shorttitle = {From Distributed Machine to Distributed Deep Learning},
  author = {Dehghani, Mohammad and Yazdanparast, Zahra},
  year = {2023},
  month = oct,
  journal = {Journal of Big Data},
  volume = {10},
  number = {1},
  pages = {158},
  issn = {2196-1115},
  urldate = {2024-01-01},
  abstract = {Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.},
  keywords = {Artificial intelligence,Data-parallelism,Distributed deep learning,Distributed machine learning,Ditributed reinforcement learning,Machine learning,Model-parallelism}
}

@inproceedings{dengDistributionallyRobustFederated2020,
  title = {Distributionally {{Robust Federated Averaging}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
  year = {2020},
  volume = {33},
  pages = {15111--15122},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-12-16},
  abstract = {In this paper, we study communication efficient distributed algorithms for distributionally robust federated learning via periodic averaging with adaptive sampling. In contrast to standard empirical risk minimization,  due to the minimax structure of the underlying optimization problem, a key difficulty arises from the fact that the global parameter that controls the mixture of local losses can only be updated infrequently on the global stage. To compensate for this, we propose a Distributionally Robust Federated Averaging (DRFA) algorithm that employs a novel snapshotting scheme to approximate the accumulation of history gradients of the mixing parameter. We analyze the convergence rate of DRFA in both convex-linear and nonconvex-linear settings. We also generalize the proposed idea to objectives with regularization on the mixture parameter and propose a  proximal variant, dubbed as DRFA-Prox, with provable convergence rates. We also analyze an alternative optimization method for regularized case in strongly-convex-strongly-concave and non-convex (under PL condition)-strongly-concave settings. To the best of our knowledge, this paper is the first to solve distributionally robust federated learning with reduced communication, and to analyze the efficiency of local descent methods on distributed minimax problems. We give corroborating experimental evidence for our theoretical results in federated learning settings.}
}

@inproceedings{dengFAIRQualityAwareFederated2021,
  title = {{{FAIR}}: {{Quality-Aware Federated Learning}} with {{Precise User Incentive}} and {{Model Aggregation}}},
  shorttitle = {{{FAIR}}},
  booktitle = {{{IEEE INFOCOM}} 2021 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Deng, Yongheng and Lyu, Feng and Ren, Ju and Chen, Yi-Chao and Yang, Peng and Zhou, Yuezhi and Zhang, Yaoxue},
  year = {2021},
  month = may,
  pages = {1--10},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  urldate = {2024-02-23},
  abstract = {Federated learning enables distributed learning in a privacy-protected manner, but two challenging reasons can affect learning performance significantly. First, mobile users are not willing to participate in learning due to computation and energy consumption. Second, with various factors (e.g., training data size/quality), the model update quality of mobile devices can vary dramatically, inclusively aggregating low-quality model updates can deteriorate the global model quality. In this paper, we propose a novel system named FAIR, i.e., Federated leArning with qualIty awaReness. FAIR integrates three major components: 1) learning quality estimation: we leverage historical learning records to estimate the user learning quality, where the record freshness is considered and the exponential forgetting function is utilized for weight assignment; 2) quality-aware incentive mechanism: within the recruiting budget, we model a reverse auction problem to encourage the participation of high-quality learning users, and the method is proved to be truthful, individually rational, and computationally efficient; and 3) model aggregation: we devise an aggregation algorithm that integrates the model quality into aggregation and filters out non-ideal model updates, to further optimize the global learning model. Based on real-world datasets and practical learning tasks, extensive experiments are carried out to demonstrate the efficacy of FAIR.},
  isbn = {978-1-66540-325-2},
  langid = {english},
  language = {en}
}

@article{dengMNISTDatabaseHandwritten2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}} [{{Best}} of the {{Web}}]},
  author = {Deng, Li},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  urldate = {2024-01-24},
  abstract = {In this issue, ``Best of the Web'' presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  keywords = {Machine learning}
}

@inproceedings{diaoHeteroFLComputationCommunication2020,
  title = {{{HeteroFL}}: {{Computation}} and {{Communication Efficient Federated Learning}} for {{Heterogeneous Clients}}},
  shorttitle = {{{HeteroFL}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  year = {2020},
  month = oct,
  urldate = {2024-04-20},
  abstract = {Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient.},
  langid = {english},
  language = {en}
}

@inproceedings{diazDataQualityMetrics2022,
  title = {Data {{Quality Metrics}} for {{Unlabelled Datasets}}},
  booktitle = {2022 {{IEEE}} 4th {{International Conference}} on {{BioInspired Processing}} ({{BIP}})},
  author = {D{\'i}az, Catalina and {Calderon-Ramirez}, Saul and Aguilar, Luis Diego Mora},
  year = {2022},
  month = nov,
  pages = {1--7},
  urldate = {2023-11-14},
  abstract = {Deep learning models usually need extensive amounts of data, and these data have to be labeled, becoming a concern when dealing with real-world applications. It is known that labeling a dataset is a costly task in time, money, and resource-wise. Consequently, Semi-supervised Learning Model (SSLM) approach comes into the picture as it uses labeled and unlabeled datasets to train a model, practice that is useful in improving the overall performance of the models. The unlabeled datasets may include out-of-distribution data or inside-of-distribution data points, which may affect the model's accuracy and future predictions. This investigation proposes a metric that can be useful to determine how much the unlabeled dataset can or cannot affect the accuracy of the SSLM. It also aims to demonstrate that the data quality metrics is a topic that needs further research, especially, when considering that the future of Deep learning models targets real-world applications such as healthcare. Concepts such as data quality metrics has been normally applied in structured data, however, it can also be applied in unstructured data (datasets used to train deep learning models). The method employed in this research takes the Mahalanobis distance as a base to generate a trend and then a metric. The approach follows what is demonstrated and proposed in [1], but uses the covariance matrices to compare the labeled and unlabeled datasets. The experimentation shows that the Mahalanobis distance generates results that are accordant to the proposed method, achieving a processing time lower by 99\%. Using the Pierson's correlation method the result was a hard negative correlation with the MixMatch results reported in [1].}
}

@misc{dingRoadmapEdgeAI2021,
  title = {Roadmap for {{Edge AI}}: {{A Dagstuhl Perspective}}},
  shorttitle = {Roadmap for {{Edge AI}}},
  author = {Ding, Aaron Yi and Peltonen, Ella and Meuser, Tobias and Aral, Atakan and Becker, Christian and Dustdar, Schahram and Hiessl, Thomas and Kranzlmuller, Dieter and Liyanage, Madhusanka and Magshudi, Setareh and Mohan, Nitinder and Ott, Joerg and Rellermeyer, Jan S. and Schulte, Stefan and Schulzrinne, Henning and Solmaz, Gurkan and Tarkoma, Sasu and Varghese, Blesson and Wolf, Lars},
  year = {2021},
  month = nov,
  number = {arXiv:2112.00616},
  eprint = {2112.00616},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-23},
  abstract = {Based on the collective input of Dagstuhl Seminar (21342), this paper presents a comprehensive discussion on AI methods and capabilities in the context of edge computing, referred as Edge AI. In a nutshell, we envision Edge AI to provide adaptation for data-driven applications, enhance network and radio access, and allow the creation, optimization, and deployment of distributed AI/ML pipelines with given quality of experience, trust, security and privacy targets. The Edge AI community investigates novel ML methods for the edge computing environment, spanning multiple sub-fields of computer science, engineering and ICT. The goal is to share an envisioned roadmap that can bring together key actors and enablers to further advance the domain of Edge AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,I.2.11}
}

@inproceedings{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  year = {2017},
  month = jul,
  pages = {1019--1028},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-27},
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g.{\textbackslash} Hochreiter {\textbackslash}\& Schmidhuber (1997); Keskar et al.{\textbackslash} (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Or, depending on the definition of flatness, it is the same for any given minimum. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  langid = {english},
  language = {en}
}

@inproceedings{dinurRevealingInformationPreserving2003,
  title = {Revealing Information While Preserving Privacy},
  booktitle = {Proceedings of the Twenty-Second {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Dinur, Irit and Nissim, Kobbi},
  year = {2003},
  month = jun,
  series = {{{PODS}} '03},
  pages = {202--210},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-03-11},
  abstract = {We examine the tradeoff between privacy and usability of statistical databases. We model a statistical database by an n-bit string d1,..,dn, with a query being a subset q {$\subseteq$} [n] to be answered by {$\Sigma$}i{$\varepsilon$}q di. Our main result is a polynomial reconstruction algorithm of data from noisy (perturbed) subset sums. Applying this reconstruction algorithm to statistical databases we show that in order to achieve privacy one has to add perturbation of magnitude ({\textohm}{\textsurd}n). That is, smaller perturbation always results in a strong violation of privacy. We show that this result is tight by exemplifying access algorithms for statistical databases that preserve privacy while adding perturbation of magnitude {\~O}({\textsurd}n).For time-T bounded adversaries we demonstrate a privacypreserving access algorithm whose perturbation magnitude is {$\approx$} {\textsurd}T.},
  isbn = {978-1-58113-670-8},
  keywords = {data reconstruction,integrity and security,subset-sums with noise}
}

@inproceedings{donahueFairnessModelsharingGames2023,
  title = {Fairness in Model-Sharing Games},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2023},
  author = {Donahue, Kate and Kleinberg, Jon},
  year = {2023},
  month = apr,
  series = {{{WWW}} '23},
  pages = {3775--3783},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-03-26},
  abstract = {In many real-world situations, data is distributed across multiple self-interested agents. These agents can collaborate to build a machine learning model based on data from multiple agents, potentially reducing the error each experiences. However, sharing models in this way raises questions of fairness: to what extent can the error experienced by one agent be significantly lower than the error experienced by another agent in the same coalition? In this work, we consider two notions of fairness that each may be appropriate in different circumstances: egalitarian fairness (which aims to bound how dissimilar error rates can be) and proportional fairness (which aims to reward players for contributing more data). We similarly consider two common methods of model aggregation, one where a single model is created for all agents (uniform), and one where an individualized model is created for each agent. For egalitarian fairness, we obtain a tight multiplicative bound on how widely error rates can diverge between agents collaborating (which holds for both aggregation methods). For proportional fairness, we show that the individualized aggregation method always gives a small player error that is upper bounded by proportionality. For uniform aggregation, we show that this upper bound is guaranteed for any individually rational coalition (where no player wishes to leave to do local learning).},
  isbn = {978-1-4503-9416-1},
  keywords = {fairness,federated learning,game theory}
}

@article{donahueModelsharingGamesAnalyzing2021,
  title = {Model-Sharing {{Games}}: {{Analyzing Federated Learning Under Voluntary Participation}}},
  shorttitle = {Model-Sharing {{Games}}},
  author = {Donahue, Kate and Kleinberg, Jon},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {6},
  pages = {5303--5311},
  issn = {2374-3468},
  urldate = {2024-03-27},
  abstract = {Federated learning is a setting where agents, each with access to their own data source, combine models learned from local data to create a global model. If agents are drawing their data from different distributions, though, federated learning might produce a biased global model that is not optimal for each agent. This means that agents face~a fundamental question: should they join the global model or stay with their local model? In this work, we show how this situation can be naturally analyzed through the framework of coalitional game theory.~ Motivated by these considerations, we propose the following game:~there are heterogeneous players with~different model parameters~governing their data distribution and different amounts of data they have noisily drawn from their own distribution. Each player's goal is to obtain a model with minimal expected mean squared error (MSE) on their own distribution. They have a choice of fitting a model based solely on their own data, or combining their learned parameters with those of some subset of the other players. Combining models reduces the variance component of their error through access to more data, but increases the bias because of the heterogeneity of distributions. In this work, we derive exact expected MSE values for problems in linear regression and mean estimation. We use these values to analyze the resulting game in the framework of hedonic game theory; we study how players might divide into coalitions, where each set of players within a coalition jointly constructs a single model.~ In a case with arbitrarily many players that each have either a "small" or "large" amount of data, we constructively show that there always exists a stable partition of players into coalitions.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {Distributed Machine Learning & Federated Learning}
}

@inproceedings{donahueOptimalityStabilityFederated2021,
  title = {Optimality and {{Stability}} in {{Federated Learning}}: {{A Game-theoretic Approach}}},
  shorttitle = {Optimality and {{Stability}} in {{Federated Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Donahue, Kate and Kleinberg, Jon},
  year = {2021},
  volume = {34},
  pages = {1287--1298},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-31},
  abstract = {Federated learning is a distributed learning paradigm where multiple agents, each only with access to local data, jointly learn a global model. There has recently been an explosion of research aiming not only to improve the accuracy rates of federated learning, but also provide certain guarantees around social good properties such as total error. One branch of this research has taken a game-theoretic approach, and in particular, prior work has viewed federated learning as a hedonic game, where error-minimizing players arrange themselves into federating coalitions. This past work proves the existence of stable coalition partitions, but leaves open a wide range of questions, including how far from optimal these stable solutions are. In this work, we motivate and define a notion of optimality given by the average error rates among federating agents (players). First, we provide and prove the correctness of an efficient algorithm to calculate an optimal (error minimizing) arrangement of players. Next, we analyze the relationship between the stability and optimality of an arrangement. First, we show that for some regions of parameter space, all stable arrangements are optimal (Price of Anarchy equal to 1). However, we show this is not true for all settings: there exist examples of stable arrangements with higher cost than optimal (Price of Anarchy greater than 1). Finally, we give the first constant-factor bound on the performance gap between stability and optimality, proving that the total error of the worst stable solution can be no higher than 9 times the total error of an optimal solution (Price of Anarchy bound of 9).}
}

@inproceedings{doniniEmpiricalRiskMinimization2018,
  title = {Empirical {{Risk Minimization Under Fairness Constraints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Donini, Michele and Oneto, Luca and {Ben-David}, Shai and {Shawe-Taylor}, John S and Pontil, Massimiliano},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-11-28},
  abstract = {We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our methodology. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.}
}

@article{dotsenkoStatisticalMechanicsTraining1994,
  title = {Statistical Mechanics of Training in Neural Networks},
  author = {Dotsenko, V. S. and Feldman, D. E.},
  year = {1994},
  month = nov,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {27},
  number = {21},
  pages = {L821},
  issn = {0305-4470},
  urldate = {2024-05-18},
  abstract = {A statistical mechanical approach for neural networks in which couplings are the slow dynamical variables is considered. The couplings are assumed to be confined in a restricted subspace near the Hebb-rule structure corresponding to quenched patterns. We study the situation when the couplings thermalize at a temperature different from that of the spin degrees of freedom, which makes it possible to treat the system in terms of the traditional replica approach with a finite number of replicas. The structure of the model is such that the effective evolution of the couplings tends to deepen the free energy minima corresponding to the learnt patterns. The phase diagram obtained exhibits a substantial increase of the retrieval region.},
  langid = {english},
  language = {en}
}

@inproceedings{draxlerEssentiallyNoBarriers2018,
  title = {Essentially {{No Barriers}} in {{Neural Network Energy Landscape}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  year = {2018},
  month = jul,
  pages = {1309--1318},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-07-10},
  abstract = {Training neural networks involves finding minima of a high-dimensional non-convex loss function. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that minima are perhaps best seen as points on a single connected manifold of low loss, rather than as the bottoms of distinct valleys.},
  langid = {english},
  language = {en}
}

@inproceedings{duanAstraeaSelfBalancingFederated2019,
  title = {Astraea: {{Self-Balancing Federated Learning}} for {{Improving Classification Accuracy}} of {{Mobile Deep Learning Applications}}},
  shorttitle = {Astraea},
  booktitle = {2019 {{IEEE}} 37th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Duan, Moming and Liu, Duo and Chen, Xianzhang and Tan, Yujuan and Ren, Jinting and Qiao, Lei and Liang, Liang},
  year = {2019},
  month = nov,
  pages = {246--254},
  publisher = {IEEE},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-11-16},
  isbn = {978-1-5386-6648-7}
}

@inproceedings{duanFederatedLearningDataAgnostic2023,
  title = {Federated {{Learning}} with {{Data-Agnostic Distribution Fusion}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Duan, Jian-Hui and Li, Wenzhong and Zou, Derun and Li, Ruichen and Lu, Sanglu},
  year = {2023},
  month = jun,
  pages = {8074--8083},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  urldate = {2023-12-16},
  abstract = {Federated learning has emerged as a promising distributed machine learning paradigm to preserve data privacy. One of the fundamental challenges of federated learning is that data samples across clients are usually not independent and identically distributed (non-IID), leading to slow convergence and severe performance drop of the aggregated global model. To facilitate model aggregation on non-IID data, it is desirable to infer the unknown global distributions without violating privacy protection policy. In this paper, we propose a novel data-agnostic distribution fusion based model aggregation method called FedFusion to optimize federated learning with nonIID local datasets, based on which the heterogeneous clients' data distributions can be represented by a global distribution of several virtual fusion components with different parameters and weights. We develop a Variational AutoEncoder (VAE) method to learn the optimal parameters of the distribution fusion components based on limited statistical information extracted from the local models, and apply the derived distribution fusion model to optimize federated model aggregation with non-IID data. Extensive experiments based on various federated learning scenarios with real-world datasets show that FedFusion achieves significant performance improvement compared to the state-of-the-art.},
  isbn = {9798350301298},
  langid = {english},
  language = {en}
}

@inproceedings{duBlockchainbasedDataQuality2023,
  title = {Blockchain-Based {{Data Quality Assessment}} to {{Improve Distributed Machine Learning}}},
  booktitle = {2023 {{International Conference}} on {{Computing}}, {{Networking}} and {{Communications}} ({{ICNC}})},
  author = {Du, Yao and Wang, Zehua and Leung, Cyril and Victor C.M., Leung},
  year = {2023},
  month = feb,
  pages = {170--175},
  urldate = {2023-11-14},
  abstract = {Data quality assessment is critical for distributed machine learning (DML). Data collected from heterogeneous Internet of things (IoT) devices may contain biased information that decreases the prediction accuracy of DML models. To address these challenges, we propose a blockchain-based approach to assess the quality of data that are not independent and identically distributed (non-IID). A blockchain running atop mobile edge computing (MEC) is helpful to protect privacy, security, and integrity of healthcare data when IoT devices are connected to MEC servers. Therefore, it is critical to integrate data quality assessment module on blockchain when building a blockchain-enabled DML system. In this paper, we jointly consider information loss and marginal utility of non-IID data samples. Specifically, we use Kullback-Leibler (KL) divergence to evaluate the information loss between IID and non-IID data samples and apply the reciprocal of data quantity to model the marginal utility of data samples. Human activities and handwritten digit recognition data sets are used for performance evaluations. Experiments show that our proposed scheme outperforms benchmarks regarding model test accuracy on various non-IID data samples.}
}

@inproceedings{duGradientDescentFinds2019,
  title = {Gradient {{Descent Finds Global Minima}} of {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  year = {2019},
  month = may,
  pages = {1675--1685},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-11},
  abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
  langid = {english},
  language = {en}
}

@inproceedings{dundarLearningClassifiersWhen,
  title = {Learning {{Classifiers When The Training Data Is Not IID}}},
  booktitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  author = {Dundar, Murat and Krishnapuram, Balaji and Bi, Jinbo and Rao, R Bharat},
  abstract = {Most methods for classifier design assume that the training samples are drawn independently and identically from an unknown data generating distribution, although this assumption is violated in several real life problems. Relaxing this i.i.d. assumption, we consider algorithms from the statistics literature for the more realistic situation where batches or sub-groups of training samples may have internal correlations, although the samples from different batches may be considered to be uncorrelated. Next, we propose simpler (more efficient) variants that scale well to large datasets; theoretical results from the literature are provided to support their validity. Experimental results from real-life computer aided diagnosis (CAD) problems indicate that relaxing the i.i.d. assumption leads to statistically significant improvements in the accuracy of the learned classifier. Surprisingly, the simpler algorithm proposed here is experimentally found to be even more accurate than the original version.},
  langid = {english},
  language = {en}
}

@inproceedings{duRethinkingNormalizationMethods2022,
  title = {Rethinking Normalization Methods in Federated Learning},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Distributed Machine Learning}}},
  author = {Du, Zhixu and Sun, Jingwei and Li, Ang and Chen, Pin-Yu and Zhang, Jianyi and Li, Hai "Helen" and Chen, Yiran},
  year = {2022},
  month = dec,
  series = {{{DistributedML}} '22},
  pages = {16--22},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-12-15},
  abstract = {Federated learning (FL) is a popular distributed learning framework that can reduce privacy risks by not explicitly sharing private data. In this work, we explicitly uncover external covariate shift problem in FL, which is caused by the independent local training processes on different devices. We demonstrate that external covariate shifts will lead to the obliteration of some devices' contributions to the global model. Further, we show that normalization layers are indispensable in FL since their inherited properties can alleviate the problem of obliterating some devices' contributions. However, recent works have shown that batch normalization, which is one of the standard components in many deep neural networks, will incur accuracy drop of the global model in FL. The essential reason for the failure of batch normalization in FL is poorly studied. We unveil that external covariate shift is the key reason why batch normalization is ineffective in FL. We also show that layer normalization is a better choice in FL which can mitigate the external covariate shift and improve the performance of the global model. We conduct experiments on CIFAR10 under non-IID settings. The results demonstrate that models with layer normalization converge fastest and achieve the best or comparable accuracy for three different model architectures.},
  isbn = {978-1-4503-9922-7},
  keywords = {batch normalization,federated learning,layer normalization}
}

@inproceedings{dworkDifferentialPrivacySurvey2008,
  title = {Differential {{Privacy}}: {{A Survey}} of {{Results}}},
  shorttitle = {Differential {{Privacy}}},
  booktitle = {Theory and {{Applications}} of {{Models}} of {{Computation}}},
  author = {Dwork, Cynthia},
  editor = {Agrawal, Manindra and Du, Dingzhu and Duan, Zhenhua and Li, Angsheng},
  year = {2008},
  pages = {1--19},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  abstract = {Over the past five years a new approach to privacy-preserving data analysis has born fruit [13, 18, 7, 19, 5, 37, 35, 8, 32]. This approach differs from much (but not all!) of the related literature in the statistics, databases, theory, and cryptography communities, in that a formal and ad omnia privacy guarantee is defined, and the data analysis techniques presented are rigorously proved to satisfy the guarantee. The key privacy guarantee that has emerged is differential privacy. Roughly speaking, this ensures that (almost, and quantifiably) no risk is incurred by joining a statistical database.},
  isbn = {978-3-540-79228-4},
  langid = {english},
  language = {en},
  keywords = {Differential Privacy,Privacy Mechanism,Statistical Database,Statistical Query,True Answer}
}

@inproceedings{dworkFairnessAwareness2012,
  title = {Fairness through Awareness},
  booktitle = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = {2012},
  month = jan,
  series = {{{ITCS}} '12},
  pages = {214--226},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-12-30},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  isbn = {978-1-4503-1115-1}
}

@inproceedings{el-mhamdiCollaborativeLearningJungle2021,
  title = {Collaborative {{Learning}} in the {{Jungle}} ({{Decentralized}}, {{Byzantine}}, {{Heterogeneous}}, {{Asynchronous}} and {{Nonconvex Learning}})},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{El-Mhamdi}, El Mahdi and Farhadkhani, Sadegh and Guerraoui, Rachid and Guirguis, Arsany and Hoang, L{\^e}-Nguy{\^e}n and Rouault, S{\'e}bastien},
  year = {2021},
  volume = {34},
  pages = {25044--25057},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-24}
}

@inproceedings{el-mhamdiGenuinelyDistributedByzantine2020,
  title = {Genuinely {{Distributed Byzantine Machine Learning}}},
  booktitle = {Proceedings of the 39th {{Symposium}} on {{Principles}} of {{Distributed Computing}}},
  author = {{El-Mhamdi}, El-Mahdi and Guerraoui, Rachid and Guirguis, Arsany and Hoang, L{\^e} Nguy{\^e}n and Rouault, S{\'e}bastien},
  year = {2020},
  month = jul,
  pages = {355--364},
  publisher = {ACM},
  address = {Virtual Event Italy},
  urldate = {2024-02-25},
  isbn = {978-1-4503-7582-5},
  langid = {english},
  language = {en}
}

@misc{el-mhamdiImpossibleSafetyLarge2023,
  title = {On the {{Impossible Safety}} of {{Large AI Models}}},
  author = {{El-Mhamdi}, El-Mahdi and Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Hoang, L{\^e}-Nguy{\^e}n and Pinot, Rafael and Rouault, S{\'e}bastien and Stephan, John},
  year = {2023},
  month = may,
  number = {arXiv:2209.15259},
  eprint = {2209.15259},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-25},
  abstract = {Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues. This paper systematizes our knowledge about the fundamental impossibility of building arbitrarily accurate and secure machine learning models. More precisely, we identify key challenging features of many of today's machine learning settings. Namely, high accuracy seems to require memorizing large training datasets, which are often user-generated and highly heterogeneous, with both sensitive information and fake users. We then survey statistical lower bounds that, we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{el-mhamdiStrategyproofnessGeometricMedian2023,
  title = {On the {{Strategyproofness}} of the {{Geometric Median}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {{El-Mhamdi}, El-Mahdi and Farhadkhani, Sadegh and Guerraoui, Rachid and Hoang, L{\^e}-Nguy{\^e}n},
  year = {2023},
  month = apr,
  pages = {2603--2640},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-06},
  abstract = {The geometric median, an instrumental component of the secure machine learning toolbox, is known to be effective when robustly aggregating models (or gradients), gathered from potentially malicious (or strategic) users. What is less known is the extent to which the geometric median incentivizes dishonest behaviors. This paper addresses this fundamental question by quantifying its strategyproofness. While we observe that the geometric median is not even approximately strategyproof, we prove that it is asymptotically \${\textbackslash}alpha\$-strategyproof: when the number of users is large enough, a user that misbehaves can gain at most a multiplicative factor \${\textbackslash}alpha\$, which we compute as a function of the distribution followed by the users. We then generalize our results to the case where users actually care more about specific dimensions, determining how this impacts \${\textbackslash}alpha\$. We also show how the skewed geometric medians can be used to improve strategyproofness.},
  langid = {english},
  language = {en}
}

@article{elgabliHarnessingWirelessChannels2021,
  title = {Harnessing {{Wireless Channels}} for {{Scalable}} and {{Privacy-Preserving Federated Learning}}},
  author = {Elgabli, Anis and Park, Jihong and Issaid, Chaouki Ben and Bennis, Mehdi},
  year = {2021},
  month = aug,
  journal = {IEEE Transactions on Communications},
  volume = {69},
  number = {8},
  pages = {5194--5208},
  issn = {1558-0857},
  urldate = {2024-04-11},
  abstract = {Wireless connectivity is instrumental in enabling scalable federated learning (FL), yet wireless channels bring challenges for model training, in which channel randomness perturbs each worker's model update while multiple workers' updates incur significant interference under limited bandwidth. To address these challenges, in this work we formulate a novel constrained optimization problem, and propose an FL framework harnessing wireless channel perturbations and interference for improving privacy, bandwidth-efficiency, and scalability. The resultant algorithm is coined analog federated ADMM (A-FADMM) based on analog transmissions and the alternating direction method of multipliers (ADMM). In A-FADMM, all workers upload their model updates to the parameter server (PS) using a single channel via analog transmissions, during which all models are perturbed and aggregated over-the-air. This not only saves communication bandwidth, but also hides each worker's exact model update trajectory from any eavesdropper including the honest-but-curious PS, thereby preserving data privacy against model inversion attacks. We formally prove the convergence and privacy guarantees of A-FADMM for convex functions under time-varying channels, and numerically show the effectiveness of A-FADMM under noisy channels and stochastic non-convex functions, in terms of convergence speed and scalability, as well as communication bandwidth and energy efficiency.},
  keywords = {Analog federated ADMM,Bandwidth,Convergence,Convex functions,digital federated ADMM,distributed machine learning,Perturbation methods,privacy,Privacy,time-varying channels,Time-varying channels,Wireless communication}
}

@article{elmestariPreservingDataPrivacy2024,
  title = {Preserving Data Privacy in Machine Learning Systems},
  author = {El Mestari, Soumia Zohra and Lenzini, Gabriele and Demirci, Huseyin},
  year = {2024},
  month = feb,
  journal = {Computers \& Security},
  volume = {137},
  pages = {103605},
  issn = {0167-4048},
  urldate = {2024-03-12},
  abstract = {The wide adoption of Machine Learning to solve a large set of real-life problems came with the need to collect and process large volumes of data, some of which are considered personal and sensitive, raising serious concerns about data protection. Privacy-enhancing technologies (PETs) are often indicated as a solution to protect personal data and to achieve a general trustworthiness as required by current EU regulations on data protection and AI. However, an off-the-shelf application of PETs is insufficient to ensure a high-quality of data protection, which one needs to understand. This work systematically discusses the risks against data protection in modern Machine Learning systems taking the original perspective of the data owners, who are those who hold the various data sets, data models, or both, throughout the machine learning life cycle and considering the different Machine Learning architectures. It argues that the origin of the threats, the risks against the data, and the level of protection offered by PETs depend on the data processing phase, the role of the parties involved, and the architecture where the machine learning systems are deployed. By offering a framework in which to discuss privacy and confidentiality risks for data owners and by identifying and assessing privacy-preserving countermeasures for machine learning, this work could facilitate the discussion about compliance with EU regulations and directives. We discuss current challenges and research questions that are still unsolved in the field. In this respect, this paper provides researchers and developers working on machine learning with a comprehensive body of knowledge to let them advance in the science of data protection in machine learning field as well as in closely related fields such as Artificial Intelligence.},
  keywords = {Differential privacy,Functional encryption,Homomorphic encryption,Machine learning,Privacy enhancing technologies,Privacy threats,Secure multiparty computation,Trustworthy machine learning}
}

@phdthesis{elsenerRegularizedEmpiricalRisk2019,
  type = {Doctoral {{Thesis}}},
  title = {Regularized {{Empirical Risk Minimization}} under {{Missing Observations}}},
  author = {Elsener, Andreas Luca},
  year = {2019},
  urldate = {2023-11-30},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  langid = {english},
  language = {en},
  school = {ETH Zurich},
  annotation = {Accepted: 2020-11-27T11:09:03Z}
}

@article{ezzeldinFairFedEnablingGroup2023,
  title = {{{FairFed}}: {{Enabling Group Fairness}} in {{Federated Learning}}},
  shorttitle = {{{FairFed}}},
  author = {Ezzeldin, Yahya H. and Yan, Shen and He, Chaoyang and Ferrara, Emilio and Avestimehr, A. Salman},
  year = {2023},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {6},
  pages = {7494--7502},
  issn = {2374-3468},
  urldate = {2024-02-11},
  abstract = {Training ML models which are fair across different demographic groups is of critical importance due to the increased integration of ML in crucial decision-making scenarios such as healthcare and recruitment. Federated learning has been viewed as a promising solution for collaboratively training machine learning models among multiple parties while maintaining their local data privacy. However, federated learning also poses new challenges in mitigating the potential bias against certain populations (e.g., demographic groups), as this typically requires centralized access to the sensitive information (e.g., race, gender) of each datapoint. Motivated by the importance and challenges of group fairness in federated learning, in this work, we propose FairFed, a novel algorithm for fairness-aware aggregation to enhance group fairness in federated learning. Our proposed approach is server-side and agnostic to the applied local debiasing thus allowing for flexible use of different local debiasing methods across clients. We evaluate FairFed empirically versus common baselines for fair ML and federated learning and demonstrate that it provides fairer models, particularly under highly heterogeneous data distributions across clients. We also demonstrate the benefits of FairFed in scenarios involving naturally distributed real-life data collected from different geographical locations or departments within an organization.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {Fairness & Equity,PEAI: Bias}
}

@inproceedings{fangLocalModelPoisoning2020,
  title = {Local Model Poisoning Attacks to Byzantine-Robust Federated Learning},
  booktitle = {Proceedings of the 29th {{USENIX Conference}} on {{Security Symposium}}},
  author = {Fang, Minghong and Cao, Xiaoyu and Jia, Jinyuan and Gong, Neil Zhenqiang},
  year = {2020},
  month = aug,
  series = {{{SEC}}'20},
  pages = {1623--1640},
  publisher = {USENIX Association},
  address = {USA},
  urldate = {2023-11-27},
  abstract = {In federated learning, multiple client devices jointly learn a machine learning model: each client device maintains a local model for its local training dataset, while a master device maintains a global model via aggregating the local models from the client devices. The machine learning community recently proposed several federated learning methods that were claimed to be robust against Byzantine failures (e.g., system failures, adversarial manipulations) of certain client devices. In this work, we perform the first systematic study on local model poisoning attacks to federated learning. We assume an attacker has compromised some client devices, and the attacker manipulates the local model parameters on the compromised client devices during the learning process such that the global model has a large testing error rate. We formulate our attacks as optimization problems and apply our attacks to four recent Byzantine-robust federated learning methods. Our empirical results on four real-world datasets show that our attacks can substantially increase the error rates of the models learnt by the federated learning methods that were claimed to be robust against Byzantine failures of some client devices. We generalize two defenses for data poisoning attacks to defend against our local model poisoning attacks. Our evaluation results show that one defense can effectively defend against our attacks in some cases, but the defenses are not effective enough in other cases, highlighting the need for new defenses against our local model poisoning attacks to federated learning.},
  isbn = {978-1-939133-17-5}
}

@article{fanJointTaskOffloading2023,
  title = {Joint {{Task Offloading}} and {{Resource Allocation}} for {{Quality-Aware Edge-Assisted Machine Learning Task Inference}}},
  author = {Fan, Wenhao and Chen, Zeyu and Hao, Zhibo and Wu, Fan and Liu, Yuan'an},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {72},
  number = {5},
  pages = {6739--6752},
  issn = {1939-9359},
  urldate = {2024-01-22},
  abstract = {Edge computing is essential to enhance delay-sensitive and computation-intensive machine learning (ML) task inference services. Quality of inference results, which is mainly impacted by the task data and ML models, is an important indicator impacting the system performance. In this paper, we consider a quality-aware edge-assisted ML task inference scenario and propose a resource management scheme to minimize the total task processing delay while guaranteeing the stability of all the task queues and the inference accuracy requirements of all the tasks. In our scheme, the task offloading, task data adjustment, computing resource allocation, and wireless channel allocation are jointly optimized. The Lyapunov optimization technique is adopted to transform the original optimization problem into a deterministic problem for each time slot. Considering the high complexity of the optimization problem, we design an algorithm that decomposes the problem into a task offloading and channel allocation (TOCA) sub-problem, a task data adjustment sub-problem, and a computing resource allocation sub-problem, and then solves them iteratively. A low-complexity heuristic algorithm is also designed to solve the TOCA sub-problem efficiently. Extensive simulations are conducted by varying different crucial parameters. The results demonstrate the superiority of our scheme in comparison with 4 other schemes.}
}

@incollection{fanRethinkingPrivacyPreserving2020,
  title = {Rethinking {{Privacy Preserving Deep Learning}}: {{How}} to {{Evaluate}} and {{Thwart Privacy Attacks}}},
  shorttitle = {Rethinking {{Privacy Preserving Deep Learning}}},
  booktitle = {Federated {{Learning}}: {{Privacy}} and {{Incentive}}},
  author = {Fan, Lixin and Ng, Kam Woh and Ju, Ce and Zhang, Tianyu and Liu, Chang and Chan, Chee Seng and Yang, Qiang},
  editor = {Yang, Qiang and Fan, Lixin and Yu, Han},
  year = {2020},
  pages = {32--50},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-03-27},
  abstract = {This chapter investigates capabilities of Privacy-Preserving Deep Learning (PPDL) mechanisms against various forms of privacy attacks. First, we propose to quantitatively measure the trade-off between model accuracy and privacy losses incurred by reconstruction, tracing and membership attacks. Second, a novel Secret Polarization Network (SPN) is proposed to thwart privacy attacks, which is highly competitive against existing PPDL methods. Extensive experiments showed that model accuracies are improved on average by 5--20\% compared with baseline mechanisms, in regimes where data privacy are satisfactorily protected.},
  isbn = {978-3-030-63076-8},
  langid = {english},
  language = {en},
  keywords = {Differential privacy,Federated learning,Privacy attack}
}

@inproceedings{farhadkhaniEquivalenceDataPoisoning2022,
  title = {An {{Equivalence Between Data Poisoning}} and {{Byzantine Gradient Attacks}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Farhadkhani, Sadegh and Guerraoui, Rachid and Hoang, L{\^e} Nguy{\^e}n and Villemaud, Oscar},
  year = {2022},
  month = jun,
  pages = {6284--6323},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-11-27},
  abstract = {To study the resilience of distributed learning, the ``Byzantine" literature considers a strong threat model where workers can report arbitrary gradients to the parameter server. Whereas this model helped obtain several fundamental results, it has sometimes been considered unrealistic, when the workers are mostly trustworthy machines. In this paper, we show a surprising equivalence between this model and data poisoning, a threat considered much more realistic. More specifically, we prove that every gradient attack can be reduced to data poisoning, in any personalized federated learning system with PAC guarantees (which we show are both desirable and realistic). This equivalence makes it possible to obtain new impossibility results on the resilience of any ``robust'' learning algorithm to data poisoning in highly heterogeneous applications, as corollaries of existing impossibility theorems on Byzantine machine learning. Moreover, using our equivalence, we derive a practical attack that we show (theoretically and empirically) can be very effective against classical personalized federated learning models.},
  langid = {english},
  language = {en}
}

@inproceedings{farhangInvestigatingGeneralizationControlling2022,
  title = {Investigating {{Generalization}} by {{Controlling Normalized Margin}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Farhang, Alexander R. and Bernstein, Jeremy D. and Tirumala, Kushal and Liu, Yang and Yue, Yisong},
  year = {2022},
  month = jun,
  pages = {6324--6336},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-21},
  abstract = {Weight norm {$\parallel$}w{$\parallel$}{\textbardbl}w{\textbardbl}{\textbackslash}{\textbar}w{\textbackslash}{\textbar} and margin {$\gamma\gamma\backslash$}gamma participate in learning theory via the normalized margin {$\gamma$}/{$\parallel$}w{$\parallel\gamma$}/{\textbardbl}w{\textbardbl}{\textbackslash}gamma/{\textbackslash}{\textbar}w{\textbackslash}{\textbar}. Since standard neural net optimizers do not control normalized margin, it is hard to test whether this quantity causally relates to generalization. This paper designs a series of experimental studies that explicitly control normalized margin and thereby tackle two central questions. First: does normalized margin always have a causal effect on generalization? The paper finds that no---networks can be produced where normalized margin has seemingly no relationship with generalization, counter to the theory of Bartlett et al. (2017). Second: does normalized margin ever have a causal effect on generalization? The paper finds that yes---in a standard training setup, test performance closely tracks normalized margin. The paper suggests a Gaussian process model as a promising explanation for this behavior.},
  langid = {english},
  language = {en}
}

@article{fenzaDataSetQuality2021,
  title = {Data Set Quality in {{Machine Learning}}: {{Consistency}} Measure Based on {{Group Decision Making}}},
  shorttitle = {Data Set Quality in {{Machine Learning}}},
  author = {Fenza, Giuseppe and Gallo, Mariacristina and Loia, Vincenzo and Orciuoli, Francesco and {Herrera-Viedma}, Enrique},
  year = {2021},
  month = jul,
  journal = {Applied Soft Computing},
  volume = {106},
  pages = {107366},
  issn = {15684946},
  urldate = {2023-11-14},
  abstract = {Performance of Machine Learning models heavily depends on the quality of the training dataset. Among others, the quality of training data relies on the consistency of the labels assigned to similar items. Indeed, the labels should be coherently assigned (or collected) by avoiding inconsistencies for increasing the performance of the machine learning model. This study focuses on evaluating training data consistency for machine learning algorithms dealing with ranking problems, i.e., the Learning to Rank methods (LTR). This work defines a training data consistency measure based on the consensus value introduced in Group Decision Making. It investigates the statistical relationship between the proposed consistency measure and the performance of a deep neural network implementing an LTR method. This measure could drive data filtering at the training stage and guide model update decisions. Experimentation reveals a strong correlation between the proposed consistency measure and the performance of the model.},
  langid = {english},
  language = {en},
  keywords = {important}
}

@inproceedings{ferraguigSurveyBiasMitigation2021,
  title = {Survey of {{Bias Mitigation}} in {{Federated Learning}}},
  booktitle = {Conf{\'e}rence Francophone d'informatique En {{Parall{\'e}lisme}}, {{Architecture}} et {{Syst{\`e}me}}},
  author = {Ferraguig, Lynda and Djebrouni, Yasmine and Bouchenak, Sara and Marangozova, Vania},
  year = {2021},
  month = jul,
  address = {Lyon (virtuel), France},
  urldate = {2024-04-08},
  abstract = {Federated Learning (FL) interestingly allows a set of participants to collectively resolve a machine learning problem in a decentralized and privacy preserving manner. However, data distribution and heterogeneity, that are inherent to FL, may induce and exacerbate the problem of bias, with its prejudicial consequences such as racial or sexist segregation, illegal actions, or reduced revenues. In this paper, we describe the problem of bias in Federated Learning, and provide a comparative review of existing approaches of FL bias mitigation, before discussing open challenges and interesting research directions.},
  keywords = {Bias,Federated Learning,Machine Learning}
}

@misc{feuerExploringDatasetScaleIndicators2023,
  title = {Exploring {{Dataset-Scale Indicators}} of {{Data Quality}}},
  author = {Feuer, Benjamin and Hegde, Chinmay},
  year = {2023},
  month = nov,
  number = {arXiv:2311.04016},
  eprint = {2311.04016},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-29},
  abstract = {Modern computer vision foundation models are trained on massive amounts of data, incurring large economic and environmental costs. Recent research has suggested that improving data quality can significantly reduce the need for data quantity. But what constitutes data quality in computer vision? We posit that the quality of a given dataset can be decomposed into distinct sample-level and dataset-level constituents, and that the former have been more extensively studied than the latter. We ablate the effects of two important dataset-level constituents: label set design, and class balance. By monitoring these constituents using key indicators we provide, researchers and practitioners can better anticipate model performance, measured in terms of its accuracy and robustness to distribution shifts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  pages = {1126--1135},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-17},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  langid = {english},
  language = {en}
}

@inproceedings{foretSharpnessawareMinimizationEfficiently2020,
  title = {Sharpness-Aware {{Minimization}} for {{Efficiently Improving Generalization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  year = {2020},
  month = oct,
  urldate = {2024-05-12},
  abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-\{10, 100\}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.},
  langid = {english},
  language = {en}
}

@misc{fortDeepEnsemblesLoss2019,
  title = {Deep {{Ensembles}}: {{A Loss Landscape Perspective}}},
  shorttitle = {Deep {{Ensembles}}},
  author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  year = {2019},
  publisher = {arXiv},
  urldate = {2024-06-23},
  abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@inproceedings{fortDeepLearningKernel2020,
  title = {Deep Learning versus Kernel Learning: An Empirical Study of Loss Landscape Geometry and the Time Evolution of the {{Neural Tangent Kernel}}},
  shorttitle = {Deep Learning versus Kernel Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  year = {2020},
  volume = {33},
  pages = {5850--5861},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-07},
  abstract = {In suitably initialized wide networks, small learning rates transform deep neural networks (DNNs) into neural tangent kernel (NTK) machines, whose training dynamics is well-approximated by a linear weight expansion of the network at initialization.  Standard training, however, diverges from its linearization in ways that are poorly understood. We study the relationship between the training dynamics of nonlinear deep networks, the geometry of the loss landscape, and the time evolution of a data-dependent NTK. We do so through a large-scale phenomenological analysis of training, synthesizing diverse measures characterizing loss landscape geometry and NTK dynamics. In multiple neural architectures and datasets, we find these diverse measures evolve in a highly correlated manner, revealing a universal picture of the deep learning process.  In this picture, deep network training exhibits a highly chaotic rapid initial transient that within 2 to 3 epochs determines the final linearly connected basin of low loss containing the end point of training. During this chaotic transient, the NTK changes rapidly, learning useful features from the training data that enables it to outperform the standard initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid chaotic transient, the NTK changes at constant velocity, and its performance matches that of full network training in 15{\textbackslash}\% to 45{\textbackslash}\% of training time. Overall, our analysis reveals a striking correlation between a diverse set of metrics over training time, governed by a rapid chaotic to stable transition in the first few epochs, that together poses challenges and opportunities for the development of more accurate theories of deep learning.}
}

@misc{fortEmergentPropertiesLocal2019,
  title = {Emergent Properties of the Local Geometry of Neural Loss Landscapes},
  author = {Fort, Stanislav and Ganguli, Surya},
  year = {2019},
  month = oct,
  number = {arXiv:1910.05929},
  eprint = {1910.05929},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-10},
  abstract = {The local geometry of high dimensional neural network loss landscapes can both challenge our cherished theoretical intuitions as well as dramatically impact the practical success of neural network training. Indeed recent works have observed 4 striking local properties of neural loss landscapes on classification tasks: (1) the landscape exhibits exactly C directions of high positive curvature, where C is the number of classes; (2) gradient directions are largely confined to this extremely low dimensional subspace of positive Hessian curvature, leaving the vast majority of directions in weight space unexplored; (3) gradient descent transiently explores intermediate regions of higher positive curvature before eventually finding flatter minima; (4) training can be successful even when confined to low dimensional random affine hyperplanes, as long as these hyperplanes intersect a Goldilocks zone of higher than average curvature. We develop a simple theoretical model of gradients and Hessians, justified by numerical experiments on architectures and datasets used in practice, that simultaneously accounts for all 4 of these surprising and seemingly unrelated properties. Our unified model provides conceptual insights into the emergence of these properties and makes connections with diverse topics in neural networks, random matrix theory, and spin glasses, including the neural tangent kernel, BBP phase transitions, and Derrida's random energy model.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{fortGoldilocksZoneBetter2019,
  title = {The {{Goldilocks Zone}}: {{Towards Better Understanding}} of {{Neural Network Loss Landscapes}}},
  shorttitle = {The {{Goldilocks Zone}}},
  author = {Fort, Stanislav and Scherlis, Adam},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {3574--3581},
  issn = {2374-3468},
  urldate = {2024-05-19},
  abstract = {We explore the loss landscape of fully-connected and convolutional neural networks using random, low-dimensional hyperplanes and hyperspheres. Evaluating the Hessian, H, of the loss function on these hypersurfaces, we observe 1) an unusual excess of the number of positive eigenvalues of H, and 2) a large value of Tr(H)/{\textbar}{\textbar}H{\textbar}{\textbar} at a well defined range of configuration space radii, corresponding to a thick, hollow, spherical shell we refer to as the Goldilocks zone. We observe this effect for fully-connected neural networks over a range of network widths and depths on MNIST and CIFAR-10 datasets with the ReLU and tanh non-linearities, and a similar effect for convolutional networks. Using our observations, we demonstrate a close connection between the Goldilocks zone, measures of local convexity/prevalence of positive curvature, and the suitability of a network initialization. We show that the high and stable accuracy reached when optimizing on random, low-dimensional hypersurfaces is directly related to the overlap between the hypersurface and the Goldilocks zone, and as a corollary demonstrate that the notion of intrinsic dimension is initialization-dependent. We note that common initialization techniques initialize neural networks in this particular region of unusually high convexity/prevalence of positive curvature, and offer a geometric intuition for their success. Furthermore, we demonstrate that initializing a neural network at a number of points and selecting for high measures of local convexity such as Tr(H)/{\textbar}{\textbar}H{\textbar}{\textbar}, number of positive eigenvalues of H, or low initial loss, leads to statistically significantly faster training on MNIST. Based on our observations, we hypothesize that the Goldilocks zone contains an unusually high density of suitable initialization configurations.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en}
}

@inproceedings{fortLargeScaleStructure2019,
  title = {Large {{Scale Structure}} of {{Neural Network Loss Landscapes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fort, Stanislav and Jastrzebski, Stanislaw},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-06-22}
}

@inproceedings{frankleLinearModeConnectivity2020,
  title = {Linear {{Mode Connectivity}} and the {{Lottery Ticket Hypothesis}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  year = {2020},
  month = nov,
  pages = {3259--3269},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-07-10},
  abstract = {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).},
  langid = {english},
  language = {en}
}

@inproceedings{freemanTopologyGeometryHalfRectified2022,
  title = {Topology and {{Geometry}} of {{Half-Rectified Network Optimization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Freeman, C. Daniel and Bruna, Joan},
  year = {2022},
  month = jul,
  urldate = {2024-05-28},
  abstract = {The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such approximation and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important folklore facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay. The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors.},
  langid = {english},
  language = {en}
}

@article{frenayClassificationPresenceLabel2014,
  title = {Classification in the {{Presence}} of {{Label Noise}}: {{A Survey}}},
  shorttitle = {Classification in the {{Presence}} of {{Label Noise}}},
  author = {Frenay, Benoit and Verleysen, Michel},
  year = {2014},
  month = may,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {5},
  pages = {845--869},
  issn = {2162-2388},
  urldate = {2024-01-23},
  abstract = {Label noise is an important issue in classification, with many potential negative consequences. For example, the accuracy of predictions may decrease, whereas the complexity of inferred models and the number of necessary training samples may increase. Many works in the literature have been devoted to the study of label noise and the development of techniques to deal with label noise. However, the field lacks a comprehensive survey on the different types of label noise, their consequences and the algorithms that consider label noise. This paper proposes to fill this gap. First, the definitions and sources of label noise are considered and a taxonomy of the types of label noise is proposed. Second, the potential consequences of label noise are discussed. Third, label noise-robust, label noise cleansing, and label noise-tolerant algorithms are reviewed. For each category of approaches, a short discussion is proposed to help the practitioner to choose the most suitable technique in its own particular field of application. Eventually, the design of experiments is also discussed, what may interest the researchers who would like to test their own algorithms. In this paper, label noise consists of mislabeled instances: no additional information is assumed to be available like e.g., confidences on labels.}
}

@phdthesis{fryeFindingCriticalGradientFlat2020,
  title = {Finding {{Critical}} and {{Gradient-Flat Points}} of {{Deep Neural Network Loss Functions}}},
  author = {Frye, Charles},
  year = {2020},
  copyright = {Charles Gearhart Frye},
  langid = {english},
  language = {en},
  school = {University of California, Berkeley}
}

@misc{fungMitigatingSybilsFederated2020,
  title = {Mitigating {{Sybils}} in {{Federated Learning Poisoning}}},
  author = {Fung, Clement and Yoon, Chris J. M. and Beschastnikh, Ivan},
  year = {2020},
  month = jul,
  number = {arXiv:1808.04866},
  eprint = {1808.04866},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-06},
  abstract = {Machine learning (ML) over distributed multi-party data is required for a variety of domains. Existing approaches, such as federated learning, collect the outputs computed by a group of devices at a central aggregator and run iterative algorithms to train a globally shared model. Unfortunately, such approaches are susceptible to a variety of attacks, including model poisoning, which is made substantially worse in the presence of sybils. In this paper we first evaluate the vulnerability of federated learning to sybil-based poisoning attacks. We then describe {\textbackslash}emph\{FoolsGold\}, a novel defense to this problem that identifies poisoning sybils based on the diversity of client updates in the distributed learning process. Unlike prior work, our system does not bound the expected number of attackers, requires no auxiliary information outside of the learning process, and makes fewer assumptions about clients and their data. In our evaluation we show that FoolsGold exceeds the capabilities of existing state of the art approaches to countering sybil-based label-flipping and backdoor poisoning attacks. Our results hold for different distributions of client data, varying poisoning targets, and various sybil strategies. Code can be found at: https://github.com/DistributedML/FoolsGold},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{gafniFederatedLearningSignal2022,
  title = {Federated {{Learning}}: {{A}} Signal Processing Perspective},
  shorttitle = {Federated {{Learning}}},
  author = {Gafni, Tomer and Shlezinger, Nir and Cohen, Kobi and Eldar, Yonina C. and Poor, H. Vincent},
  year = {2022},
  month = may,
  journal = {IEEE Signal Processing Magazine},
  volume = {39},
  number = {3},
  pages = {14--41},
  issn = {1558-0792},
  urldate = {2024-04-08},
  abstract = {The dramatic success of deep learning is largely due to the availability of data. Data samples are often acquired on edge devices, such as smartphones, vehicles, and sensors, and in some cases cannot be shared due to privacy considerations. Federated learning is an emerging machine learning paradigm for training models across multiple edge devices holding local data sets, without explicitly exchanging the data. Learning in a federated manner differs from conventional centralized machine learning and poses several core unique challenges and requirements, which are closely related to classical problems studied in the areas of signal processing and communications. Consequently, dedicated schemes derived from these areas are expected to play an important role in the success of federated learning and the transition of deep learning from the domain of centralized servers to mobile edge devices.},
  keywords = {Collaborative work,Data models,Data privacy,Deep learning,Sensors,Signal processing,Training data}
}

@misc{gallegosBiasFairnessLarge2023,
  title = {Bias and {{Fairness}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Bias and {{Fairness}} in {{Large Language Models}}},
  author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
  year = {2023},
  month = sep,
  number = {arXiv:2309.00770},
  eprint = {2309.00770},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-17},
  abstract = {Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@inproceedings{gambergerExperimentsNoiseLtering1999,
  title = {Experiments with Noise Ltering in a Medical Domain},
  booktitle = {Proceedings of the {{Sixteenth International Conference}} on {{Machine Learning}}                   \{(\vphantom\}{{ICML}}\vphantom\{\} 1999), {{Bled}}, {{Slovenia}}, {{June}} 27 - 30, 1999},
  author = {Gamberger, Dragan and Lavrac, Nada and Groselj, Ciril},
  year = {1999},
  pages = {143--151},
  publisher = {Morgan Kaufmann},
  abstract = {The paper presents a series of noise detection experiments in a medical problem of coronary artery disease diagnosis. The following algorithms for noise detection and elimination are tested: a saturation lter, a classi cation lter, a combined classi cation-saturation lter, and a consensus saturation lter. The distinguishing feature of the novel consensus saturation lter is its high reliability which is due to the multiple detection of potentially noisy examples. Reliable detection of noisy examples is important for the analysis of patient records in medical databases, as well as for the induction of rules from ltered data, representing genuine characteristics of the diagnostic domain. Medical evaluation in the problem of coronary artery disease diagnosis shows that the detected noisy examples are indeed noisy or non-typical class representatives.},
  langid = {english},
  language = {en}
}

@inproceedings{gambergerNoiseEliminationInductive1996,
  title = {Noise Elimination in Inductive Concept Learning: {{A}} Case Study in Medical Diagnosis},
  shorttitle = {Noise Elimination in Inductive Concept Learning},
  booktitle = {Algorithmic {{Learning Theory}}},
  author = {Gamberger, Dragan and Lavra{\v c}, Nada and D{\v z}eroski, Sa{\v s}o},
  editor = {Arikawa, Setsuo and Sharma, Arun K.},
  year = {1996},
  pages = {199--212},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  abstract = {Compression measures used in inductive learners, such as measures based on the MDL (Minimum Description Length) principle, provide a theoretically justified basis for grading candidate hypotheses. Compression-based induction is appropriate also for handling of noisy data. This paper shows that a simple compression measure can be used to detect noisy examples. A technique is proposed in which noisy examples are detected and eliminated from the training set, and a hypothesis is then built from the set of remaining examples. The separation of noise detection and hypothesis formation has the advantage that noisy examples do not influence hypothesis construction as opposed to most standard approaches to noise handling in which the learner typically tries to avoid overfitting the noisy example set. This noise elimination method is applied to a problem of early diagnosis of rheumatic diseases which is known to be a difficult problem, due both to its nature and to the imperfections in the dataset. The method is evaluated by applying the noise elimination algorithm in conjunction with the CN2 rule induction algorithm, and by comparing their performance to earlier results obtained by CN2 in this diagnostic domain.},
  isbn = {978-3-540-70719-6},
  langid = {english},
  language = {en},
  keywords = {Inductive Logic Programming,Likelihood Ratio Statistic,Minimum Description Length,Noise Detection,Rheumatic Disease}
}

@article{gaoRiskMinimizationPresence2016,
  title = {Risk {{Minimization}} in the {{Presence}} of {{Label Noise}}},
  author = {Gao, Wei and Wang, Lu and Li, Yu-Feng and Zhou, Zhi-Hua},
  year = {2016},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  issn = {2374-3468},
  urldate = {2023-12-01},
  abstract = {Matrix concentration inequalities have attracted much attention in diverse applications such as linear algebra, statistical estimation, combinatorial optimization, etc. In this paper, we present new Bernstein concentration inequalities depending only on the first moments of random matrices, whereas previous Bernstein inequalities are heavily relevant to the first and second moments. Based on those results, we analyze the empirical risk minimization in the presence of label noise. We find that many popular losses used in risk minimization can be decomposed into two parts, where the first part won't be affected and only the second part will be affected by noisy labels. We show that the influence of noisy labels on the second part can be reduced by our proposed LICS (Labeled Instance Centroid Smoothing) approach. The effectiveness of the LICS algorithm is justified both theoretically and empirically.},
  copyright = {Copyright (c)},
  langid = {english},
  language = {en}
}

@misc{gaoWhenDecentralizedOptimization2023,
  title = {When {{Decentralized Optimization Meets Federated Learning}}},
  author = {Gao, Hongchang and Thai, My T. and Wu, Jie},
  year = {2023},
  month = jun,
  number = {arXiv:2306.02570},
  eprint = {2306.02570},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2023-11-07},
  abstract = {Federated learning is a new learning paradigm for extracting knowledge from distributed data. Due to its favorable properties in preserving privacy and saving communication costs, it has been extensively studied and widely applied to numerous data analysis applications. However, most existing federated learning approaches concentrate on the centralized setting, which is vulnerable to a single-point failure. An alternative strategy for addressing this issue is the decentralized communication topology. In this article, we systematically investigate the challenges and opportunities when renovating decentralized optimization for federated learning. In particular, we discussed them from the model, data, and communication sides, respectively, which can deepen our understanding about decentralized federated learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@inproceedings{gargExperimentingZeroKnowledgeProofs2023,
  title = {Experimenting with {{Zero-Knowledge Proofs}} of {{Training}}},
  booktitle = {Proceedings of the 2023 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Garg, Sanjam and Goel, Aarushi and Jha, Somesh and Mahloujifar, Saeed and Mahmoody, Mohammad and Policharla, Guru-Vamsi and Wang, Mingyuan},
  year = {2023},
  month = nov,
  pages = {1880--1894},
  publisher = {ACM},
  address = {Copenhagen Denmark},
  urldate = {2024-04-07},
  isbn = {9798400700507},
  langid = {english},
  language = {en}
}

@inproceedings{garipovLossSurfacesMode2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-18},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood.  We show that the optima of these complex loss functions are in fact connected by simple curves, over which training and test accuracy are nearly constant.  We introduce a training procedure to discover these high-accuracy pathways between modes.  Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model.  We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on  CIFAR-10, CIFAR-100, and ImageNet.}
}

@article{geirhosShortcutLearningDeep2020,
  title = {Shortcut Learning in Deep Neural Networks},
  author = {Geirhos, Robert and link will open in a new tab {Link to external site}, this and {J{\"o}rn-Henrik}, Jacobsen and Michaelis, Claudio and link will open in a new tab {Link to external site}, this and Zemel, Richard and Wieland, Brendel and Matthias, Bethge and Wichmann, Felix A. and link will open in a new tab {Link to external site}, this},
  year = {2020},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {11},
  pages = {665--673},
  publisher = {Nature Publishing Group},
  address = {Basingstoke, United States},
  urldate = {2023-12-16},
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning's failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications. Deep learning has resulted in impressive achievements, but under what circumstances does it fail, and why? The authors propose that its failures are a consequence of shortcut learning, a common characteristic across biological and artificial systems in which strategies that appear to have solved a problem fail unexpectedly under different circumstances.},
  copyright = {{\copyright} Springer Nature Limited 2020.},
  langid = {english},
  language = {English},
  keywords = {Benchmarking,Deep learning,Machine learning,Neural network,Shortcut}
}

@article{ghoshEdgeCloudComputingIoT2020,
  title = {Edge-{{Cloud Computing}} for {{IoT Data Analytics}}: {{Embedding Intelligence}} in the {{Edge}} with {{Deep Learning}}},
  shorttitle = {Edge-{{Cloud Computing}} for {{IoT Data Analytics}}},
  author = {Ghosh, Ananda and Grolinger, Katarina},
  year = {2020},
  journal = {IEEE Transactions on Industrial Informatics},
  pages = {1--1},
  issn = {1551-3203, 1941-0050},
  urldate = {2023-09-23},
  langid = {english},
  language = {english}
}

@inproceedings{gilad-bachrachCryptoNetsApplyingNeural2016,
  title = {{{CryptoNets}}: {{Applying Neural Networks}} to {{Encrypted Data}} with {{High Throughput}} and {{Accuracy}}},
  shorttitle = {{{CryptoNets}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {{Gilad-Bachrach}, Ran and Dowlin, Nathan and Laine, Kim and Lauter, Kristin and Naehrig, Michael and Wernsing, John},
  year = {2016},
  month = jun,
  pages = {201--210},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-04-11},
  abstract = {Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99\% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions.},
  langid = {english},
  language = {en}
}

@inproceedings{girgisShuffledModelDifferential2021,
  title = {Shuffled {{Model}} of {{Differential Privacy}} in {{Federated Learning}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Girgis, Antonious and Data, Deepesh and Diggavi, Suhas and Kairouz, Peter and Suresh, Ananda Theertha},
  year = {2021},
  month = mar,
  pages = {2521--2529},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-05},
  abstract = {We consider a distributed empirical risk minimization (ERM) optimization problem with communication efficiency and privacy requirements, motivated by the federated learning (FL) framework. We propose a distributed communication-efficient and local differentially private stochastic gradient descent (CLDP-SGD) algorithm and analyze its communication, privacy, and convergence trade-offs. Since each iteration of the CLDP-SGD aggregates the client-side local gradients, we develop (optimal) communication-efficient schemes for mean estimation for several {$\ell$}p{$\ell$}p{\textbackslash}ell\_p spaces under local differential privacy (LDP). To overcome performance limitation of LDP, CLDP-SGD takes advantage of the inherent privacy amplification provided by client subsampling and data subsampling at each selected client (through SGD) as well as the recently developed shuffled model of privacy. For convex loss functions, we prove that the proposed CLDP-SGD algorithm matches the known lower bounds on the {\textbackslash}textit\{centralized\} private ERM while using a finite number of bits per iteration for each client, {\textbackslash}emph\{i.e.,\} effectively getting communication efficiency for ``free''. We also provide preliminary experimental results supporting the theory.},
  langid = {english},
  language = {en}
}

@inproceedings{goldbergerTrainingDeepNeuralnetworks2022,
  title = {Training Deep Neural-Networks Using a Noise Adaptation Layer},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Goldberger, Jacob and {Ben-Reuven}, Ehud},
  year = {2022},
  month = jul,
  urldate = {2024-04-25},
  abstract = {The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise and to estimate the correct label. In this study we present a neural-network approach that optimizes the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended to the case where the noisy labels are dependent on the features in addition to the correct labels. Experimental results demonstrate that this approach outperforms previous methods.},
  langid = {english},
  language = {en}
}

@article{gongDiversityMachineLearning2019,
  title = {Diversity in {{Machine Learning}}},
  author = {Gong, Zhiqiang and Zhong, Ping and Hu, Weidong},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {64323--64350},
  issn = {2169-3536},
  urldate = {2023-11-27},
  abstract = {Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a totally good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though diversity plays an important role in the machine learning process, there is no systematical analysis of the diversification in the machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work. Our analysis provides a deeper understanding of the diversity technology in machine learning tasks and hence can help design and learn more effective models for real-world applications.}
}

@article{gongSurveyDatasetQuality2023,
  title = {A Survey on Dataset Quality in Machine Learning},
  author = {Gong, Youdi and Liu, Guangzhen and Xue, Yunzhi and Li, Rui and Meng, Lingzhong},
  year = {2023},
  month = oct,
  journal = {Information and Software Technology},
  volume = {162},
  pages = {107268},
  issn = {0950-5849},
  urldate = {2024-01-21},
  abstract = {With the rise of big data, the quality of datasets has become a crucial factor affecting the performance of machine learning models. High-quality datasets are essential for the realization of data value. This survey article summarizes the research direction of dataset quality in machine learning, including the definition of related concepts, analysis of quality issues and risks, and a review of dataset quality dimensions and metrics throughout the dataset lifecycle and a review of dataset quality metrics analyzed from a dataset lifecycle perspective and summarized in literatures. Furthermore, this article introduces a comprehensive quality evaluation process, which includes a framework for dataset quality evaluation with dimensions and metrics, computation methods for quality metrics, and assessment models. These studies provide valuable guidance for evaluating dataset quality in the field of machine learning, which can help improve the accuracy, efficiency, and generalization ability of machine learning models, and promote the development and application of artificial intelligence technology.},
  keywords = {Dataset,Dataset quality,Machine Learning}
}

@misc{goodfellowQualitativelyCharacterizingNeural2015,
  title = {Qualitatively Characterizing Neural Network Optimization Problems},
  author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
  year = {2015},
  month = may,
  number = {arXiv:1412.6544},
  eprint = {1412.6544},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-10},
  abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{gopakumarLossLandscapeEngineering2023,
  title = {Loss Landscape Engineering via {{Data Regulation}} on {{PINNs}}},
  author = {Gopakumar, Vignesh and Pamela, Stanislas and Samaddar, Debasmita},
  year = {2023},
  month = jun,
  journal = {Machine Learning with Applications},
  volume = {12},
  pages = {100464},
  issn = {2666-8270},
  urldate = {2024-05-08},
  abstract = {Physics-Informed Neural Networks have shown unique utility in parameterising the solution of a well-defined partial differential equation using automatic differentiation and residual losses. Though they provide theoretical guarantees of convergence, in practice the required training regimes tend to be exacting and demanding. Through the course of this paper, we take a deep dive into understanding the loss landscapes associated with a PINN and how that offers some insight as to why PINNs are fundamentally hard to optimise for. We demonstrate how PINNs can be forced to converge better towards the solution, by way of feeding in sparse or coarse data as a regulator. The data regulates and morphs the topology of the loss landscape associated with the PINN to make it easily traversable for the minimiser. Data regulation of PINNs helps ease the optimisation required for convergence by invoking a hybrid unsupervised--supervised training approach, where the labelled data pushes the network towards the vicinity of the solution, and the unlabelled regime fine-tunes it to the solution.},
  keywords = {Loss landscape,Optimisation,Partial differential equations,Physics-Informed Neural Networks,Sparse regularisation}
}

@article{gorbanBlessingDimensionalityMathematical2018,
  title = {Blessing of Dimensionality: Mathematical Foundations of the Statistical Physics of Data},
  shorttitle = {Blessing of Dimensionality},
  author = {Gorban, A. N. and Tyukin, I. Y.},
  year = {2018},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {376},
  number = {2118},
  pages = {20170237},
  issn = {1364-503X, 1471-2962},
  urldate = {2024-07-13},
  abstract = {The concentrations of measure phenomena were discovered as the mathematical background to statistical mechanics at the end of the nineteenth/beginning of the twentieth century and have been explored in mathematics ever since. At the beginning of the twenty-first century, it became clear that the proper utilization of these phenomena in machine learning might transform the               curse of dimensionality               into the               blessing of dimensionality               . This paper summarizes recently discovered phenomena of measure concentration which drastically simplify some machine learning problems in high dimension, and allow us to correct legacy artificial intelligence systems. The classical concentration of measure theorems state that i.i.d. random points are concentrated in a thin layer near a surface (a sphere or equators of a sphere, an average or median-level set of energy or another Lipschitz function, etc.). The new               stochastic separation theorems               describe the thin structure of these thin layers: the random points are not only concentrated in a thin layer but are all linearly separable from the rest of the set, even for exponentially large random sets. The linear functionals for separation of points can be selected in the form of the linear Fisher's discriminant. All artificial intelligence systems make errors. Non-destructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behaviour by a simple and robust classifier. The stochastic separation theorems provide us with such classifiers and determine a non-iterative (one-shot) procedure for their construction.                          This article is part of the theme issue `Hilbert's sixth problem'.},
  langid = {english},
  language = {en},
  keywords = {ensemble equivalence,extreme points,Fisher's discriminant,linear separability,measure concentration}
}

@misc{gotmareUsingModeConnectivity2018,
  title = {Using {{Mode Connectivity}} for {{Loss Landscape Analysis}}},
  author = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  year = {2018},
  month = jun,
  number = {arXiv:1806.06977},
  eprint = {1806.06977},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-13},
  abstract = {Mode connectivity is a recently introduced framework that empirically establishes the connectedness of minima by finding a high accuracy curve between two independently trained models. To investigate the limits of this setup, we examine the efficacy of this technique in extreme cases where the input models are trained or initialized differently. We find that the procedure is resilient to such changes. Given this finding, we propose using the framework for analyzing loss surfaces and training trajectories more generally, and in this direction, study SGD with cosine annealing and restarts (SGDR). We report that while SGDR moves over barriers in its trajectory, propositions claiming that it converges to and escapes from multiple local minima are not substantiated by our empirical results.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{gowdaHighperformanceSymbolicnumericsMultiple2022,
  title = {High-Performance Symbolic-Numerics via Multiple Dispatch},
  author = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gw{\'o}{\'z}zd{\'z}, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},
  year = {2022},
  month = jan,
  journal = {ACM Communications in Computer Algebra},
  volume = {55},
  number = {3},
  pages = {92--96},
  issn = {1932-2240},
  urldate = {2024-06-09},
  abstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.}
}

@inproceedings{grgic-hlacaCaseProcessFairness2016,
  title = {The {{Case}} for {{Process Fairness}} in {{Learning}}: {{Feature Selection}} for {{Fair Decision Making}}},
  shorttitle = {The {{Case}} for {{Process Fairness}} in {{Learning}}},
  author = {{Grgic-Hlaca}, Nina and Zafar, M. B. and Gummadi, K. and Weller, Adrian},
  year = {2016},
  urldate = {2024-01-21},
  abstract = {Machine learning methods are increasingly being used to inform, or sometimes even directly to make, important decisions about humans. A number of recent works have focussed on the fairness of the outcomes of such decisions, particularly on avoiding decisions that affect users of different sensitive groups ( e.g. , race, gender) disparately. In this paper, we propose to consider the fairness of the process of decision making. Process fairness can be measured by estimating the degree to which people consider various features to be fair to use when making an important legal decision. We examine the task of predicting whether or not a prisoner is likely to commit a crime again once released by analyzing the dataset considered by ProPublica relating to the COMPAS system. We introduce new measures of people's discomfort with using various features, show how these measures can be estimated, and consider the effect of removing the uncomfortable features on prediction accuracy and on outcome fairness. Our empirical analysis suggests that process fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
  langid = {english},
  language = {en}
}

@inproceedings{grgic-hlacaDistributiveFairnessAlgorithmic2018,
  title = {Beyond Distributive Fairness in Algorithmic Decision Making: {{Feature}} Selection for Procedurally Fair Learning},
  shorttitle = {Beyond Distributive Fairness in Algorithmic Decision Making},
  booktitle = {32nd {{AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2018},
  author = {{Grgi{\'c}-Hla{\v c}a}, N. and Zafar, M.B. and Gummadi, K.P. and Weller, A.},
  year = {2018},
  pages = {51--60},
  abstract = {With widespread use of machine learning methods in numerous domains involving humans, several studies have raised questions about the potential for unfairness towards certain individuals or groups. A number of recent works have proposed methods to measure and eliminate unfairness from machine learning models. However, most of this work has focused on only one dimension of fair decision making: distributive fairness, i.e., the fairness of the decision outcomes. In this work, we leverage the rich literature on organizational justice and focus on another dimension of fair decision making: procedural fairness, i.e., the fairness of the decision making process. We propose measures for procedural fairness that consider the input features used in the decision process, and evaluate the moral judgments of humans regarding the use of these features. We operationalize these measures on two real world datasets using human surveys on the Amazon Mechanical Turk (AMT) platform, demonstrating that our measures capture important properties of procedurally fair decision making. We provide fast submodular mechanisms to optimize the tradeoff between procedural fairness and prediction accuracy. On our datasets, we observe empirically that procedural fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable. Copyright {\copyright} 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  isbn = {978-1-57735-800-8},
  langid = {english},
  language = {English}
}

@book{grohsMathematicalAspectsDeep2023,
  title = {Mathematical {{Aspects}} of {{Deep Learning}}},
  author = {Grohs, Philipp},
  year = {2023},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  abstract = {In recent years the development of new classification and regression algorithms based on deep learning has led to a revolution in the fields of artificial intelligence, machine learning, and data analysis. The development of a theoretical foundation to guarantee the success of these algorithms constitutes one of the most active and exciting research topics in applied mathematics. This book presents the current mathematical understanding of deep learning methods from the point of view of the leading experts in the field. It serves both as a starting point for researchers and graduate students in computer science, mathematics, and statistics trying to get into the field and as an invaluable reference for future research.},
  isbn = {978-1-316-51678-2},
  langid = {english},
  language = {eng}
}

@unpublished{grosseChapterAdaptiveGradient2021,
  type = {Lecture Notes},
  title = {Chapter 5 {{Adaptive Gradient Methods}}, {{Normalization}}, and {{Weight Decay}}},
  author = {Grosse, Roger},
  year = {2021},
  langid = {english},
  language = {en}
}

@unpublished{grosseChapterMetrics2021,
  title = {Chapter 3: {{Metrics}}},
  author = {Grosse, Roger},
  year = {2021},
  langid = {english},
  language = {en}
}

@unpublished{grosseChapterSecondOrderOptimization2021,
  title = {Chapter 4 {{Second-Order Optimization}}},
  author = {Grosse, Roger},
  year = {2021},
  langid = {english},
  language = {en}
}

@unpublished{grosseChapterTaylorApproximations2021,
  title = {Chapter 2: {{Taylor Approximations}}},
  author = {Grosse, Roger},
  year = {2021},
  langid = {english},
  language = {en}
}

@unpublished{grosseChapterToyModel2021,
  title = {Chapter 1 {{A Toy Model}}: {{Linear Regression}}},
  author = {Grosse, Roger},
  year = {2021},
  langid = {english},
  language = {en}
}

@misc{grosseCSC2541Winter20222022,
  title = {{{CSC2541 Winter}} 2022  {{Topics}} in {{Machine Learning}}: {{Neural Net Training Dynamics}}},
  author = {Grosse, Roger},
  year = {2022},
  urldate = {2024-07-24},
  abstract = {Neural nets have achieved amazing results over the past decade in domains as broad as vision, speech, language understanding, medicine, robotics, and game playing. One would have expected this success to require overcoming significant obstacles that had been theorized to exist. After all, the optimization landscape is nonconvex, highly nonlinear, and high-dimensional, so why are we able to train these networks? In many cases, they have far more than enough parameters to memorize the data, so why do they generalize well? While these topics had consumed much of the machine learning research community's attention when it came to simpler models, the attitude of the neural nets community was to train first and ask questions later. Apparently this worked. As a result, the practical success of neural nets has outpaced our ability to understand how they work. This class is about developing the conceptual tools to understand what happens when a neural net trains. Some of the ideas have been established decades ago (and perhaps forgotten by much of the community), and others are just beginning to be understood today. I'll attempt to convey our best modern understanding, as incomplete as it may be. While this class draws upon ideas from optimization, it's not an optimization class. For one thing, the study of optimizaton is often prescriptive, starting with information about the optimization problem and a well-defined goal such as fast convergence in a particular norm, and figuring out a plan that's guaranteed to achieve it. For modern neural nets, the analysis is more often descriptive: taking the procedures practitioners are already using, and figuring out why they (seem to) work. Hopefully this understanding will let us improve the algorithms. Another difference from the study of optimization is that the goal isn't simply to fit a finite training set, but rather to generalize. Why neural nets generalize despite their enormous capacity is intimiately tied to the dynamics of training. Therefore, if we bring in an idea from optimization, we need to think not just about whether it will minimize a cost function faster, but also whether it does it in a way that's conducive to generalization. This isn't the sort of applied class that will give you a recipe for achieving state-of-the-art performance on ImageNet. Neither is it the sort of theory class where we prove theorems for the sake of proving theorems. Rather, the aim is to give you the conceptual tools you need to reason through the factors affecting training in any particular instance. Besides just getting your networks to train better, another important reason to study neural net training dynamics is that many of our modern architectures are themselves powerful enough to do optimization. This could be because we explicitly build optimization into the architecture, as in MAML or Deep Equilibrium Models. Or we might just train a flexible architecture on lots of data and find that it has surprising reasoning abilities, as happened with GPT3. Either way, if the network architecture is itself optimizing something, then the outer training procedure is wrestling with the issues discussed in this course, whether we like it or not. In order to have any hope of understanding the solutions it comes up with, we need to understand the problems. Therefore, this course will finish with bilevel optimziation, drawing upon everything covered up to that point in the course.},
  howpublished = {https://www.cs.toronto.edu/{\textasciitilde}rgrosse/courses/csc2541\_2022/}
}

@article{guastellaEdgeBasedMissingData2021,
  title = {Edge-{{Based Missing Data Imputation}} in {{Large-Scale Environments}}},
  author = {Guastella, Davide Andrea and Marcillaud, Guilhem and Valenti, Cesare},
  year = {2021},
  month = apr,
  journal = {Information},
  volume = {12},
  number = {5},
  pages = {195},
  issn = {2078-2489},
  urldate = {2023-11-16},
  abstract = {Smart cities leverage large amounts of data acquired in the urban environment in the context of decision support tools. These tools enable monitoring the environment to improve the quality of services offered to citizens. The increasing diffusion of personal Internet of things devices capable of sensing the physical environment allows for low-cost solutions to acquire a large amount of information within the urban environment. On the one hand, the use of mobile and intermittent sensors implies new scenarios of large-scale data analysis; on the other hand, it involves different challenges such as intermittent sensors and integrity of acquired data. To this effect, edge computing emerges as a methodology to distribute computation among different IoT devices to analyze data locally. We present here a new methodology for imputing environmental information during the acquisition step, due to missing or otherwise out of order sensors, by distributing the computation among a variety of fixed and mobile devices. Numerous experiments have been carried out on real data to confirm the validity of the proposed method.},
  langid = {english},
  language = {en}
}

@phdthesis{gubriWhatMattersModel2023,
  title = {What {{Matters}} in {{Model Training}} to {{Transfer Adversarial Examples}}},
  author = {Gubri, Martin},
  year = {2023},
  month = jun,
  urldate = {2024-05-21},
  abstract = {Despite state-of-the-art performance on natural data, Deep Neural Networks (DNNs) are highly vulnerable to adversarial examples, i.e., imperceptible, carefully crafted perturbations of inputs applied at test time. Adversarial examples can transfer: an adversarial example against one model is likely to be adversarial against another independently trained model. This dissertation investigates the characteristics of the surrogate weight space that lead to the transferability of adversarial examples. Our research covers three complementary aspects of the weight space exploration: the multimodal exploration to obtain multiple models from different vicinities, the local exploration to obtain multiple models in the same vicinity, and the point selection to obtain a single transferable representation.     First, from a probabilistic perspective, we argue that transferability is fundamentally related to uncertainty. The unknown weights of the target DNN can be treated as random variables. Under a specified threat model, deep ensemble can produce a surrogate by sampling from the distribution of the target model. Unfortunately, deep ensembles are computationally expensive. We propose an efficient alternative by approximately sampling surrogate models from the posterior distribution using cSGLD, a state-of-the-art Bayesian deep learning technique. Our extensive experiments show that our approach improves and complements four attacks, three transferability techniques, and five more training methods significantly on ImageNet, CIFAR-10, and MNIST (up to 83.2 percentage points), while reducing training computations from 11.6 to 2.4 exaflops compared to deep ensemble on ImageNet.     Second, we propose transferability from Large Geometric Vicinity (LGV), a new technique based on the local exploration of the weight space. LGV starts from a pretrained model and collects multiple weights in a few additional training epochs with a constant and high learning rate. LGV exploits two geometric properties that we relate to transferability. First, we show that LGV explores a flatter region of the weight space and generates flatter adversarial examples in the input space. We present the surrogate-target misalignment hypothesis to explain why flatness could increase transferability. Second, we show that the LGV weights span a dense weight subspace whose geometry is intrinsically connected to transferability. Through extensive experiments, we show that LGV alone outperforms all (combinations of) four established transferability techniques by 1.8 to 59.9 percentage points.     Third, we investigate how to train a transferable representation, that is, a single model for transferability. First, we refute a common hypothesis from previous research to explain why early stopping improves transferability. We then establish links between transferability and the exploration dynamics of the weight space, in which early stopping has an inherent effect. More precisely, we observe that transferability peaks when the learning rate decays, which is also the time at which the sharpness of the loss significantly drops. This leads us to propose RFN, a new approach to transferability that minimises the sharpness of the loss during training. We show that by searching for large flat neighbourhoods, RFN always improves over early stopping (by up to 47 points of success rate) and is competitive to (if not better than) strong state-of-the-art baselines.    Overall, our three complementary techniques provide an extensive and practical method to obtain highly transferable adversarial examples from the multimodal and local exploration of flatter vicinities in the weight space. Our probabilistic and geometric approaches demonstrate that the way to train the surrogate model has been overlooked, although both the training noise and the flatness of the loss landscape are important elements of transfer-based attacks.},
  langid = {english},
  language = {English},
  school = {University of Luxembourg}
}

@article{gudivadaDataQualityConsiderations2017,
  title = {Data {{Quality Considerations}} for {{Big Data}} and {{Machine Learning}}: {{Going Beyond Data Cleaning}} and {{Transformations}}},
  author = {Gudivada, Venkat N and Apon, Amy and Ding, Junhua},
  year = {2017},
  abstract = {Data quality issues trace back their origin to the early days of computing. A wide range of domainspecific techniques to assess and improve the quality of data exist in the literature. These solutions primarily target data which resides in relational databases and data warehouses. The recent emergence of big data analytics and renaissance in machine learning necessitates evaluating the suitability relational database-centric approaches to data quality. In this paper, we describe the nature of the data quality issues in the context of big data and machine learning. We discuss facets of data quality, present a data governance-driven framework for data quality lifecycle for this new scenario, and describe an approach to its implementation. A sampling of the tools available for data quality management are indicated and future trends are discussed.},
  langid = {english},
  language = {en}
}

@article{guoClassificationNoniSampling2011,
  title = {Classification with Non-i.i.d. Sampling},
  author = {Guo, Zheng-Chu and Shi, Lei},
  year = {2011},
  month = sep,
  journal = {Mathematical and Computer Modelling},
  volume = {54},
  number = {5},
  pages = {1347--1364},
  issn = {0895-7177},
  urldate = {2024-04-14},
  abstract = {We study learning algorithms for classification generated by regularization schemes in reproducing kernel Hilbert spaces associated with a general convex loss function in a non-i.i.d. process. Error analysis is studied and our main purpose is to provide an elaborate capacity dependent error bounds by applying concentration techniques involving the {$\ell$}2-empirical covering numbers.},
  keywords = {-empirical covering number,-mixing sequence,Capacity dependent error bounds,Learning theory,Regularized classification,Reproducing kernel Hilbert spaces}
}

@inproceedings{guptaDataQualityMachine2021,
  title = {Data {{Quality}} for {{Machine Learning Tasks}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Gupta, Nitin and Mujumdar, Shashank and Patel, Hima and Masuda, Satoshi and Panwar, Naveen and Bandyopadhyay, Sambaran and Mehta, Sameep and Guttula, Shanmukha and Afzal, Shazia and Sharma Mittal, Ruhi and Munigala, Vitobha},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {4040--4041},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-10-24},
  abstract = {The quality of training data has a huge impact on the efficiency, accuracy and complexity of machine learning tasks. Data remains susceptible to errors or irregularities that may be introduced during collection, aggregation or annotation stage. This necessitates profiling and assessment of data to understand its suitability for machine learning tasks and failure to do so can result in inaccurate analytics and unreliable decisions. While researchers and practitioners have focused on improving the quality of models, there are limited efforts towards improving the data quality. Assessing the quality of the data across intelligently designed metrics and developing corresponding transformation operations to address the quality gaps helps to reduce the effort of a data scientist for iterative debugging of the ML pipeline to improve model performance. This tutorial highlights the importance of analysing data quality in terms of its value for ML applications. Finding the data quality issues in data helps different personas like data stewards, data scientists, subject matter experts, or machine learning scientists to get relevant data insights and take remedial actions to rectify any issue. This tutorial surveys all the important data quality related approaches for structured, unstructured and spatio-temporal domains discussed in literature, focusing on the intuition behind them, highlighting their strengths and similarities, and illustrates their applicability to real-world problems. Finally we will discuss the interesting work IBM Research is doing in this space.},
  isbn = {978-1-4503-8332-5},
  keywords = {data quality,machine learning,quality metrics}
}

@misc{guptaDataQualityToolkit2021,
  title = {Data {{Quality Toolkit}}: {{Automatic}} Assessment of Data Quality and Remediation for Machine Learning Datasets},
  shorttitle = {Data {{Quality Toolkit}}},
  author = {Gupta, Nitin and Patel, Hima and Afzal, Shazia and Panwar, Naveen and Mittal, Ruhi Sharma and Guttula, Shanmukha and Jain, Abhinav and Nagalapatti, Lokesh and Mehta, Sameep and Hans, Sandeep and Lohia, Pranay and Aggarwal, Aniya and Saha, Diptikalyan},
  year = {2021},
  month = sep,
  number = {arXiv:2108.05935},
  eprint = {2108.05935},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-24},
  abstract = {The quality of training data has a huge impact on the efficiency, accuracy and complexity of machine learning tasks. Various tools and techniques are available that assess data quality with respect to general cleaning and profiling checks. However these techniques are not applicable to detect data issues in the context of machine learning tasks, like noisy labels, existence of overlapping classes etc. We attempt to re-look at the data quality issues in the context of building a machine learning pipeline and build a tool that can detect, explain and remediate issues in the data, and systematically and automatically capture all the changes applied to the data. We introduce the Data Quality Toolkit for machine learning as a library of some key quality metrics and relevant remediation techniques to analyze and enhance the readiness of structured training datasets for machine learning projects. The toolkit can reduce the turn-around times of data preparation pipelines and streamline the data quality assessment process. Our toolkit is publicly available via IBM API Hub [1] platform, any developer can assess the data quality using the IBM's Data Quality for AI apis [2]. Detailed tutorials are also available on IBM Learning Path [3].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{hagendorffLinkingHumanMachine2021,
  title = {Linking {{Human And Machine Behavior}}: {{A New Approach}} to {{Evaluate Training Data Quality}} for {{Beneficial Machine Learning}}},
  shorttitle = {Linking {{Human And Machine Behavior}}},
  author = {Hagendorff, Thilo},
  year = {2021},
  month = dec,
  journal = {Minds and Machines},
  volume = {31},
  number = {4},
  pages = {563--593},
  issn = {1572-8641},
  urldate = {2023-11-12},
  abstract = {Machine behavior that is based on learning algorithms can be significantly influenced by the exposure to data of different qualities. Up to now, those qualities are solely measured in technical terms, but not in ethical ones, despite the significant role of training and annotation data in supervised machine learning. This is the first study to fill this gap by describing new dimensions of data quality for supervised machine learning applications. Based on the rationale that different social and psychological backgrounds of individuals correlate in practice with different modes of human--computer-interaction, the paper describes from an ethical perspective how varying qualities of behavioral data that individuals leave behind while using digital technologies have socially relevant ramification for the development of machine learning applications. The specific objective of this study is to describe how training data can be selected according to ethical assessments of the behavior it originates from, establishing an innovative filter regime to transition from the big data rationale n\,=\,all to a more selective way of processing data for training sets in machine learning. The overarching aim of this research is to promote methods for achieving beneficial machine learning applications that could be widely useful for industry as well as academia.},
  langid = {english},
  language = {en},
  keywords = {Artificial intelligence,Data quality,Machine behavior,Machine learning,Technology ethics,Training data}
}

@article{haimReconstructingTrainingData2022,
  title = {Reconstructing {{Training Data From Trained Neural Networks}}},
  author = {Haim, Niv and Vardi, Gal and Yehudai, Gilad and Shamir, Ohad and Irani, Michal},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {22911--22924},
  urldate = {2024-02-27},
  langid = {english},
  language = {en}
}

@unpublished{hairerSolvingDifferentialEquations,
  type = {Lecture Notes},
  title = {Solving {{Differential Equations}} on {{Manifolds}}},
  author = {Hairer, Ernst},
  langid = {english},
  language = {en}
}

@misc{hanSurveyLabelnoiseRepresentation2021,
  title = {A {{Survey}} of {{Label-noise Representation Learning}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {A {{Survey}} of {{Label-noise Representation Learning}}},
  author = {Han, Bo and Yao, Quanming and Liu, Tongliang and Niu, Gang and Tsang, Ivor W. and Kwok, James T. and Sugiyama, Masashi},
  year = {2021},
  month = feb,
  number = {arXiv:2011.04406},
  eprint = {2011.04406},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-10},
  abstract = {Classical machine learning implicitly assumes that labels of the training data are sampled from a clean distribution, which can be too restrictive for real-world scenarios. However, statistical-learning-based methods may not train deep learning models robustly with these noisy labels. Therefore, it is urgent to design Label-Noise Representation Learning (LNRL) methods for robustly training deep models with noisy labels. To fully understand LNRL, we conduct a survey study. We first clarify a formal definition for LNRL from the perspective of machine learning. Then, via the lens of learning theory and empirical study, we figure out why noisy labels affect deep models' performance. Based on the theoretical guidance, we categorize different LNRL methods into three directions. Under this unified taxonomy, we provide a thorough discussion of the pros and cons of different categories. More importantly, we summarize the essential components of robust LNRL, which can spark new directions. Lastly, we propose possible research directions within LNRL, such as new datasets, instance-dependent LNRL, and adversarial LNRL. We also envision potential directions beyond LNRL, such as learning with feature-noise, preference-noise, domain-noise, similarity-noise, graph-noise and demonstration-noise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{haoEfficientPrivacyEnhancedFederated2020,
  title = {Efficient and {{Privacy-Enhanced Federated Learning}} for {{Industrial Artificial Intelligence}}},
  author = {Hao, Meng and Li, Hongwei and Luo, Xizhao and Xu, Guowen and Yang, Haomiao and Liu, Sen},
  year = {2020},
  month = oct,
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {16},
  number = {10},
  pages = {6532--6542},
  issn = {1941-0050},
  urldate = {2024-04-11},
  abstract = {By leveraging deep learning-based technologies, industrial artificial intelligence (IAI) has been applied to solve various industrial challenging problems in Industry 4.0. However, for privacy reasons, traditional centralized training may be unsuitable for sensitive data-driven industrial scenarios, such as healthcare and autopilot. Recently, federated learning has received widespread attention, since it enables participants to collaboratively learn a shared model without revealing their local data. However, studies have shown that, by exploiting the shared parameters adversaries can still compromise industrial applications such as auto-driving navigation systems, medical data in wearable devices, and industrial robots' decision making. In this article, to solve this problem, we propose an efficient and privacy-enhanced federated learning (PEFL) scheme for IAI. Compared with existing solutions, PEFL is noninteractive, and can prevent private data from being leaked even if multiple entities collude with each other. Moreover, extensive experiments with real-world data demonstrate the superiority of PEFL in terms of accuracy and efficiency.},
  keywords = {Cloud computing,Differential privacy,Federated learning (FL),industrial artificial intelligence,Informatics,Privacy,privacy protection,Servers,Training}
}

@inproceedings{hardtEqualityOpportunitySupervised2016,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
  year = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-01-05},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.}
}

@inproceedings{hardtTrainFasterGeneralize2016,
  title = {Train Faster, Generalize Better: {{Stability}} of Stochastic Gradient Descent},
  shorttitle = {Train Faster, Generalize Better},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Hardt, Moritz and Recht, Ben and Singer, Yoram},
  year = {2016},
  month = jun,
  pages = {1225--1234},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-05-27},
  abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.},
  langid = {english},
  language = {en}
}

@article{hassensteinDataQualityConcepts2022,
  title = {Data {{Quality}}---{{Concepts}} and {{Problems}}},
  author = {Hassenstein, Max J. and Vanella, Patrizio},
  year = {2022},
  month = mar,
  journal = {Encyclopedia},
  volume = {2},
  number = {1},
  pages = {498--510},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-8392},
  urldate = {2024-02-25},
  abstract = {Data Quality is, in essence, understood as the degree to which the data of interest satisfies the requirements, is free of flaws, and is suited for the intended purpose. Data Quality is usually measured utilizing several criteria, which may differ in terms of assigned importance, depending on, e.g., the data at hand, stakeholders, or the intended use.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {data life cycle,data quality,data quality dimensions,information quality}
}

@article{hastieSURPRISESHIGHDIMENSIONALRIDGELESS2022,
  title = {{{SURPRISES IN HIGH-DIMENSIONAL RIDGELESS LEAST SQUARES INTERPOLATION}}},
  author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J.},
  year = {2022},
  month = apr,
  journal = {Annals of statistics},
  volume = {50},
  number = {2},
  pages = {949--986},
  issn = {0090-5364},
  urldate = {2024-07-25},
  abstract = {Interpolators---estimators that achieve zero training error---have attracted growing attention in machine learning, mainly because state-of-the art neural networks appear to be models of this type. In this paper, we study minimum {$\ell$}2 norm (``ridgeless'') interpolation least squares regression, focusing on the high-dimensional regime in which the number of unknown parameters p is of the same order as the number of samples n. We consider two different models for the feature distribution: a linear model, where the feature vectors xi{$\in\mathbb{R}$}p are obtained by applying a linear transform to a vector of i.i.d. entries, xi = {$\Sigma$}1/2zi (with zi{$\in\mathbb{R}$}p); and a nonlinear model, where the feature vectors are obtained by passing the input through a random one-layer neural network, xi = {$\varphi$}(Wzi) (with zi{$\in\mathbb{R}$}d, W{$\in\mathbb{R}$}p{\texttimes}d a matrix of i.i.d. entries, and {$\varphi$} an activation function acting componentwise on Wzi). We recover---in a precise quantitative way---several phenomena that have been observed in large-scale neural networks and kernel machines, including the ``double descent'' behavior of the prediction risk, and the potential benefits of overparametrization.},
  pmcid = {PMC9481183},
  pmid = {36120512}
}

@inproceedings{hitajDeepModelsGAN2017,
  title = {Deep {{Models Under}} the {{GAN}}: {{Information Leakage}} from {{Collaborative Deep Learning}}},
  shorttitle = {Deep {{Models Under}} the {{GAN}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Hitaj, Briland and Ateniese, Giuseppe and {Perez-Cruz}, Fernando},
  year = {2017},
  month = oct,
  series = {{{CCS}} '17},
  pages = {603--618},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-03-11},
  abstract = {Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases. Its success is due to a combination of recent algorithmic breakthroughs, increasingly powerful computers, and access to significant amounts of data. Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level differential privacy applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).},
  isbn = {978-1-4503-4946-8},
  keywords = {collaborative learning,deep learning,privacy,security}
}

@inproceedings{horoiExploringGeometryTopology2022,
  title = {Exploring the {{Geometry}} and {{Topology}} of {{Neural Network Loss Landscapes}}},
  booktitle = {Advances in {{Intelligent Data Analysis XX}}},
  author = {Horoi, Stefan and Huang, Jessie and Rieck, Bastian and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita},
  editor = {Bouadi, Tassadit and Fromont, Elisa and H{\"u}llermeier, Eyke},
  year = {2022},
  pages = {171--184},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {Recent work has established clear links between the generalization performance of trained neural networks and the geometry of their loss landscape near the local minima to which they converge. This suggests that qualitative and quantitative examination of the loss landscape geometry could yield insights about neural network generalization performance during training. To this end, researchers have proposed visualizing the loss landscape through the use of simple dimensionality reduction techniques. However, such visualization methods have been limited by their linear nature and only capture features in one or two dimensions, thus restricting sampling of the loss landscape to lines or planes. Here, we expand and improve upon these in three ways. First, we present a novel ``jump and retrain'' procedure for sampling relevant portions of the loss landscape. We show that the resulting sampled data holds more meaningful information about the network's ability to generalize. Next, we show that non-linear dimensionality reduction of the jump and retrain trajectories via PHATE, a trajectory and manifold-preserving method, allows us to visualize differences between networks that are generalizing well vs poorly. Finally, we combine PHATE trajectories with a computational homology characterization to quantify trajectory differences.},
  isbn = {978-3-031-01333-1},
  langid = {english},
  language = {en},
  keywords = {Artificial neural network loss landscape,Non-linear dimensionality reduction,Topological data analysis}
}

@inproceedings{huangLearningDeepRepresentation2016,
  title = {Learning {{Deep Representation}} for {{Imbalanced Classification}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Huang, Chen and Li, Yining and Loy, Chen Change and Tang, Xiaoou},
  year = {2016},
  pages = {5375--5384},
  urldate = {2023-11-16}
}

@article{huCloudEdgeCooperation2022,
  title = {Cloud--{{Edge}} Cooperation for Meteorological Radar Big Data: {{A}} Review of Data Quality Control},
  shorttitle = {Cloud--{{Edge}} Cooperation for Meteorological Radar Big Data},
  author = {Hu, Zhichen and Xu, Xiaolong and Zhang, Yulan and Tang, Hongsheng and Cheng, Yong and Qian, Cheng and Khosravi, Mohammad R.},
  year = {2022},
  month = oct,
  journal = {Complex \& Intelligent Systems},
  volume = {8},
  number = {5},
  pages = {3789--3803},
  issn = {2198-6053},
  urldate = {2023-07-28},
  abstract = {With the rapid development of information technology construction, increasing specialized data in the field of informatization have become a hot spot for research. Among them, meteorological data, as one of the foundations and core contents of meteorological informatization, is the key production factor of meteorology in the era of digital economy as well as the basis of meteorological services for people and decision-making services. However, the existing centralized cloud computing service model is unable to satisfy the performance demand of low latency, high reliability and high bandwidth for weather data quality control. In addition, strong convective weather is characterized by rapid development, small convective scale and short life cycle, making the complexity of real-time weather data quality control increased to provide timely strong convective weather monitoring services. In order to solve the above problems, this paper proposed the cloud--edge cooperation approach, whose core idea is to effectively combine the advantages of edge computing and cloud computing by taking full advantage of the computing resources distributed at the edge to provide service environment for users to satisfy the real-time demand. The powerful computing and storage resources of the cloud data center are utilized to provide users with massive computing services to fulfill the intensive computing demands.},
  langid = {english},
  language = {english},
  keywords = {Cloud-edge cooperation,Data quality control,Meteorological data,Service deployment}
}

@inproceedings{huDoesDistributionallyRobust2018,
  title = {Does {{Distributionally Robust Supervised Learning Give Robust Classifiers}}?},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Hu, Weihua and Niu, Gang and Sato, Issei and Sugiyama, Masashi},
  year = {2018},
  month = jul,
  pages = {2029--2037},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-12-16},
  abstract = {Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.},
  langid = {english},
  language = {en}
}

@article{huMembershipInferenceAttacks2022,
  title = {Membership {{Inference Attacks}} on {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Membership {{Inference Attacks}} on {{Machine Learning}}},
  author = {Hu, Hongsheng and Salcic, Zoran and Sun, Lichao and Dobbie, Gillian and Yu, Philip S. and Zhang, Xuyun},
  year = {2022},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {11s},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  urldate = {2024-03-09},
  abstract = {Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature.},
  langid = {english},
  language = {en}
}

@inproceedings{huSurprisingSimplicityEarlyTime2020,
  title = {The {{Surprising Simplicity}} of the {{Early-Time Learning Dynamics}} of {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hu, Wei and Xiao, Lechao and Adlam, Ben and Pennington, Jeffrey},
  year = {2020},
  volume = {33},
  pages = {17116--17128},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-11},
  abstract = {Modern neural networks are often regarded as complex black-box functions whose behavior is difficult to understand owing to their nonlinear dependence on the data and the nonconvexity in their loss landscapes. In this work, we show that these common perceptions can be completely false in the early phase of learning. In particular, we formally prove that, for a class of well-behaved input distributions, the early-time learning dynamics of a two-layer fully-connected neural network can be mimicked by training a simple linear model on the inputs. We additionally argue that this surprising simplicity can persist in networks with more layers and with convolutional architecture, which we verify empirically. Key to our analysis is to bound the spectral norm of the difference between the Neural Tangent Kernel (NTK) and an affine transform of the data kernel; however, unlike many previous results utilizing the NTK, we do not require the network to have disproportionately large width, and the network is allowed to escape the kernel regime later in training.}
}

@inproceedings{iacobCanFairFederated2023,
  title = {Can {{Fair Federated Learning Reduce}} the Need for {{Personalisation}}?},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Machine Learning}} and {{Systems}}},
  author = {Iacob, Alex and Gusm{\~a}o, Pedro Porto Buarque and Lane, Nicholas},
  year = {2023},
  month = may,
  pages = {131--139},
  publisher = {ACM},
  address = {Rome Italy},
  urldate = {2024-04-03},
  isbn = {9798400700842},
  langid = {english},
  language = {en},
  keywords = {TERM}
}

@misc{ibayashiMinimumSharpnessScaleinvariant2021,
  title = {Minimum Sharpness: {{Scale-invariant}} Parameter-Robustness of Neural Networks},
  shorttitle = {Minimum Sharpness},
  author = {Ibayashi, Hikaru and Hamaguchi, Takuo and Imaizumi, Masaaki},
  year = {2021},
  month = jun,
  urldate = {2024-05-12},
  abstract = {Toward achieving robust and defensive neural networks, the robustness against the weight parameters perturbations, i.e., sharpness, attracts attention in recent years (Sun et al., 2020). However, sharpness is known to remain a critical issue, "scale-sensitivity." In this paper, we propose a novel sharpness measure, Minimum Sharpness. It is known that NNs have a specific scale transformation that constitutes equivalent classes where functional properties are completely identical, and at the same time, their sharpness could change unlimitedly. We define our sharpness through a minimization problem over the equivalent NNs being invariant to the scale transformation. We also develop an efficient and exact technique to make the sharpness tractable, which reduces the heavy computational costs involved with Hessian. In the experiment, we observed that our sharpness has a valid correlation with the generalization of NNs and runs with less computational cost than existing sharpness measures.},
  langid = {english},
  language = {en}
}

@article{imaizumiGeneralizationBoundsDeep2023,
  title = {On {{Generalization Bounds}} for {{Deep Networks Based}} on {{Loss Surface Implicit Regularization}}},
  author = {Imaizumi, Masaaki and {Schmidt-Hieber}, Johannes},
  year = {2023},
  month = feb,
  journal = {IEEE Transactions on Information Theory},
  volume = {69},
  number = {2},
  pages = {1203--1223},
  issn = {0018-9448, 1557-9654},
  urldate = {2024-05-12},
  abstract = {The classical statistical learning theory implies that fitting too many parameters leads to overfitting and poor performance. That modern deep neural networks generalize well despite a large number of parameters contradicts this finding and constitutes a major unsolved problem towards explaining the success of deep learning. While previous work focuses on the implicit regularization induced by stochastic gradient descent (SGD), we study here how the local geometry of the energy landscape around local minima affects the statistical properties of SGD with Gaussian gradient noise. We argue that under reasonable assumptions, the local geometry forces SGD to stay close to a low dimensional subspace and that this induces another form of implicit regularization and results in tighter bounds on the generalization error for deep neural networks. To derive generalization error bounds for neural networks, we first introduce a notion of stagnation sets around the local minima and impose a local essential convexity property of the population risk. Under these conditions, lower bounds for SGD to remain in these stagnation sets are derived. If stagnation occurs, we derive a bound on the generalization error of deep neural networks involving the spectral norms of the weight matrices but not the number of network parameters. Technically, our proofs are based on controlling the change of parameter values in the SGD iterates and local uniform convergence of the empirical loss functions based on the entropy of suitable neighborhoods around local minima.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  language = {en}
}

@article{imEmpiricalAnalysisDeep2016,
  title = {An {{Empirical Analysis}} of {{Deep Network Loss Surfaces}}},
  author = {Im, Daniel Jiwoong and Tao, Michael and Branson, Kristin},
  year = {2016},
  month = nov,
  urldate = {2024-05-18},
  abstract = {The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.},
  langid = {english},
  language = {en}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-25},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@phdthesis{izzoTheoryAlgorithmsDatacentric2023,
  title = {Theory and Algorithms for Data-Centric Machine Learning},
  author = {Izzo, Zachary and Ying, Lexing and Zou, James and Chatterjee, Sourav},
  year = {2023},
  address = {Stanford, California},
  abstract = {Machine learning (ML) and AI have achieved remarkable, super-human performance in a wide variety of domains: computer vision, natural language processing, and protein folding, to name but a few. Until recently, most advancements have taken a model-centric approach, focusing primarily on improved neural network architectures (ConvNets, ResNets, transformers, etc.) and optimization procedures for training these models (batch norm, dropout, neural architecture search, etc.). Relatively less attention has been paid to the data used to train these models, in spite of the well-known fact that ML is critically dependent on high-quality data, captured succinctly with the phrase "garbage in, garbage out." As the returns on ever larger and more complicated models diminish (MT-NLG from Nvidia and Microsoft having 530B parameters), researchers have begun to realize the importance of taking a data-centric approach and developing principled methods for studying the fuel for these models: the data itself. Beyond improved task performance, a data-centric perspective also allows us to take socially critical considerations, such as data privacy, into account. In this thesis, we will take a critical look at several points in the ML data pipeline: before, during, and after model training. Before model training, we will explore the problem of data selection: which data should be used to train the model, and on what type of data should we expect our model to work? As we move forward into model training, we will turn our attention to two issues which can result from the interaction of our ML systems with the environment in which they are deployed. The first issue is that of data privacy: how can we prevent our models from leaking sensitive information about their training data? The second issue concerns the dynamic nature of some modeled populations. Especially when our model is used to make socially impactful decisions (e.g., automated loan approval or recommender systems), the model itself may impact the distribution of the data, leading to degraded performance. Lastly, despite following best practices before and during model training, it may be the case that we want to post-process a model to remove the effects of certain data after training. How can this be achieved in a computationally efficient manner? This thesis covers novel solutions for each of the preceding problems, with an emphasis on the provable guarantees for each of the proposed algorithms. By applying mathematical rigor to challenging real-world problems, we can develop algorithms which are both effective and trustworthy},
  collaborator = {{Stanford University} and {Stanford University}},
  school = {Stanford University}
}

@inproceedings{jacotNeuralTangentKernel2018,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-25},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.}
}

@misc{jafarigolParadoxNoiseEmpirical2023,
  title = {The {{Paradox}} of {{Noise}}: {{An Empirical Study}} of {{Noise-Infusion Mechanisms}} to {{Improve Generalization}}, {{Stability}}, and {{Privacy}} in {{Federated Learning}}},
  shorttitle = {The {{Paradox}} of {{Noise}}},
  author = {Jafarigol, Elaheh and Trafalis, Theodore},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05790},
  eprint = {2311.05790},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-24},
  abstract = {In a data-centric era, concerns regarding privacy and ethical data handling grow as machine learning relies more on personal information. This empirical study investigates the privacy, generalization, and stability of deep learning models in the presence of additive noise in federated learning frameworks. Our main objective is to provide strategies to measure the generalization, stability, and privacy-preserving capabilities of these models and further improve them. To this end, five noise infusion mechanisms at varying noise levels within centralized and federated learning settings are explored. As model complexity is a key component of the generalization and stability of deep learning models during training and evaluation, a comparative analysis of three Convolutional Neural Network (CNN) architectures is provided. The paper introduces Signal-to-Noise Ratio (SNR) as a quantitative measure of the trade-off between privacy and training accuracy of noise-infused models, aiming to find the noise level that yields optimal privacy and accuracy. Moreover, the Price of Stability and Price of Anarchy are defined in the context of privacy-preserving deep learning, contributing to the systematic investigation of the noise infusion strategies to enhance privacy without compromising performance. Our research sheds light on the delicate balance between these critical factors, fostering a deeper understanding of the implications of noise-based regularization in machine learning. By leveraging noise as a tool for regularization and privacy enhancement, we aim to contribute to the development of robust, privacy-aware algorithms, ensuring that AI-driven solutions prioritize both utility and privacy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{jainDiagnosticApproachAssess2022,
  title = {A {{Diagnostic Approach}} to {{Assess}} the {{Quality}} of {{Data Splitting}} in {{Machine Learning}}},
  author = {Jain, Eklavya and Neeraja, J. and Banerjee, Buddhananda and Ghosh, Palash},
  year = {2022},
  month = jun,
  number = {arXiv:2206.11721},
  eprint = {2206.11721},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2023-11-14},
  abstract = {In machine learning, a routine practice is to split the data into a training and a test data set. A proposed model is built based on the training data, and then the performance of the model is assessed using test data. Usually, the data is split randomly into a training and a test set on an ad hoc basis. This approach, pivoted on random splitting, works well but more often than not, it fails to gauge the generalizing capability of the model with respect to perturbations in the input of training and test data. Experimentally, this sensitive aspect of randomness in the input data is realized when a new iteration of a fixed pipeline, from model building to training and testing, is executed, and an overly optimistic performance estimate is reported. Since the consistency in a model's performance predominantly depends on the data splitting, any conclusions on the robustness of the model are unreliable in such a scenario. We propose a diagnostic approach to quantitatively assess the quality of a given split in terms of its true randomness, and provide a basis for inferring model insensitivity towards the input data. We associate model robustness with random splitting using a self-defined data-driven distance metric based on the Mahalanobis squared distance between a train set and its corresponding test set. The probability distribution of the distance metric is simulated using Monte Carlo simulations, and a threshold is calculated from one-sided hypothesis testing. We motivate and showcase the performance of the proposed approach using various real data sets. We also compare the performance of the existing data splitting methods using the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation}
}

@inproceedings{jainOverviewImportanceData2020,
  title = {Overview and {{Importance}} of {{Data Quality}} for {{Machine Learning Tasks}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Jain, Abhinav and Patel, Hima and Nagalapatti, Lokesh and Gupta, Nitin and Mehta, Sameep and Guttula, Shanmukha and Mujumdar, Shashank and Afzal, Shazia and Sharma Mittal, Ruhi and Munigala, Vitobha},
  year = {2020},
  month = aug,
  series = {{{KDD}} '20},
  pages = {3561--3562},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-10-24},
  abstract = {It is well understood from literature that the performance of a machine learning (ML) model is upper bounded by the quality of the data. While researchers and practitioners have focused on improving the quality of models (such as neural architecture search and automated feature selection), there are limited efforts towards improving the data quality. One of the crucial requirements before consuming datasets for any application is to understand the dataset at hand and failure to do so can result in inaccurate analytics and unreliable decisions. Assessing the quality of the data across intelligently designed metrics and developing corresponding transformation operations to address the quality gaps helps to reduce the effort of a data scientist for iterative debugging of the ML pipeline to improve model performance. This tutorial highlights the importance of analysing data quality in terms of its value for machine learning applications. This tutorial surveys all the important data quality related approaches discussed in literature, focusing on the intuition behind them, highlighting their strengths and similarities, and illustrates their applicability to real-world problems. Finally we will discuss the interesting work IBM Research is doing in this space.},
  isbn = {978-1-4503-7998-4},
  keywords = {data quality,machine learning,quality metrics}
}

@article{jamali-radFederatedLearningTaskonomy2023,
  title = {Federated {{Learning With Taskonomy}} for {{Non-IID Data}}},
  author = {{Jamali-Rad}, Hadi and Abdizadeh, Mohammad and Singh, Anuj},
  year = {2023},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {11},
  pages = {8719--8730},
  issn = {2162-2388},
  urldate = {2024-04-14},
  abstract = {Classical federated learning approaches incur significant performance degradation in the presence of non-independent and identically distributed (non-IID) client data. A possible direction to address this issue is forming clusters of clients with roughly IID data. Most solutions following this direction are iterative and relatively slow, also prone to convergence issues in discovering underlying cluster formations. We introduce federated learning with taskonomy (FLT) that generalizes this direction by learning the task relatedness between clients for more efficient federated aggregation of heterogeneous data. In a one-off process, the server provides the clients with a pretrained (and fine-tunable) encoder to compress their data into a latent representation and transmit the signature of their data back to the server. The server then learns the task relatedness among clients via manifold learning and performs a generalization of federated averaging. FLT can flexibly handle a generic client relatedness graph, when there are no explicit clusters of clients, as well as efficiently decompose it into (disjoint) clusters for clustered federated learning. We demonstrate that FLT not only outperforms the existing state-of-the-art baselines in non-IID scenarios but also offers improved fairness across clients. Our codebase can be found at: https://github.com/hjraad/FLT/},
  keywords = {Collaborative work,Convergence,Data models,Distributed databases,Federated learning,Manifolds,non-independent and identically distributed (non-IID) client data,Servers,Task analysis}
}

@inproceedings{jangReparametrizationInvariantSharpnessMeasure2022,
  title = {A {{Reparametrization-Invariant Sharpness Measure Based}} on {{Information Geometry}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jang, Cheongjae and Lee, Sungyoon and Park, Frank C. and Noh, Yung-Kyun},
  year = {2022},
  month = oct,
  urldate = {2024-08-14},
  abstract = {It has been observed that the generalization performance of neural networks correlates with the sharpness of their loss landscape. Dinh et al. (2017) have observed that existing formulations of sharpness measures fail to be invariant with respect to scaling and reparametrization. While some scale-invariant measures have recently been proposed, reparametrization-invariant measures are still lacking. Moreover, they often do not provide any theoretical insights into generalization performance nor lead to practical use to improve the performance. Based on an information geometric analysis of the neural network parameter space, in this paper we propose a reparametrization-invariant sharpness measure that captures the change in loss with respect to changes in the probability distribution modeled by neural networks, rather than with respect to changes in the parameter values. We reveal some theoretical connections of our measure to generalization performance. In particular, experiments confirm that using our measure as a regularizer in neural network training significantly improves performance.},
  langid = {english},
  language = {en}
}

@inproceedings{jayaramanDistributedLearningDistress2018,
  title = {Distributed {{Learning}} without {{Distress}}: {{Privacy-Preserving Empirical Risk Minimization}}},
  shorttitle = {Distributed {{Learning}} without {{Distress}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jayaraman, Bargav and Wang, Lingxiao and Evans, David and Gu, Quanquan},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-11-30},
  abstract = {Distributed learning allows a group of independent data owners to collaboratively learn a model over their data sets without exposing their private data. We present a distributed learning approach that combines differential privacy with secure multi-party computation. We explore two popular methods of differential privacy, output perturbation and gradient perturbation, and advance the state-of-the-art for both methods in the distributed learning setting. In our output perturbation method, the parties combine local models within a secure computation and then add the required differential privacy noise before revealing the model. In our gradient perturbation method, the data owners collaboratively train a global model via an iterative learning algorithm.  At each iteration, the parties aggregate their local gradients within a secure computation, adding sufficient noise to ensure privacy before the gradient updates are revealed. For both methods, we show that the noise can be reduced in the multi-party setting by adding the noise inside the secure computation after aggregation, asymptotically improving upon the best previous results. Experiments on real world data sets demonstrate that our methods provide substantial utility gains for typical privacy requirements.}
}

@misc{jeongCommunicationEfficientOnDeviceMachine2023,
  title = {Communication-{{Efficient On-Device Machine Learning}}: {{Federated Distillation}} and {{Augmentation}} under {{Non-IID Private Data}}},
  shorttitle = {Communication-{{Efficient On-Device Machine Learning}}},
  author = {Jeong, Eunjeong and Oh, Seungeun and Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
  year = {2023},
  month = oct,
  number = {arXiv:1811.11479},
  eprint = {1811.11479},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-17},
  abstract = {On-device machine learning (ML) enables the training process to exploit a massive amount of user-generated private data samples. To enjoy this benefit, inter-device communication overhead should be minimized. With this end, we propose federated distillation (FD), a distributed model training algorithm whose communication payload size is much smaller than a benchmark scheme, federated learning (FL), particularly when the model size is large. Moreover, user-generated data samples are likely to become non-IID across devices, which commonly degrades the performance compared to the case with an IID dataset. To cope with this, we propose federated augmentation (FAug), where each device collectively trains a generative model, and thereby augments its local data towards yielding an IID dataset. Empirical studies demonstrate that FD with FAug yields around 26x less communication overhead while achieving 95-98\% test accuracy compared to FL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Statistics - Machine Learning}
}

@article{jereTaxonomyAttacksFederated2021,
  title = {A {{Taxonomy}} of {{Attacks}} on {{Federated Learning}}},
  author = {Jere, Malhar S. and Farnan, Tyler and Koushanfar, Farinaz},
  year = {2021},
  month = mar,
  journal = {IEEE Security \& Privacy},
  volume = {19},
  number = {2},
  pages = {20--28},
  issn = {1558-4046},
  urldate = {2024-03-24},
  abstract = {Federated learning is a privacy-by-design framework that enables training deep neural networks from decentralized sources of data, but it is fraught with innumerable attack surfaces. We provide a taxonomy of recent attacks on federated learning systems and detail the need for more robust threat modeling in federated learning environments.},
  keywords = {Collaborative work,Computational modeling,Data models,Data privacy,Security,Servers,Training data}
}

@inproceedings{jhaLabelPoisoningAll2023,
  title = {Label {{Poisoning}} Is {{All You Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jha, Rishi and Hayase, Jonathan and Oh, Sewoong},
  year = {2023},
  month = dec,
  volume = {36},
  pages = {71029--71052},
  urldate = {2024-05-11},
  langid = {english},
  language = {en}
}

@misc{jiangImprovingFederatedLearning2023,
  title = {Improving {{Federated Learning Personalization}} via {{Model Agnostic Meta Learning}}},
  author = {Jiang, Yihan and Kone{\v c}n{\'y}, Jakub and Rush, Keith and Kannan, Sreeram},
  year = {2023},
  month = jan,
  number = {arXiv:1909.12488},
  eprint = {1909.12488},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-17},
  abstract = {Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{jiaProofofLearningDefinitionsPractice2021,
  title = {Proof-of-{{Learning}}: {{Definitions}} and {{Practice}}},
  shorttitle = {Proof-of-{{Learning}}},
  booktitle = {2021 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Jia, Hengrui and Yaghini, Mohammad and {Choquette-Choo}, Christopher A. and Dullerud, Natalie and Thudi, Anvith and Chandrasekaran, Varun and Papernot, Nicolas},
  year = {2021},
  month = may,
  pages = {1039--1056},
  issn = {2375-1207},
  urldate = {2024-03-25},
  abstract = {Training machine learning (ML) models typically involves expensive iterative optimization. Once the model's final parameters are released, there is currently no mechanism for the entity which trained the model to prove that these parameters were indeed the result of this optimization procedure. Such a mechanism would support security of ML applications in several ways. For instance, it would simplify ownership resolution when multiple parties contest ownership of a specific model. It would also facilitate the distributed training across untrusted workers where Byzantine workers might otherwise mount a denial-ofservice by returning incorrect model updates.In this paper, we remediate this problem by introducing the concept of proof-of-learning in ML. Inspired by research on both proof-of-work and verified computations, we observe how a seminal training algorithm, stochastic gradient descent, accumulates secret information due to its stochasticity. This produces a natural construction for a proof-of-learning which demonstrates that a party has expended the compute require to obtain a set of model parameters correctly. In particular, our analyses and experiments show that an adversary seeking to illegitimately manufacture a proof-of-learning needs to perform at least as much work than is needed for gradient descent itself.We also instantiate a concrete proof-of-learning mechanism in both of the scenarios described above. In model ownership resolution, it protects the intellectual property of models released publicly. In distributed training, it preserves availability of the training procedure. Our empirical evaluation validates that our proof-of-learning mechanism is robust to variance induced by the hardware (e.g., ML accelerators) and software stacks.},
  keywords = {Computational modeling,Intellectual property,Machine learning,machine-learning,Privacy,proof-of-work,security,Software,Stochastic processes,Training}
}

@article{jiFedFixerMitigatingHeterogeneous2024,
  title = {{{FedFixer}}: {{Mitigating Heterogeneous Label Noise}} in {{Federated Learning}}},
  shorttitle = {{{FedFixer}}},
  author = {Ji, Xinyuan and Zhu, Zhaowei and Xi, Wei and Gadyatskaya, Olga and Song, Zilong and Cai, Yong and Liu, Yang},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {11},
  pages = {12830--12838},
  issn = {2374-3468},
  urldate = {2024-04-12},
  abstract = {Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models. We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets. The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {ML: Deep Learning Algorithms}
}

@article{joshiEnablingAllInEdge2023,
  title = {Enabling {{All In-Edge Deep Learning}}: {{A Literature Review}}},
  shorttitle = {Enabling {{All In-Edge Deep Learning}}},
  author = {Joshi, Praveen and Hasanuzzaman, Mohammed and Thapa, Chandra and Afli, Haithem and Scully, Ted},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {3431--3460},
  issn = {2169-3536},
  urldate = {2023-10-01},
  abstract = {In recent years, deep learning (DL) models have demonstrated remarkable achievements on non-trivial tasks such as speech recognition, image processing, and natural language understanding. One of the significant contributors to the success of DL is the proliferation of end devices that act as a catalyst to provide data for data-hungry DL models. However, computing DL training and inference still remains the biggest challenge. Moreover, most of the time central cloud servers are used for such computation, thus opening up other significant challenges, such as high latency, increased communication costs, and privacy concerns. To mitigate these drawbacks, considerable efforts have been made to push the processing of DL models to edge servers (a mesh of computing devices near end devices). Recently, the confluence point of DL and edge has given rise to edge intelligence (EI), defined by the International Electrotechnical Commission (IEC) as the concept where the data is acquired, stored, and processed utilizing edge computing with DL and advanced networking capabilities. Broadly, EI has six levels of categories based on the three locations where the training and inference of DL take place, e.g., cloud server, edge server, and end devices. This survey paper focuses primarily on the fifth level of EI, called all in-edge level, where DL training and inference (deployment) are performed solely by edge servers. All in-edge is suitable when the end devices have low computing resources, e.g., Internet-of-Things, and other requirements such as latency and communication cost are important such as in mission-critical applications (e.g., health care). Besides, 5G/6G networks are envisioned to use all in-edge. Firstly, this paper presents all in-edge computing architectures, including centralized, decentralized, and distributed. Secondly, this paper presents enabling technologies, such as model parallelism, data parallelism, and split learning, which facilitates DL training and deployment at edge servers. Thirdly, model adaptation techniques based on model compression and conditional computation are described because the standard cloud-based DL deployment cannot be directly applied to all in-edge due to its limited computational resources. Fourthly, this paper discusses eleven key performance metrics to evaluate the performance of DL at all in-edge efficiently. Finally, several open research challenges in the area of all in-edge are presented.},
  langid = {english},
  language = {en}
}

@article{kaddourWhenFlatMinima2022,
  title = {When {{Do Flat Minima Optimizers Work}}?},
  author = {Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J.},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {16577--16595},
  urldate = {2024-05-16},
  langid = {english},
  language = {en}
}

@inproceedings{kamathDoesInvariantRisk2021,
  title = {Does {{Invariant Risk Minimization Capture Invariance}}?},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kamath, Pritish and Tangella, Akilesh and Sutherland, Danica and Srebro, Nathan},
  year = {2021},
  month = mar,
  pages = {4069--4077},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-12-03},
  abstract = {We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et al. (2019) can fail to capture "natural" invariances, at least when used in its practical "linear" form, and even on very simple problems which directly follow the motivating examples for IRM. This can lead to worse generalization on new environments, even when compared to unconstrained ERM. The issue stems from a significant gap between the linear variant (as in their concrete method IRMv1) and the full non-linear IRM formulation. Additionally, even when capturing the "right" invariances, we show that it is possible for IRM to learn a sub-optimal predictor, due to the loss function not being invariant across environments. The issues arise even when measuring invariance on the population distributions, but are exacerbated by the fact that IRM is extremely fragile to sampling.},
  langid = {english},
  language = {en}
}

@article{kamiranDataPreprocessingTechniques2012,
  title = {Data Preprocessing Techniques for Classification without Discrimination},
  author = {Kamiran, Faisal and Calders, Toon},
  year = {2012},
  month = oct,
  journal = {Knowledge and Information Systems},
  volume = {33},
  number = {1},
  pages = {1--33},
  issn = {0219-1377, 0219-3116},
  urldate = {2024-03-19},
  abstract = {Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data.},
  langid = {english},
  language = {en}
}

@inproceedings{kamishimaFairnessAwareClassifierPrejudice2012,
  title = {Fairness-{{Aware Classifier}} with {{Prejudice Remover Regularizer}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  editor = {Flach, Peter A. and De Bie, Tijl and Cristianini, Nello},
  year = {2012},
  pages = {35--50},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  abstract = {With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.},
  isbn = {978-3-642-33486-3},
  langid = {english},
  language = {en},
  keywords = {classification,discrimination,fairness,information theory,logistic regression,social responsibility}
}

@incollection{karanikaEnsembleInterpretableMachine2020,
  title = {An {{Ensemble Interpretable Machine Learning Scheme}} for {{Securing Data Quality}} at the {{Edge}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Karanika, Anna and Oikonomou, Panagiotis and Kolomvatsos, Kostas and Anagnostopoulos, Christos},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2020},
  volume = {12279},
  pages = {517--534},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2023-07-20},
  abstract = {Data quality is a significant research subject for any application that requests for analytics to support decision making. It becomes very important when we focus on Internet of Things (IoT) where numerous devices can interact to exchange and process data. IoT devices are connected to Edge Computing (EC) nodes to report the collected data, thus, we have to secure data quality not only at the IoT infrastructure but also at the edge of the network. In this paper, we focus on the specific problem and propose the use of interpretable machine learning to deliver the features that are important to be based on for any data processing activity. Our aim is to secure data quality for those features, at least, that are detected as significant in the collected datasets. We have to notice that the selected features depict the highest correlation with the remaining ones in every dataset, thus, they can be adopted for dimensionality reduction. We focus on multiple methodologies for having interpretability in our learning models and adopt an ensemble scheme for the final decision. Our scheme is capable of timely retrieving the final result and efficiently selecting the appropriate features. We evaluate our model through extensive simulations and present numerical results. Our aim is to reveal its performance under various experimental scenarios that we create varying a set of parameters adopted in our mechanism.},
  isbn = {978-3-030-57320-1 978-3-030-57321-8},
  langid = {english},
  language = {english}
}

@inproceedings{karimireddyLearningHistoryByzantine2021,
  title = {Learning from {{History}} for {{Byzantine Robust Optimization}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  year = {2021},
  month = jul,
  pages = {5311--5319},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-07},
  abstract = {Byzantine robustness has received significant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe flaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers. Secondly, we prove that even if the aggregation rules may succeed in limiting the influence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks. This is the first provably robust method for the standard stochastic optimization setting.},
  langid = {english},
  language = {en}
}

@article{karkouchDataQualityInternet2016,
  title = {Data Quality in Internet of Things: {{A}} State-of-the-Art Survey},
  shorttitle = {Data Quality in Internet of Things},
  author = {Karkouch, Aimad and Mousannif, Hajar and Al Moatassime, Hassan and Noel, Thomas},
  year = {2016},
  month = sep,
  journal = {Journal of Network and Computer Applications},
  volume = {73},
  pages = {57--81},
  issn = {1084-8045},
  urldate = {2023-07-28},
  abstract = {In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.},
  langid = {english},
  language = {english},
  keywords = {Data cleaning,Data quality,Internet of things,Outlier detection}
}

@inproceedings{kearnsEmpiricalStudyRich2019,
  title = {An {{Empirical Study}} of {{Rich Subgroup Fairness}} for {{Machine Learning}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {100--109},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-01-06},
  abstract = {Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
  isbn = {978-1-4503-6125-5},
  keywords = {Algorithmic Bias,Fair Classification,Fairness Auditing,Subgroup Fairness}
}

@book{kearnsIntroductionComputationalLearning1994,
  title = {An Introduction to Computational Learning Theory},
  author = {Kearns, Michael J. and Vazirani, Umesh Virkumar},
  year = {1994},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-11193-5},
  langid = {english},
  language = {english},
  lccn = {Q325.5 .K44 1994},
  keywords = {Algorithms,Artificial intelligence,Machine learning,Neural networks (Computer science)}
}

@inproceedings{kearnsPreventingFairnessGerrymandering2018,
  title = {Preventing {{Fairness Gerrymandering}}: {{Auditing}} and {{Learning}} for {{Subgroup Fairness}}},
  shorttitle = {Preventing {{Fairness Gerrymandering}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  year = {2018},
  month = jul,
  pages = {2564--2572},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-01-06},
  abstract = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning --- which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
  langid = {english},
  language = {en}
}

@inproceedings{keskarLargeBatchTrainingDeep2022,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2022},
  month = jul,
  urldate = {2024-05-27},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$--\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  langid = {english},
  language = {en}
}

@incollection{kesslerTrustProxyMeasure2013,
  title = {Trust as a {{Proxy Measure}} for the {{Quality}} of {{Volunteered Geographic Information}} in the {{Case}} of {{OpenStreetMap}}},
  booktitle = {Geographic {{Information Science}} at the {{Heart}} of {{Europe}}},
  author = {Ke{\ss}ler, Carsten and {de Groot}, Ren{\'e} Theodore Anton},
  editor = {Vandenbroucke, Danny and Bucher, B{\'e}n{\'e}dicte and Crompvoets, Joep},
  year = {2013},
  pages = {21--37},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-04-07},
  abstract = {High availability and diversity make Volunteered Geographic Information (VGI) an interesting source of information for an increasing number of use cases. Varying quality, however, is a concern often raised when it comes to using VGI in professional applications. Recent research directs towards the estimation of VGI quality through the notion of trust as a proxy measure. In this chapter, we investigate which indicators influence trust, focusing on inherent properties that do not require any comparison with a ground truth dataset. The indicators are tested on a sample dataset extracted from OpenStreetMap. High numbers of contributors, versions and confirmations are considered as positive indicators, while corrections and revisions are treated as indicators that have a negative influence on the development of feature trustworthiness. In order to evaluate the trust measure, its results have been compared to the results of a quality measure obtained from a field survey. The quality measure is based on thematic accuracy, topological consistency, and information completeness. To address information completeness as a criterion of data quality, the importance of individual tags for a given feature type was determined based on a method adopted from information retrieval. The results of the comparison between trust assessments and quality measure show significant support for the hypothesis that feature-level VGI data quality can be assessed using a trust model based on data provenance.},
  isbn = {978-3-319-00615-4},
  langid = {english},
  language = {en},
  keywords = {Data Consumer,Data Quality,Ground Truth Data,Trust Assessment,Volunteer Geographic Information}
}

@misc{khaledFirstAnalysisLocal2020,
  title = {First {{Analysis}} of {{Local GD}} on {{Heterogeneous Data}}},
  author = {Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  year = {2020},
  month = mar,
  number = {arXiv:1909.04715},
  eprint = {1909.04715},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-11},
  abstract = {We provide the first convergence analysis of local gradient descent for minimizing the average of smooth and convex but otherwise arbitrary functions. Problems of this form and local gradient descent as a solution method are of importance in federated learning, where each function is based on private data stored by a user on a mobile device, and the data of different users can be arbitrarily heterogeneous. We show that in a low accuracy regime, the method has the same communication complexity as gradient descent.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@misc{khouasTrainingMachineLearning2024,
  title = {Training {{Machine Learning}} Models at the {{Edge}}: {{A Survey}}},
  shorttitle = {Training {{Machine Learning}} Models at the {{Edge}}},
  author = {Khouas, Aymen Rayane and Bouadjenek, Mohamed Reda and Hacid, Hakim and Aryal, Sunil},
  year = {2024},
  month = mar,
  number = {arXiv:2403.02619},
  eprint = {2403.02619},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-20},
  abstract = {Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, libraries, and simulation tools available for EL. In doing so, the paper contributes to a holistic understanding of the current landscape and future directions in the intersection of edge computing and machine learning, paving the way for informed comparisons between optimization methods and techniques designed for edge learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning}
}

@inproceedings{kimExploringClusteredFederated2023,
  title = {Exploring {{Clustered Federated Learning}}'s {{Vulnerability}} against {{Property Inference Attack}}},
  booktitle = {Proceedings of the 26th {{International Symposium}} on {{Research}} in {{Attacks}}, {{Intrusions}} and {{Defenses}}},
  author = {Kim, Hyunjun and Cho, Yungi and Lee, Younghan and Bae, Ho and Paek, Yunheung},
  year = {2023},
  month = oct,
  series = {{{RAID}} '23},
  pages = {236--249},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-03-10},
  abstract = {Clustered federated learning (CFL) is an advanced technique in the field of federated learning (FL) that addresses the issue of catastrophic forgetting caused by non-independent and identically distributed (non-IID) datasets. CFL achieves this by clustering clients based on the similarity of their datasets and training a global model for each cluster. Despite the effectiveness of CFL in mitigating performance degradation resulting from non-IID datasets, the potential risk of privacy leakages in CFL has not been thoroughly studied. Previous work evaluated the risk of privacy leakages in FL using the property inference attack (PIA), which extracts information about unintended properties (i.e., attributes that differ from the target attribute of the global model's main task). In this paper, we explore the potential risk of unintended property leakage in CFL by subjecting it to both passive and active PIAs. Our empirical analysis shows that the passive PIA performance on CFL is substantially better than that on FL in terms of the attack AUC score. Moreover, we propose an enhanced active PIA method tailored for CFL to improve the attack performance. Our method introduces a scale-up parameter that amplifies the impact of malicious local updates, resulting in better performance than the previous technique. Furthermore, we demonstrate that the vulnerability of CFL can be alleviated by applying differential privacy (DP) mechanisms at the client-level. Unlike previous works, which have shown that applying DP to FL can induce a high utility loss, our empirical results indicate that DP can be used as a defense mechanism in CFL, leading to a better trade-off between privacy and utility.},
  isbn = {9798400707650},
  keywords = {clustered federated learning,differential privacy,property inference attack}
}

@inproceedings{kimFederatedLearningLocal2021,
  title = {Federated {{Learning}} with {{Local Differential Privacy}}: {{Trade-Offs Between Privacy}}, {{Utility}}, and {{Communication}}},
  shorttitle = {Federated {{Learning}} with {{Local Differential Privacy}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kim, Muah and G{\"u}nl{\"u}, Onur and Schaefer, Rafael F.},
  year = {2021},
  month = jun,
  pages = {2650--2654},
  issn = {2379-190X},
  urldate = {2024-04-11},
  abstract = {Federated learning (FL) allows to train a massive amount of data privately due to its decentralized structure. Stochastic gradient descent (SGD) is commonly used for FL due to its good empirical performance, but sensitive user information can still be inferred from weight updates shared during FL iterations. We consider Gaussian mechanisms to preserve local differential privacy (LDP) of user data in the FL model with SGD. The trade-offs between user privacy, global utility, and transmission rate are proved by defining appropriate metrics for FL with LDP. Compared to existing results, the query sensitivity used in LDP is defined as a variable, and a tighter privacy accounting method is applied. The proposed utility bound allows heterogeneous parameters over all users. Our bounds characterize how much utility decreases and transmission rate increases if a stronger privacy regime is targeted. Furthermore, given a target privacy level, our results guarantee a significantly larger utility and a smaller transmission rate as compared to existing privacy accounting methods.},
  keywords = {Collaborative work,composition theorems,Conferences,Differential privacy,federated learning (FL),Gaussian randomization,local differential privacy (LDP),Privacy,Sensitivity,Signal processing,stochastic gradient descent (SGD),Stochastic processes}
}

@misc{klinglerEdgeDevicesOndevice,
  title = {Edge {{Devices}} for {{On-device Machine Learning}} and {{Computer Vision}} - {{Viso}}.{{Ai}}},
  author = {Klingler, Nico},
  urldate = {2023-09-23},
  langid = {english},
  language = {english}
}

@misc{kohUnderstandingBlackboxPredictions2020,
  title = {Understanding {{Black-box Predictions}} via {{Influence Functions}}},
  author = {Koh, Pang Wei and Liang, Percy},
  year = {2020},
  month = dec,
  number = {arXiv:1703.04730},
  eprint = {1703.04730},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-27},
  abstract = {How can we explain the predictions of a blackbox model? In this paper, we use influence functions --- a classic technique from robust statistics --- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{koLocalConvergenceTheory2022,
  title = {A {{Local Convergence Theory}} for the {{Stochastic Gradient Descent Method}} in {{Non-Convex Optimization With Non-isolated Local Minima}}},
  author = {Ko, Taehee and Li, Xiantao},
  year = {2022},
  month = may,
  number = {arXiv:2203.10973},
  eprint = {2203.10973},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-29},
  abstract = {Loss functions with non-isolated minima have emerged in several machine learning problems, creating a gap between theory and practice. In this paper, we formulate a new type of local convexity condition that is suitable to describe the behavior of loss functions near non-isolated minima. We show that such condition is general enough to encompass many existing conditions. In addition we study the local convergence of the SGD under this mild condition by adopting the notion of stochastic stability. The corresponding concentration inequalities from the convergence analysis help to interpret the empirical observation from some practical training results.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@inproceedings{kolomvatsosSpatioTemporalDataImputation2019,
  title = {A {{Spatio-Temporal Data Imputation Model}} for {{Supporting Analytics}} at the {{Edge}}},
  booktitle = {Digital {{Transformation}} for a {{Sustainable Society}} in the 21st {{Century}}},
  author = {Kolomvatsos, Kostas and Papadopoulou, Panagiota and Anagnostopoulos, Christos and Hadjiefthymiades, Stathes},
  editor = {Pappas, Ilias O. and Mikalef, Patrick and Dwivedi, Yogesh K. and Jaccheri, Letizia and Krogstie, John and M{\"a}ntym{\"a}ki, Matti},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {138--150},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {Current applications developed for the Internet of Things (IoT) usually involve the processing of collected data for delivering analytics and support efficient decision making. The basis for any processing mechanism is data analysis, usually having as an outcome responses in various analytics queries defined by end users or applications. However, as already noted in the respective literature, data analysis cannot be efficient when missing values are present. The research community has already proposed various missing data imputation methods paying more attention of the statistical aspect of the problem. In this paper, we study the problem and propose a method that combines machine learning and a consensus scheme. We focus on the clustering of the IoT devices assuming they observe the same phenomenon and report the collected data to the edge infrastructure. Through a sliding window approach, we try to detect IoT nodes that report similar contextual values to edge nodes and base on them to deliver the replacement value for missing data. We provide the description of our model together with results retrieved by an extensive set of simulations on top of real data. Our aim is to reveal the potentials of the proposed scheme and place it in the respective literature.},
  isbn = {978-3-030-29374-1},
  langid = {english},
  language = {en},
  keywords = {Clustering,Consensus,Edge computing,Internet of things,Missing values imputation}
}

@misc{konecnyFederatedOptimizationDistributed2015,
  title = {Federated {{Optimization}}:{{Distributed Optimization Beyond}} the {{Datacenter}}},
  shorttitle = {Federated {{Optimization}}},
  author = {Kone{\v c}n{\'y}, Jakub and McMahan, Brendan and Ramage, Daniel},
  year = {2015},
  month = nov,
  number = {arXiv:1511.03575},
  eprint = {1511.03575},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-01-20},
  abstract = {We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed (unevenly) over an extremely large number of {\textbackslash}nodes, but the goal remains to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of utmost importance. A motivating example for federated optimization arises when we keep the training data locally on users' mobile devices rather than logging it to a data center for training. Instead, the mobile devices are used as nodes performing computation on their local data in order to update a global model. We suppose that we have an extremely large number of devices in our network, each of which has only a tiny fraction of data available totally; in particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, we assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results. This work also sets a path for future research needed in the context of federated optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@inproceedings{krishnapriyanCharacterizingPossibleFailure2021,
  title = {Characterizing Possible Failure Modes in Physics-Informed Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krishnapriyan, Aditi and Gholami, Amir and Zhe, Shandian and Kirby, Robert and Mahoney, Michael W},
  year = {2021},
  volume = {34},
  pages = {26548--26560},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-19},
  abstract = {Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.}
}

@article{krizhevskyLearningMultipleLayers,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  langid = {english},
  language = {en}
}

@book{kulkarniElementaryIntroductionStatistical2011,
  title = {An Elementary Introduction to Statistical Learning Theory},
  author = {Kulkarni, Sanjeev and Harman, Gilbert},
  year = {2011},
  series = {Wiley Series in Probability and Statistics},
  publisher = {Wiley},
  address = {Hoboken, N.J},
  abstract = {"A joint endeavor from leading researchers in the fields of philosophy and electrical engineering An Introduction to Statistical Learning Theory provides a broad and accessible introduction to rapidly evolving field of statistical pattern recognition and statistical learning theory. Exploring topics that are not often covered in introductory level books on statistical learning theory, including PAC learning, VC dimension, and simplicity, the authors present upper-undergraduate and graduate levels with the basic theory behind contemporary machine learning and uniquely suggest it serves as an excellent framework for philosophical thinking about inductive inference"--Back cover},
  isbn = {978-0-470-64183-5 978-1-118-02343-3 978-1-118-02346-4 978-1-118-02347-1},
  lccn = {Q325.5 .K85 2011},
  keywords = {Machine learning,Pattern recognition systems,Statistical methods},
  annotation = {OCLC: ocn685239939}
}

@article{kulynychDisparateVulnerabilityMembership2022,
  title = {Disparate {{Vulnerability}} to {{Membership Inference Attacks}}},
  author = {Kulynych, Bogdan and Yaghini, Mohammad and Cherubin, Giovanni and Veale, Michael and Troncoso, Carmela},
  year = {2022},
  journal = {Proceedings on Privacy Enhancing Technologies},
  issn = {2299-0984},
  urldate = {2024-03-12}
}

@article{kumarBlackHolesLoss2023,
  title = {Black {{Holes}} and the Loss Landscape in Machine Learning},
  author = {Kumar, Pranav and Mandal, Taniya and Mondal, Swapnamay},
  year = {2023},
  month = oct,
  journal = {Journal of High Energy Physics},
  volume = {2023},
  number = {10},
  pages = {107},
  issn = {1029-8479},
  urldate = {2024-05-07},
  abstract = {Understanding the loss landscape is an important problem in machine learning. One key feature of the loss function, common to many neural network architectures, is the presence of exponentially many low lying local minima. Physical systems with similar energy landscapes may provide useful insights. In this work, we point out that black holes naturally give rise to such landscapes, owing to the existence of black hole entropy. For definiteness, we consider 1/8 BPS black holes in \$\$ {\textbackslash}mathcal\{N\} \$\$= 8 string theory. These provide an infinite family of potential landscapes arising in the microscopic descriptions of corresponding black holes. The counting of minima amounts to black hole microstate counting. Moreover, the exact numbers of the minima for these landscapes are a priori known from dualities in string theory. Some of the minima are connected by paths of low loss values, resembling mode connectivity. We estimate the number of runs needed to find all the solutions. Initial explorations suggest that Stochastic Gradient Descent can find a significant fraction of the minima.},
  langid = {english},
  language = {en},
  keywords = {Black Holes in String Theory,D-Branes}
}

@misc{kumarDifferentiallyPrivateEmpirical2019,
  title = {Differentially {{Private Empirical Risk Minimization}} with {{Sparsity-Inducing Norms}}},
  author = {Kumar, K. S. Sesh and Deisenroth, Marc Peter},
  year = {2019},
  month = may,
  number = {arXiv:1905.04873},
  eprint = {1905.04873},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-02},
  abstract = {Differential privacy is concerned about the prediction quality while measuring the privacy impact on individuals whose information is contained in the data. We consider differentially private risk minimization problems with regularizers that induce structured sparsity. These regularizers are known to be convex but they are often non-differentiable. We analyze the standard differentially private algorithms, such as output perturbation, Frank-Wolfe and objective perturbation. Output perturbation is a differentially private algorithm that is known to perform well for minimizing risks that are strongly convex. Previous works have derived excess risk bounds that are independent of the dimensionality. In this paper, we assume a particular class of convex but non-smooth regularizers that induce structured sparsity and loss functions for generalized linear models. We also consider differentially private Frank-Wolfe algorithms to optimize the dual of the risk minimization problem. We derive excess risk bounds for both these algorithms. Both the bounds depend on the Gaussian width of the unit ball of the dual norm. We also show that objective perturbation of the risk minimization problems is equivalent to the output perturbation of a dual optimization problem. This is the first work that analyzes the dual optimization problems of risk minimization problems in the context of differential privacy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kuninLossLandscapesRegularized2019,
  title = {Loss {{Landscapes}} of {{Regularized Linear Autoencoders}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kunin, Daniel and Bloom, Jonathan and Goeva, Aleksandrina and Seed, Cotton},
  year = {2019},
  month = may,
  pages = {3560--3569},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-07-03},
  abstract = {Autoencoders are a deep learning model for representation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the top principal directions but cannot learn the principal directions themselves. In this paper, we prove that L2L2L\_2-regularized LAEs are symmetric at all critical points and learn the principal directions as the left singular vectors of the decoder. We smoothly parameterize the critical manifold and relate the minima to the MAP estimate of probabilistic PCA. We illustrate these results empirically and consider implications for PCA algorithms, computational neuroscience, and the algebraic topology of learning.},
  langid = {english},
  language = {en}
}

@inproceedings{kuninNeuralMechanicsSymmetry2020,
  title = {Neural {{Mechanics}}: {{Symmetry}} and {{Broken Conservation Laws}} in {{Deep Learning Dynamics}}},
  shorttitle = {Neural {{Mechanics}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kunin, Daniel and {Sagastuy-Brena}, Javier and Ganguli, Surya and Yamins, Daniel LK and Tanaka, Hidenori},
  year = {2020},
  month = oct,
  urldate = {2024-06-29},
  abstract = {Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.},
  langid = {english},
  language = {en}
}

@inproceedings{kusnerCounterfactualFairness2017,
  title = {Counterfactual {{Fairness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-12-30},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.  In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.  Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it  the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.}
}

@inproceedings{lampinenAnalyticTheoryGeneralization2018,
  title = {An Analytic Theory of Generalization Dynamics and Transfer Learning in Deep Linear Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lampinen, Andrew K. and Ganguli, Surya},
  year = {2018},
  month = sep,
  urldate = {2024-05-18},
  abstract = {Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.},
  langid = {english},
  language = {en}
}

@article{lamportByzantineGeneralsProblem1982,
  title = {The {{Byzantine Generals Problem}}},
  author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
  year = {1982},
  month = jul,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {4},
  number = {3},
  pages = {382--401},
  issn = {0164-0925, 1558-4593},
  urldate = {2023-10-22},
  langid = {english},
  language = {en}
}

@inproceedings{larsenHowManyDegrees2021,
  title = {How Many Degrees of Freedom Do We Need to Train Deep Networks: A Loss Landscape Perspective},
  shorttitle = {How Many Degrees of Freedom Do We Need to Train Deep Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Larsen, Brett W. and Fort, Stanislav and Becker, Nic and Ganguli, Surya},
  year = {2021},
  month = oct,
  urldate = {2024-07-13},
  abstract = {A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality. We find a sharp phase transition in the success probability from \$0\$ to \$1\$ as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of properties of the high-dimensional geometry of the loss landscape. In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sublevel set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large. In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total parameters, implying by our theory that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sublevel sets is very large. Moreover, we compare this threshold training dimension to more sophisticated ways of reducing training degrees of freedom, including lottery tickets as well as a new, analogous method: lottery subspaces.},
  langid = {english},
  language = {en}
}

@inproceedings{laueComputingHigherOrder2018,
  title = {Computing {{Higher Order Derivatives}} of {{Matrix}} and {{Tensor Expressions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Laue, Soeren and Mitterreiter, Matthias and Giesen, Joachim},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-06-09},
  abstract = {Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly.  Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives.}
}

@misc{lauerUniformRiskBounds2023,
  title = {Uniform {{Risk Bounds}} for {{Learning}} with {{Dependent Data Sequences}}},
  author = {Lauer, Fabien},
  year = {2023},
  month = mar,
  number = {arXiv:2303.11650},
  eprint = {2303.11650},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-14},
  abstract = {This paper extends standard results from learning theory with independent data to sequences of dependent data. Contrary to most of the literature, we do not rely on mixing arguments or sequential measures of complexity and derive uniform risk bounds with classical proof patterns and capacity measures. In particular, we show that the standard classification risk bounds based on the VC-dimension hold in the exact same form for dependent data, and further provide Rademacher complexity-based bounds, that remain unchanged compared to the standard results for the identically and independently distributed case. Finally, we show how to apply these results in the context of scenario-based optimization in order to compute the sample complexity of random programs with dependent constraints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{laueSimpleEfficientTensor2020,
  title = {A {{Simple}} and {{Efficient Tensor Calculus}}},
  author = {Laue, S{\"o}ren and Mitterreiter, Matthias and Giesen, Joachim},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {4527--4534},
  issn = {2374-3468},
  urldate = {2024-06-09},
  abstract = {Computing derivatives of tensor expressions, also known as tensor calculus, is a fundamental task in machine learning. A key concern is the efficiency of evaluating the expressions and their derivatives that hinges on the representation of these expressions. Recently, an algorithm for computing higher order derivatives of tensor expressions like Jacobians or Hessians has been introduced that is a few orders of magnitude faster than previous state-of-the-art approaches. Unfortunately, the approach is based on Ricci notation and hence cannot be incorporated into automatic differentiation frameworks like TensorFlow, PyTorch, autograd, or JAX that use the simpler Einstein notation. This leaves two options, to either change the underlying tensor representation in these frameworks or to develop a new, provably correct algorithm based on Einstein notation. Obviously, the first option is impractical. Hence, we pursue the second option. Here, we show that using Ricci notation is not necessary for an efficient tensor calculus and develop an equally efficient method for the simpler Einstein notation. It turns out that turning to Einstein notation enables further improvements that lead to even better efficiency.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en}
}

@article{leeConvergenceFederatedLearning2023,
  title = {Towards {{Convergence}} in {{Federated Learning}} via {{Non-IID Analysis}} in a {{Distributed Solar Energy Grid}}},
  author = {Lee, Hyeongok},
  year = {2023},
  month = jan,
  journal = {Electronics},
  volume = {12},
  number = {7},
  pages = {1580},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  urldate = {2024-01-20},
  abstract = {Federated Learning (FL) is an effective framework for a distributed system that constructs a powerful global deep learning model, which diminishes the local bias and accommodates the successful aggregation of locally trained models with heterogeneous datasets. However, when local datasets have the non-IID attribute, the optimization metric tends to diverge or show unstable convergence in the trajectory space. This paper delves into building a global model for the distributed Smart Grid environment, with regionally cumulated three solar energy datasets from January 2017 to August 2021 in a decentralized power grid in South Korea via FL. This distributed energy network involves local properties and physical distance between the regions, which raises a fundamental question of ``Will time-serially curated non-IID local features be effective in constructing a global regression model?''. This paper probes this question by leveraging FL and conducts the theoretically viable non-IID case-by-case convergence analysis, providing the interpretation of the embedded temporal non-IID features and application on real-world data. Moreover, most of the FL studies predetermine the global update period, which lacks applicability when adapting FL in actual practice. As FL is a cumulative-basis structure, the update term is a crucial factor that needs to be carefully selected. This paper articulates this problem and explores the effective update period via multiple experiments on the 4.5 years of solar energy dataset, and to the best of my knowledge, this is the first literature that presents the optimal update period in the FL regression in an energy domain.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {convergence optimization,decentralized power grid environment,federated learning regression,IID and Non-IID local dataset,solar energy prediction}
}

@misc{leeLargeDimensionalMultibodyDynamics2022,
  title = {Large-{{Dimensional Multibody Dynamics Simulation Using Contact Nodalization}} and {{Diagonalization}}},
  author = {Lee, Jeongmin and Lee, Minji and Lee, Dongjun},
  year = {2022},
  month = aug,
  number = {arXiv:2201.09212},
  eprint = {2201.09212},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {We propose a novel multibody dynamics simulation framework that can efficiently deal with large-dimensionality and complementarity multi-contact conditions. Typical contact simulation approaches require performing contact impulse fixedpoint iteration (I-FPI), which has high time-complexity from large-size matrix factorization and multiplication, as well as susceptibility to ill-conditioned contact situations. To circumvent this, we propose a novel framework based on velocity fixedpoint iteration (V-FPI), which, by utilizing a certain surrogate dynamics and contact nodalization (with virtual nodes), we achieve not only inter-contact decoupling but also their interaxes decoupling (i.e., contact diagonalization) at each iteration step. This then enables us to one-shot/parallel-solve the contact problem during each V-FPI iteration-loop, while avoiding largesize/dense matrix inversion/multiplication, thereby, significantly speeding up the simulation time with improved convergence property. We theoretically show that the solution of our framework is consistent with that of the original problem and, further, elucidate mathematical conditions for the convergence of our proposed solver. Performance and properties of our proposed simulation framework are also demonstrated and experimentally-validated for various large-dimensional/multi-contact scenarios including deformable objects.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Robotics}
}

@inproceedings{leeNewCharacterizationEdge2022,
  title = {A New Characterization of the Edge of Stability Based on a Sharpness Measure Aware of Batch Gradient Distribution},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Lee, Sungyoon and Jang, Cheongjae},
  year = {2022},
  month = sep,
  urldate = {2024-05-25},
  abstract = {For full-batch gradient descent (GD), it has been empirically shown that the sharpness, the top eigenvalue of the Hessian, increases and then hovers above \$2/{\textbackslash}text\{(learning rate)\}\$, and this is called ``the edge of stability'' phenomenon. However, it is unclear why the sharpness is somewhat larger than \$2/{\textbackslash}text\{(learning rate)\}\$ and how this can be extended to general mini-batch stochastic gradient descent (SGD). We propose a new sharpness measure (interaction-aware-sharpness) aware of the {\textbackslash}emph\{interaction\} between the batch gradient distribution and the loss landscape geometry. This leads to a more refined and general characterization of the edge of stability for SGD. Moreover, based on the analysis of a concentration measure of the batch gradient, we propose a more accurate scaling rule, Linear and Saturation Scaling Rule (LSSR), between batch size and learning rate.},
  langid = {english},
  language = {en}
}

@misc{leeSurveySocialBias2023,
  title = {Survey of {{Social Bias}} in {{Vision-Language Models}}},
  author = {Lee, Nayeon and Bang, Yejin and Lovenia, Holy and Cahyawijaya, Samuel and Dai, Wenliang and Fung, Pascale},
  year = {2023},
  month = sep,
  number = {arXiv:2309.14381},
  eprint = {2309.14381},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-17},
  abstract = {In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community. The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across NLP, CV, and VL. By examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. The findings and recommendations presented here can benefit the ML community, fostering the development of fairer and non-biased AI models in various applications and research endeavors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{leeUnderstandingEffectsData2020,
  title = {Understanding the Effects of Data Parallelism and Sparsity on Neural Network Training},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip and Jaggi, Martin},
  year = {2020},
  month = oct,
  urldate = {2024-05-11},
  abstract = {We study two factors in neural network training: data parallelism and sparsity; here, data parallelism means processing training data in parallel using distributed systems (or equivalently increasing batch size), so that training can be accelerated; for sparsity, we refer to pruning parameters in a neural network model, so as to reduce computational and memory cost. Despite their promising benefits, however, understanding of their effects on neural network training remains elusive. In this work, we first measure these effects rigorously by conducting extensive experiments while tuning all metaparameters involved in the optimization. As a result, we find across various workloads of data set, network model, and optimization algorithm that there exists a general scaling trend between batch size and number of training steps to convergence for the effect of data parallelism, and further, difficulty of training under sparsity. Then, we develop a theoretical analysis based on the convergence properties of stochastic gradient methods and smoothness of the optimization landscape, which illustrates the observed phenomena precisely and generally, establishing a better account of the effects of data parallelism and sparsity on neural network training.},
  langid = {english},
  language = {en}
}

@inproceedings{leeWideNeuralNetworks2019,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and {Sohl-Dickstein}, Jascha and Pennington, Jeffrey},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-12},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.}
}

@article{leiStochasticGradientDescent2020,
  title = {Stochastic {{Gradient Descent}} for {{Nonconvex Learning Without Bounded Gradient Assumptions}}},
  author = {Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  year = {2020},
  month = oct,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {10},
  pages = {4394--4400},
  issn = {2162-2388},
  urldate = {2024-05-29},
  abstract = {Stochastic gradient descent (SGD) is a popular and efficient method with wide applications in training deep neural nets and other nonconvex models. While the behavior of SGD is well understood in the convex learning setting, the existing theoretical results for SGD applied to nonconvex objective functions are far from mature. For example, existing results require to impose a nontrivial assumption on the uniform boundedness of gradients for all iterates encountered in the learning process, which is hard to verify in practical implementations. In this article, we establish a rigorous theoretical foundation for SGD in nonconvex learning by showing that this boundedness assumption can be removed without affecting convergence rates, and relaxing the standard smoothness assumption to H{\"o}lder continuity of gradients. In particular, we establish sufficient conditions for almost sure convergence as well as optimal convergence rates for SGD applied to both general nonconvex and gradient-dominated objective functions. A linear convergence is further derived in the case with zero variances.},
  keywords = {Convergence,Learning systems,Learning theory,Loss measurement,Nickel,nonconvex optimization,Optimization,Polyak-Lojasiewicz condition,stochastic gradient descent (SGD),Stochastic processes,Training}
}

@article{lequySurveyDatasetsFairnessaware2022,
  title = {A Survey on Datasets for Fairness-Aware Machine Learning},
  author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
  year = {2022},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {12},
  number = {3},
  pages = {e1452},
  issn = {1942-4795},
  urldate = {2024-01-21},
  abstract = {As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under: Commercial, Legal, and Ethical Issues {$>$} Fairness in Data Mining Fundamental Concepts of Data and Knowledge {$>$} Data Concepts Technologies {$>$} Data Preprocessing},
  langid = {english},
  language = {en},
  keywords = {benchmark datasets,bias,datasets for fairness,discrimination,fairness-aware machine learning}
}

@inproceedings{lianCanDecentralizedAlgorithms2017,
  title = {Can {{Decentralized Algorithms Outperform Centralized Algorithms}}? {{A Case Study}} for {{Decentralized Parallel Stochastic Gradient Descent}}},
  shorttitle = {Can {{Decentralized Algorithms Outperform Centralized Algorithms}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-24},
  abstract = {Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart?  Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.}
}

@article{liangReviewSurveyLearning2022,
  title = {Review--{{A Survey}} of {{Learning}} from {{Noisy Labels}}},
  author = {Liang, Xuefeng and Liu, Xingyu and Yao, Longshan},
  year = {2022},
  month = jun,
  journal = {ECS Sensors Plus},
  volume = {1},
  number = {2},
  pages = {021401},
  publisher = {IOP Publishing},
  issn = {2754-2726},
  urldate = {2024-01-23},
  abstract = {Deep Learning has achieved remarkable successes in many industry applications and scientific research fields. One essential reason is that deep models can learn rich information from large-scale training datasets through supervised learning. It has been well accepted that the robust deep models heavily rely on the quality of data labels. However, current large-scale datasets mostly involve noisy labels, which are caused by sensor errors, human mistakes, or inaccuracy of search engines, and may severely degrade the performance of deep models. In this survey, we summaries existing works on noisy label learning into two main categories, Loss Correction and Sample Selection, and present their methodologies, commonly used experimental setups, datasets, and the state-of-the-art results. Finally, we discuss a promising research direction that might be valuable for the future study.},
  langid = {english},
  language = {en}
}

@inproceedings{liangUnderstandingLossSurface2018,
  title = {Understanding the {{Loss Surface}} of {{Neural Networks}} for {{Binary Classification}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  year = {2018},
  month = jul,
  pages = {2835--2843},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-22},
  abstract = {It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.},
  langid = {english},
  language = {en}
}

@inproceedings{liCleanMLStudyEvaluating2021,
  title = {{{CleanML}}: {{A Study}} for {{Evaluating}} the {{Impact}} of {{Data Cleaning}} on {{ML Classification Tasks}}},
  shorttitle = {{{CleanML}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Li, Peng and Rao, Xi and Blase, Jennifer and Zhang, Yue and Chu, Xu and Zhang, Ce},
  year = {2021},
  month = apr,
  pages = {13--24},
  issn = {2375-026X},
  urldate = {2024-05-07},
  abstract = {Data quality affects machine learning (ML) model performances, and data scientists spend considerable amount of time on data cleaning before model training. However, to date, there does not exist a rigorous study on how exactly cleaning affects ML - ML community usually focuses on developing ML algorithms that are robust to some particular noise types of certain distributions, while database (DB) community has been mostly studying the problem of data cleaning alone without considering how data is consumed by downstream ML analytics.We propose a CleanML study that systematically investigates the impact of data cleaning on ML classification tasks. The open-source and extensible CleanML study currently includes 14 real-world datasets with real errors, five common error types, seven different ML models, and multiple cleaning algorithms for each error type (including both commonly used algorithms in practice as well as state-of-the-art solutions in academic literature). We control the randomness in ML experiments using statistical hypothesis testing, and we also control false discovery rate in our experiments using the Benjamini-Yekutieli (BY) procedure. We analyze the results in a systematic way to derive many interesting and nontrivial observations. We also put forward multiple research directions for researchers.},
  keywords = {Classification algorithms,Cleaning,Data Cleaning,Data models,Machine learning,Machine Learning,Machine learning algorithms,Systematics,Training}
}

@misc{liConvergenceFedAvgNonIID2020,
  title = {On the {{Convergence}} of {{FedAvg}} on {{Non-IID Data}}},
  author = {Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  year = {2020},
  month = jun,
  number = {arXiv:1907.02189},
  eprint = {1907.02189},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-01-20},
  abstract = {Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging ({\textbackslash}texttt\{FedAvg\}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of {\textbackslash}texttt\{FedAvg\} on non-iid data and establish a convergence rate of \${\textbackslash}mathcal\{O\}({\textbackslash}frac\{1\}\{T\})\$ for strongly convex and smooth problems, where \$T\$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning. Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for {\textbackslash}texttt\{FedAvg\} on non-iid data: the learning rate \${\textbackslash}eta\$ must decay, even if full-gradient is used; otherwise, the solution will be \${\textbackslash}Omega ({\textbackslash}eta)\$ away from the optimal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@inproceedings{liConvergenceTheoryFederated2022,
  title = {A {{Convergence Theory}} for {{Federated Average}}: {{Beyond Smoothness}}},
  shorttitle = {A {{Convergence Theory}} for {{Federated Average}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Li, Xiaoxiao and Song, Zhao and Tao, Runzhou and Zhang, Guangyi},
  year = {2022},
  month = dec,
  pages = {1292--1297},
  urldate = {2024-05-12},
  abstract = {Federated learning enables a large amount of edge computing devices to learn a model without data sharing jointly. As a leading algorithm in this setting, Federated Average (FedAvg), which runs Stochastic Gradient Descent (SGD) in parallel on local devices and averages the sequences only once in a while, have been widely used due to their simplicity and low communication cost. However, despite recent research efforts, it lacks theoretical analysis under assumptions beyond smoothness. In this paper, we analyze the convergence of FedAvg. Different from the existing work, we relax the assumption of strong smoothness. More specifically, we assume the semi-smoothness and semi-Lipschitz properties for the loss function, which have an additional first-order term in assumption definitions. In addition, we also assume bound on the gradient, which is weaker than the commonly used bounded gradient assumption in the convergence analysis scheme. As a solution, this paper provides a theoretical convergence study on Federated Learning.},
  keywords = {Big Data,Computational modeling,Convergence,Costs,Data models,Federated learning,no-critical-point,semi-Lipschitz,semi-smoothness,Stochastic processes}
}

@article{liFederatedLearningChallenges2020,
  title = {Federated {{Learning}}: {{Challenges}}, {{Methods}}, and {{Future Directions}}},
  shorttitle = {Federated {{Learning}}},
  author = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  year = {2020},
  month = may,
  journal = {IEEE Signal Processing Magazine},
  volume = {37},
  number = {3},
  pages = {50--60},
  issn = {1558-0792},
  urldate = {2023-11-27},
  abstract = {Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.}
}

@article{liFederatedMetalearningSpatialtemporal2022,
  title = {Federated Meta-Learning for Spatial-Temporal Prediction},
  author = {Li, Wenzhu and Wang, Shuang},
  year = {2022},
  month = jul,
  journal = {Neural Computing and Applications},
  volume = {34},
  number = {13},
  pages = {10355--10374},
  issn = {1433-3058},
  urldate = {2024-04-17},
  abstract = {Spatial-temporal prediction is a fundamental problem for constructing smart city, and existing approaches by deep learning models have achieved excellent success based on a large volume of datasets. However, data privacy of cities becomes the public concerns in recent years. Therefore, how to develop accurate spatial-temporal prediction while preserving privacy is a significant problem. To address this challenge, we propose a privacy-preserving spatial-temporal prediction technique via federated learning (FL). Due to inherent non-independent identically distributed (non-IID) characteristic of spatial-temporal data, the basic FL-based method cannot deal with this data heterogeneity well by sharing global model; furthermore, we propose the personalized federated learning methods based on meta-learning. We automatically construct the global spatial-temporal pattern graph under a data federation. This global pattern graph incorporates and memorizes the local learned patterns of all of the clients, and each client leverages those global patterns to customize its own model by evaluating the difference between global and local pattern graph. Then, each client could use this customized parameters as its model initialization parameters for spatial-temporal prediction tasks. We conduct extensive experiments on bike sharing datasets to demonstrate the superiority and effectiveness of our methods in privacy protection settings.},
  langid = {english},
  language = {en},
  keywords = {Federated learning,Meta-learning,Spatial-temporal prediction}
}

@article{liGeometricStrategyAlgorithm2019,
  title = {A {{Geometric Strategy Algorithm}} for {{Orthogonal Projection}} onto a {{Parametric Surface}}},
  author = {Li, Xiaowu and Wu, Zhinan and Pan, Feng and Liang, Juan and Zhang, Jiafeng and Hou, Linke},
  year = {2019},
  month = nov,
  journal = {Journal of Computer Science and Technology},
  volume = {34},
  number = {6},
  pages = {1279--1293},
  issn = {1860-4749},
  urldate = {2024-09-14},
  abstract = {In this paper, we investigate how to compute the minimum distance between a point and a parametric surface, and then to return the nearest point (foot point) on the surface as well as its corresponding parameter, which is also called the point projection problem of a parametric surface. The geometric strategy algorithm (hereafter GSA) presented consists of two parts as follows. The normal curvature to a given parametric surface is used to find the corresponding foot point firstly, and then the Taylor's expansion of the parametric surface is employed to compute parameter increments and to get the iteration formula to calculate the orthogonal projection point of test point to the parametric surface. Our geometric strategy algorithm is essentially dependent on the geometric property of the normal curvature, and performs better than existing methods in two ways. Firstly, GSA converges faster than existing methods, such as the method to turn the problem into a root-finding of nonlinear system, subdividing methods, clipping methods, geometric methods (tangent vector and geometric curvature) and hybrid second-order method, etc. Specially, it converges faster than the classical Newton's iterative method. Secondly, GSA is independent of the initial iterative value, which we prove in Theorem 1. Many numerical examples confirm GSA's robustness and efficiency.},
  langid = {english},
  language = {en},
  keywords = {Artificial Intelligence,convergence analysis,normal curvature,normal curvature sphere,point inversion problem,point projection problem}
}

@inproceedings{liMeasuringIntrinsicDimension2018,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  year = {2018},
  month = feb,
  urldate = {2024-07-10},
  abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
  langid = {english},
  language = {en}
}

@article{liModelArchitectureLevel2023,
  title = {Model Architecture Level Privacy Leakage in Neural Networks},
  author = {Li, Yan and Yan, Hongyang and Huang, Teng and Pan, Zijie and Lai, Jiewei and Zhang, Xiaoxue and Chen, Kongyang and Li, Jin},
  year = {2023},
  month = oct,
  journal = {Science China Information Sciences},
  volume = {67},
  number = {3},
  pages = {132101},
  issn = {1869-1919},
  urldate = {2024-03-12},
  abstract = {Privacy leakage is one of the most critical issues in machine learning and has attracted growing interest for tasks such as demonstrating potential threats in model attacks and creating model defenses. In recent years, numerous studies have revealed various privacy leakage risks (e.g., data reconstruction attack, membership inference attack, backdoor attack, and adversarial attack) and several targeted defense approaches (e.g., data denoising, differential privacy, and data encryption). However, existing solutions generally focus on model parameter levels to disclose (or repair) privacy threats during the model training and/or model interference process, which are rarely applied at the model architecture level. Thus, in this paper, we aim to exploit the potential privacy leakage at the model architecture level through a pioneer study on neural architecture search (NAS) paradigms which serves as a powerful tool to automate a neural network design. By investigating the NAS procedure, we discover two attack threats in the model architecture level called the architectural dataset reconstruction attack and the architectural membership inference attack. Our theoretical analysis and experimental evaluation reveal that an attacker may leverage the output architecture of an ongoing NAS paradigm to reconstruct its original training set, or accurately infer the memberships of its training set simply from the model architecture. In this work, we also propose several defense approaches related to these model architecture attacks. We hope our work can highlight the need for greater attention to privacy protection in model architecture levels (e.g., NAS paradigms).},
  langid = {english},
  language = {en},
  keywords = {data reconstruction attack,membership inference attack,neural architecture search}
}

@article{lindellSecureMultipartyComputation2021,
  title = {Secure Multiparty Computation},
  author = {Lindell, Yehuda},
  year = {2021},
  month = jan,
  journal = {Communications of the ACM},
  volume = {64},
  number = {1},
  pages = {86--96},
  issn = {0001-0782, 1557-7317},
  urldate = {2024-04-11},
  abstract = {MPC has moved from theoretical study to real-world usage. How is it doing?},
  langid = {english},
  language = {en}
}

@article{linDistributionallyRobustOptimization2022,
  title = {Distributionally {{Robust Optimization}}: {{A}} Review on Theory and Applications},
  shorttitle = {Distributionally {{Robust Optimization}}},
  author = {Lin, Fengming and Fang, Xiaolei and Gao, Zheming},
  year = {Mon Feb 28 19:00:00 EST 2022},
  journal = {Numerical Algebra, Control and Optimization},
  volume = {12},
  number = {1},
  pages = {159--212},
  publisher = {{Numerical Algebra, Control and Optimization}},
  issn = {2155-3289},
  urldate = {2023-12-03},
  abstract = {In this paper, we survey the primary research on the theory and applications of distributionally robust optimization (DRO). We start with reviewing the modeling power and computational attractiveness of DRO approaches, induced by the ambiguity sets structure and tractable robust counterpart reformulations. Next, we summarize the efficient solution methods, out-of-sample performance guarantee, and convergence analysis. Then, we illustrate some applications of DRO in machine learning and operations research, and finally, we discuss the future research directions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en}
}

@misc{liReviewTaxonomyEdge2023,
  title = {A {{Review}} and a {{Taxonomy}} of {{Edge Machine Learning}}: {{Requirements}}, {{Paradigms}}, and {{Techniques}}},
  shorttitle = {A {{Review}} and a {{Taxonomy}} of {{Edge Machine Learning}}},
  author = {Li, Wenbin and Hacid, Hakim and Almazrouei, Ebtesam and Debbah, Merouane},
  year = {2023},
  month = feb,
  number = {arXiv:2302.08571},
  eprint = {2302.08571},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-05},
  abstract = {The union of Edge Computing (EC) and Artificial Intelligence (AI) has brought forward the Edge AI concept to provide intelligent solutions close to end-user environment, for privacy preservation, low latency to real-time performance, as well as resource optimization. Machine Learning (ML), as the most advanced branch of AI in the past few years, has shown encouraging results and applications in the edge environment. Nevertheless, edge powered ML solutions are more complex to realize due to the joint constraints from both edge computing and AI domains, and the corresponding solutions are expected to be efficient and adapted in technologies such as data processing, model compression, distributed inference, and advanced learning paradigms for Edge ML requirements. Despite that a great attention of Edge ML is gained in both academic and industrial communities, we noticed the lack of a complete survey on existing Edge ML technologies to provide a common understanding of this concept. To tackle this, this paper aims at providing a comprehensive taxonomy and a systematic review of Edge ML techniques: we start by identifying the Edge ML requirements driven by the joint constraints. We then survey more than twenty paradigms and techniques along with their representative work, covering two main parts: edge inference, and edge learning. In particular, we analyze how each technique fits into Edge ML by meeting a subset of the identified requirements. We also summarize Edge ML open issues to shed light on future directions for Edge ML.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning}
}

@inproceedings{liRevisitingWeightedAggregation2023,
  title = {Revisiting {{Weighted Aggregation}} in {{Federated Learning}} with {{Neural Networks}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Zexi and Lin, Tao and Shang, Xinyi and Wu, Chao},
  year = {2023},
  month = jul,
  pages = {19767--19788},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-06},
  abstract = {In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Aggregation Weights, named as FedLAW. Extensive experiments verify that our method can improve the generalization of the global model by a large margin on different datasets and models.},
  langid = {english},
  language = {en}
}

@article{liRSAByzantineRobustStochastic2019,
  title = {{{RSA}}: {{Byzantine-Robust Stochastic Aggregation Methods}} for {{Distributed Learning}} from {{Heterogeneous Datasets}}},
  shorttitle = {{{RSA}}},
  author = {Li, Liping and Xu, Wei and Chen, Tianyi and Giannakis, Georgios B. and Ling, Qing},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {1544--1551},
  issn = {2374-3468},
  urldate = {2024-03-25},
  abstract = {In this paper, we propose a class of robust stochastic subgradient methods for distributed learning from heterogeneous datasets at presence of an unknown number of Byzantine workers. The Byzantine workers, during the learning process, may send arbitrary incorrect messages to the master due to data corruptions, communication failures or malicious attacks, and consequently bias the learned model. The key to the proposed methods is a regularization term incorporated with the objective function so as to robustify the learning task and mitigate the negative effects of Byzantine attacks. The resultant subgradient-based algorithms are termed Byzantine-Robust Stochastic Aggregation methods, justifying our acronym RSA used henceforth. In contrast to most of the existing algorithms, RSA does not rely on the assumption that the data are independent and identically distributed (i.i.d.) on the workers, and hence fits for a wider class of applications. Theoretically, we show that: i) RSA converges to a near-optimal solution with the learning error dependent on the number of Byzantine workers; ii) the convergence rate of RSA under Byzantine attacks is the same as that of the stochastic gradient descent method, which is free of Byzantine attacks. Numerically, experiments on real dataset corroborate the competitive performance of RSA and a complexity reduction compared to the state-of-the-art alternatives.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en}
}

@inproceedings{liSamplelevelDataSelection2021,
  title = {Sample-Level {{Data Selection}} for {{Federated Learning}}},
  booktitle = {{{IEEE INFOCOM}} 2021 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Li, Anran and Zhang, Lan and Tan, Juntao and Qin, Yaxuan and Wang, Junhao and Li, Xiang-Yang},
  year = {2021},
  month = may,
  pages = {1--10},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  urldate = {2023-12-02},
  abstract = {Federated learning (FL) enables participants to collaboratively               construct a global machine learning model without sharing their               local training data to the remote server. In FL systems, the               selection of training samples has a significant impact on model               performances, e.g., selecting participants whose datasets have               erroneous samples, skewed categorical distributions, and low               content diversity would result in low accuracy and unstable models. In this work, we aim to solve the exigent optimization problem that selects a collection of high-quality training samples for a given FL task under a monetary budget in a privacy-preserving way, which is extremely challenging without visibility to participants' local data and training process. We provide a systematic analysis of important data related factors affecting the model performance and propose a holistic design to privately and efficiently select high-quality data samples considering all these factors. We verify the merits of our proposed solution with extensive experiments on a real AIoT system with 50 clients, including 20 edge computers, 20 laptops, and 10 desktops. The experimental results validates that our solution achieves accurate and efficient selection of high-quality data samples, and consequently an FL model with a faster convergence speed and higher accuracy than that achieved by existing solutions.},
  isbn = {978-1-66540-325-2}
}

@article{liuClassificationNoisyLabels2016,
  title = {Classification with {{Noisy Labels}} by {{Importance Reweighting}}},
  author = {Liu, Tongliang and Tao, Dacheng},
  year = {2016},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {3},
  pages = {447--461},
  issn = {1939-3539},
  urldate = {2024-04-13},
  abstract = {In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability {\textbackslash}rho {\i}n [0,0.5) , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate {$\rho$} . We show that the rate is upper bounded by the conditional probability P({\textbackslash}hatY{\textbar}X) of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.},
  keywords = {Algorithm design and analysis,Classification,consistency,Convergence,Estimation,importance reweighting,Kernel,label noise,Noise,Noise measurement,noise rate estimation,Robustness}
}

@article{liuContributionAwareFederatedLearning2022,
  title = {Contribution-{{Aware Federated Learning}} for {{Smart Healthcare}}},
  author = {Liu, Zelei and Chen, Yuanyuan and Zhao, Yansong and Yu, Han and Liu, Yang and Bao, Renyi and Jiang, Jinpeng and Nie, Zaiqing and Xu, Qian and Yang, Qiang},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {11},
  pages = {12396--12404},
  issn = {2374-3468},
  urldate = {2024-03-27},
  abstract = {Artificial intelligence (AI) is a promising technology to transform the healthcare industry. Due to the highly sensitive nature of patient data, federated learning (FL) is often leveraged to build models for smart healthcare applications. Existing deployed FL frameworks cannot address the key issues of varying data quality and heterogeneous data distributions across multiple institutions in this sector. In this paper, we report our experience developing and deploying the Contribution-Aware Federated Learning (CAFL) framework for smart healthcare. It provides an efficient and accurate approach to fairly evaluate FL participants' contribution to model performance without exposing their private data, and improves the FL model training protocol to allow the best performing intermediate models to be distributed to participants for FL training. Since its deployment in Yidu Cloud Technology Inc. in March 2021, CAFL has served 8 well-established medical institutions in China to build healthcare decision support models. It can perform contribution evaluations 2.84 times faster than the best existing approach, and has improved the average accuracy of the resulting models by 2.62\% compared to the previous system (which is significant in industrial settings). To our knowledge, it is the first contribution-aware federated learning successfully deployed in the healthcare industry.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {Contribution Evaluation}
}

@article{liuGameTheoreticDistributedEmpirical2023,
  title = {Game-{{Theoretic Distributed Empirical Risk Minimization With Strategic Network Design}}},
  author = {Liu, Shutian and Li, Tao and Zhu, Quanyan},
  year = {2023},
  journal = {IEEE Transactions on Signal and Information Processing over Networks},
  volume = {9},
  pages = {542--556},
  issn = {2373-776X},
  urldate = {2023-12-02},
  abstract = {This article considers a game-theoretic framework for distributed empirical risk minimization (ERM) problems over networks where the information acquisition at a node is modeled as a rational choice of a player. In the proposed game, players decide both the learning parameters and the network structure. The Nash equilibrium (NE) characterizes the tradeoff between the local performance and the global agreement of the learned classifiers. We first introduce an interleaved approach that features a joint learning process that integrates the iterative learning at each node with the network formation. We show that our game is equivalent to a generalized potential game in the setting of undirected networks. We study the convergence of the proposed interleaved algorithm, analyze the network structures determined by our game, and show the improvement of social welfare compared to a standard distributed ERM over fixed networks. To adapt our framework to streaming data, we derive a distributed Kalman filter. A concurrent algorithm based on the online mirror descent algorithm is also introduced to solve for NE in a holistic manner. In the case study, we use data from telemonitoring of Parkinson's disease to corroborate the results.}
}

@article{liuLossLandscapesOptimization2022,
  title = {Loss Landscapes and Optimization in Over-Parameterized Non-Linear Systems and Neural Networks},
  author = {Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  year = {2022},
  month = jul,
  journal = {Applied and Computational Harmonic Analysis},
  series = {Special {{Issue}} on {{Harmonic Analysis}} and {{Machine Learning}}},
  volume = {59},
  pages = {85--116},
  issn = {1063-5203},
  urldate = {2024-05-18},
  abstract = {The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL⁎, a variant of the Polyak-{\L}ojasiewicz condition [32], [25] on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL⁎ condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL⁎-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL⁎ condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL⁎ condition applicable to ``almost'' over-parameterized systems.},
  keywords = {Deep learning,Non-linear optimization,Over-parameterized models,PL condition}
}

@article{liUnderstandingCombatingRobust2023,
  title = {Understanding and Combating Robust Overfitting via Input Loss Landscape Analysis and Regularization},
  author = {Li, Lin and Spratling, Michael},
  year = {2023},
  month = apr,
  journal = {Pattern Recognition},
  volume = {136},
  pages = {109229},
  issn = {0031-3203},
  urldate = {2024-05-13},
  abstract = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape's curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods. Code is available at https://github.com/TreeLLi/Combating-RO-AdvLC.},
  keywords = {Adversarial robustness,Adversarial training,Logit regularization,Loss landscape analysis,Robust overfitting}
}

@inproceedings{liUnifiedAnalysisRandom2019,
  title = {Towards a {{Unified Analysis}} of {{Random Fourier Features}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Zhu and Ton, Jean-Francois and Oglic, Dino and Sejdinovic, Dino},
  year = {2019},
  month = may,
  pages = {3905--3914},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-07},
  abstract = {Random Fourier features is a widely used, simple, and effective technique for scaling up kernel methods. The existing theoretical analysis of the approach, however, remains focused on specific learning tasks and typically gives pessimistic bounds which are at odds with the empirical results. We tackle these problems and provide the first unified risk analysis of learning with random Fourier features using the squared error and Lipschitz continuous loss functions. In our bounds, the trade-off between the computational cost and the expected risk convergence rate is problem specific and expressed in terms of the regularization parameter and the number of effective degrees of freedom. We study both the standard random Fourier features method for which we improve the existing bounds on the number of features required to guarantee the corresponding minimax risk convergence rate of kernel ridge regression, as well as a data-dependent modification which samples features proportional to ridge leverage scores and further reduces the required number of features. As ridge leverage scores are expensive to compute, we devise a simple approximation scheme which provably reduces the computational cost without loss of statistical efficiency.},
  langid = {english},
  language = {en}
}

@misc{liuOutOfDistributionGeneralizationSurvey2023,
  title = {Towards {{Out-Of-Distribution Generalization}}: {{A Survey}}},
  shorttitle = {Towards {{Out-Of-Distribution Generalization}}},
  author = {Liu, Jiashuo and Shen, Zheyan and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  year = {2023},
  month = jul,
  number = {arXiv:2108.13624},
  eprint = {2108.13624},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-31},
  abstract = {Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed (\$i.i.d.\$). However, in real-world applications, this \$i.i.d.\$ assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our discussion begins with a precise, formal characterization of the OOD generalization problem. Following that, we categorize existing methodologies into three segments: unsupervised representation learning, supervised model learning, and optimization, according to their positions within the overarching learning process. We provide an in-depth discussion on representative methodologies for each category, further elucidating the theoretical links between them. Subsequently, we outline the prevailing benchmark datasets employed in OOD generalization studies. To conclude, we overview the existing body of work in this domain and suggest potential avenues for future research on OOD generalization. A summary of the OOD generalization methodologies surveyed in this paper can be accessed at http://out-of-distribution-generalization.com.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{liuRLDRLMeets2022,
  title = {{{RL}}/{{DRL Meets Vehicular Task Offloading Using Edge}} and {{Vehicular Cloudlet}}: {{A Survey}}},
  shorttitle = {{{RL}}/{{DRL Meets Vehicular Task Offloading Using Edge}} and {{Vehicular Cloudlet}}},
  author = {Liu, Jinshi and Ahmed, Manzoor and Mirza, Muhammad Ayzed and Khan, Wali Ullah and Xu, Dianlei and Li, Jianbo and Aziz, Abdul and Han, Zhu},
  year = {2022},
  month = jun,
  journal = {IEEE Internet of Things Journal},
  volume = {9},
  number = {11},
  pages = {8315--8338},
  issn = {2327-4662},
  abstract = {The last two decades have seen a clear trend toward crafting intelligent vehicles based on the significant advances in communication and computing paradigms, which provide a safer, stress-free, and more enjoyable driving experience. Moreover, emerging applications and services necessitate massive volumes of data, real-time data processing, and ultrareliable and low-latency communication (URLLC). However, the computing capability of current intelligent vehicles is minimal, making it challenging to meet the delay-sensitive and computation-intensive demand of such applications. In this situation, vehicular task/computation offloading toward the edge cloud (EC) and vehicular cloudlet (VC) seems to be a promising solution to improve the network's performance and applications' Quality of Service (QoS). At the same time, artificial intelligence (AI) has dramatically changed people's lives. Especially for vehicular task offloading applications, AI achieves state-of-the-art performance in various vehicular environments. Motivated by the outstanding performance of integrating reinforcement learning (RL)/deep RL (DRL) to the vehicular task offloading systems, we present a survey on various RL/DRL techniques applied to vehicular task offloading. Precisely, we classify the vehicular task offloading works into two main categories: 1) RL/ DRL solutions leveraging EC and 2) RL/DRL solutions using VC computing. Moreover, the EC section-based RL/DRL solutions are further subcategorized into multiaccess edge computing (MEC) server, nearby vehicles, and hybrid MEC (HMEC). To the best of our knowledge, we are the first to cover RL/DRL-based vehicular task offloading. Also, we provide lessons learned and open research challenges in this field and discuss the possible trend for future research.},
  keywords = {Artificial intelligence (AI),Cloud computing,deep reinforcement learning (DRL),Heuristic algorithms,Internet of Things,machine learning,Optimization,Quality of service,reinforcement learning (RL),Servers,Task analysis,task offloading,V2X communications,vehicle to roadside infrastructure (V2I),vehicle to vehicle (V2V),vehicular edge and cloudlet computing}
}

@inproceedings{liuRobustDifferentiallyPrivate2021,
  title = {Robust and Differentially Private Mean Estimation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Xiyang and Kong, Weihao and Kakade, Sham and Oh, Sewoong},
  year = {2021},
  volume = {34},
  pages = {3887--3901},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-11},
  abstract = {In statistical learning and analysis from shared data, which is increasingly widely adopted in platforms such as federated learning and meta-learning, there are two major concerns: privacy and robustness. Each participating individual should be able to contribute without the fear of leaking one's sensitive information.  At the same time, the system should be robust in the presence of malicious participants inserting corrupted data. Recent algorithmic advances in learning from shared data focus on either one of these threats, leaving the system vulnerable to the other. We bridge this gap for the canonical problem of estimating the mean from i.i.d.{\textasciitilde}samples. We introduce PRIME, which is the first efficient algorithm that achieves both privacy and robustness for a wide range of distributions. We further complement this result with a novel exponential time algorithm that improves the sample complexity of PRIME, achieving a near-optimal guarantee and matching that of a known lower bound for (non-robust) private mean estimation. This proves that there is no extra statistical cost to simultaneously guaranteeing privacy and robustness.}
}

@misc{liuSystematicLiteratureReview2020,
  title = {A {{Systematic Literature Review}} on {{Federated Learning}}: {{From A Model Quality Perspective}}},
  shorttitle = {A {{Systematic Literature Review}} on {{Federated Learning}}},
  author = {Liu, Yi and Zhang, Li and Ge, Ning and Li, Guanghao},
  year = {2020},
  month = dec,
  number = {arXiv:2012.01973},
  eprint = {2012.01973},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-01},
  abstract = {As an emerging technique, Federated Learning (FL) can jointly train a global model with the data remaining locally, which effectively solves the problem of data privacy protection through the encryption mechanism. The clients train their local model, and the server aggregates models until convergence. In this process, the server uses an incentive mechanism to encourage clients to contribute high-quality and large-volume data to improve the global model. Although some works have applied FL to the Internet of Things (IoT), medicine, manufacturing, etc., the application of FL is still in its infancy, and many related issues need to be solved. Improving the quality of FL models is one of the current research hotspots and challenging tasks. This paper systematically reviews and objectively analyzes the approaches to improving the quality of FL models. We are also interested in the research and application trends of FL and the effect comparison between FL and non-FL because the practitioners usually worry that achieving privacy protection needs compromising learning quality. We use a systematic review method to analyze 147 latest articles related to FL. This review provides useful information and insights to both academia and practitioners from the industry. We investigate research questions about academic research and industrial application trends of FL, essential factors affecting the quality of FL models, and compare FL and non-FL algorithms in terms of learning quality. Based on our review's conclusion, we give some suggestions for improving the FL model quality. Finally, we propose an FL application framework for practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{liuThreatsAttacksDefenses2022,
  title = {Threats, Attacks and Defenses to Federated Learning: Issues, Taxonomy and Perspectives},
  shorttitle = {Threats, Attacks and Defenses to Federated Learning},
  author = {Liu, Pengrui and Xu, Xiangrui and Wang, Wei},
  year = {2022},
  month = feb,
  journal = {Cybersecurity},
  volume = {5},
  number = {1},
  pages = {4},
  issn = {2523-3246},
  urldate = {2024-03-10},
  abstract = {Empirical attacks on Federated Learning (FL) systems~indicate that FL is fraught with numerous attack surfaces throughout the FL execution. These attacks can not only cause models to fail in specific tasks, but also infer private information. While previous surveys have identified the risks, listed the attack methods available in the literature or provided a basic taxonomy to classify them, they mainly focused on the risks in the training phase of FL. In this work, we survey the threats, attacks and defenses to FL throughout the whole process of FL in three phases, including Data and Behavior Auditing Phase, Training Phase and Predicting Phase. We further provide a comprehensive analysis of these threats, attacks and defenses, and summarize their issues and taxonomy. Our work~considers security and privacy of FL based on the viewpoint of the execution process of FL. We highlight that establishing a trusted FL requires adequate measures to mitigate security and privacy threats at each phase. Finally, we discuss the limitations of current attacks and defense approaches and provide an outlook on promising future research directions~in FL.},
  langid = {english},
  language = {en},
  keywords = {Defenses,Evasion attacks,Federated learning,Inference attacks,Multi-phases,Poisoning attacks,Security and privacy threats,Trusted}
}

@inproceedings{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-19},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.}
}

@misc{lowellThereSingularityLoss2022,
  title = {There Is a {{Singularity}} in the {{Loss Landscape}}},
  author = {Lowell, Mark},
  year = {2022},
  month = jan,
  number = {arXiv:2201.06964},
  eprint = {2201.06964},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-13},
  abstract = {Despite the widespread adoption of neural networks, their training dynamics remain poorly understood. We show experimentally that as the size of the dataset increases, a point forms where the magnitude of the gradient of the loss becomes unbounded. Gradient descent rapidly brings the network close to this singularity in parameter space, and further training takes place near it. This singularity explains a variety of phenomena recently observed in the Hessian of neural network loss functions, such as training on the edge of stability and the concentration of the gradient in a top subspace. Once the network approaches the singularity, the top subspace contributes little to learning, even though it constitutes the majority of the gradient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@phdthesis{lucasOptimizationLossLandscape2022,
  type = {Thesis},
  title = {Optimization and {{Loss Landscape Geometry}} of {{Deep Learning}}},
  author = {Lucas, James Robert},
  year = {2022},
  month = nov,
  urldate = {2024-06-25},
  abstract = {The impressive success of deep learning is powered by models that are rapidly growing in size along with the computational resources that are used to train them. Despite these growing demands, the dominant tools used to train these networks have not evolved significantly to match these needs. One natural hypothesis for this limitation is our lack of understanding of the training dynamics of deep neural networks.     In this thesis, I present our research on understanding and improving the optimization of deep learning models. The thesis begins by presenting two first-order optimization algorithms for deep learning: the Aggregated Momentum and Lookahead optimizers. We demonstrate their success on modern deep learning optimization problems and provide theoretical analyses of both optimizers in convex settings. However, our theoretical understanding of optimization for practical training of deep neural networks is severely limited.      Following this, we turn towards building a better understanding of deep learning optimization. We achieve this by studying the loss landscape geometry of deep neural networks. This is extremely challenging due to non-convex objective functions and extremely high-dimensional parameter spaces. We address this by first studying a simple class of neural networks: two-layer linear networks. Despite their simplicity, these models capture some core challenges of deep learning optimization effectively. Within this class, we investigate regularized linear autoencoders and linear variational autoencoders and carefully characterize their loss landscape geometry theoretically.      We then move beyond the simple class of two-layer linear networks to investigate a phenomenon that arises across a vast set of deep learning optimization problems. This phenomenon, which we term the Monotonic Linear Interpolation (MLI) property, describes a global property of the loss landscape geometry of deep learning models. We provide the first theoretical explanation of this phenomenon and conduct a thorough empirical investigation to better understand the pervasiveness and limitations of the MLI property.      In the final chapter, the thesis is discussed as a whole and promising directions for future research are presented.},
  copyright = {Attribution 4.0 International},
  langid = {english},
  language = {en},
  annotation = {Accepted: 2022-11-11T18:25:48Z}
}

@article{luFederatedLearningNonIID2024,
  title = {Federated {{Learning With Non-IID Data}}: {{A Survey}}},
  shorttitle = {Federated {{Learning With Non-IID Data}}},
  author = {Lu, Zili and Pan, Heng and Dai, Yueyue and Si, Xueming and Zhang, Yan},
  year = {2024},
  journal = {IEEE Internet of Things Journal},
  pages = {1--1},
  issn = {2327-4662},
  urldate = {2024-04-16},
  abstract = {Federated learning (FL) is an efficient decentralized machine learning methodology for processing non-independent and identically distributed (non-IID) data due to geographical and temporal distribution differences. Non-IID data generally indicates substantial disparities in data distribution and features among clients. This assumption is completely different from the conventional assumption of independent and identically distributed (IID) data in which all clients' data originates from the same distribution. There are many factors that affect the features of non-IID data, such as user preferences, data collection methods, and client characteristics. The factors of data distribution, category proportions, and feature representation also affect the statistical properties of non-IID data. This paper conducts an in-depth exploration of FL with the consideration of diverse features and statistical properties of non-IID data. Specifically, we first discuss the impact of non-IID data on communication efficiency, model convergence, and FL accuracy. The presence of non-IID data leads to increased communication overhead, imbalanced class distribution, and uneven local model updates. All of these affect FL convergence and performance. Then, we present the latest advanced techniques, such as data partitioning/sharing, client selection, differential privacy, and secure aggregation [1], which are used to address the challenges posed by non-IID data in terms of communication efficiency and privacy protection. Furthermore, we show the emerging applications and use cases of FL with non-IID data in various domains, such as healthcare, IoT, and edge computing. Overall, this survey provides a comprehensive understanding of FL with non-IID data, including the challenges, advancements, and practical applications in different areas.},
  keywords = {Adaptation models,Communication efficiency,Convergence,Data models,Distributed databases,federated learning,Internet of Things,Internet of Things (IoT),non-IID data,privacy preservation,Servers,survey,Training}
}

@article{luLearningConceptDrift2019,
  title = {Learning under {{Concept Drift}}: {{A Review}}},
  shorttitle = {Learning under {{Concept Drift}}},
  author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Jo{\~a}o and Zhang, Guangquan},
  year = {2019},
  month = dec,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {31},
  number = {12},
  pages = {2346--2363},
  issn = {1558-2191},
  urldate = {2023-11-27},
  abstract = {Concept drift describes unforeseeable changes in the underlying distribution of streaming data overtime. Concept drift research involves the development of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.}
}

@incollection{lyuCollaborativeFairnessFederated2020,
  title = {Collaborative {{Fairness}} in {{Federated Learning}}},
  booktitle = {Federated {{Learning}}: {{Privacy}} and {{Incentive}}},
  author = {Lyu, Lingjuan and Xu, Xinyi and Wang, Qian and Yu, Han},
  editor = {Yang, Qiang and Fan, Lixin and Yu, Han},
  year = {2020},
  pages = {189--204},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-03-27},
  abstract = {In current deep learning paradigms, local training or the Standalone framework tends to result in overfitting and thus low utility. This problem can be addressed by Distributed or Federated Learning (FL) that leverages a parameter server to aggregate local model updates. However, all the existing FL frameworks have overlooked an important aspect of participation: collaborative fairness. In particular, all participants can receive the same or similar models, even the ones who contribute relatively less, and in extreme cases, nothing. To address this issue, we propose a novel Collaborative Fair Federated Learning (CFFL) framework which utilizes reputations to enforce participants to converge to different models, thus ensuring fairness and accuracy at the same time. Extensive experiments on benchmark datasets demonstrate that CFFL achieves high fairness and performs comparably to the Distributed framework and better than the Standalone framework.},
  isbn = {978-3-030-63076-8},
  langid = {english},
  language = {en},
  keywords = {Collaborative learning,Fairness,Reputation}
}

@article{lyuFairPrivacyPreservingFederated2020,
  title = {Towards {{Fair}} and {{Privacy-Preserving Federated Deep Models}}},
  author = {Lyu, Lingjuan and Yu, Jiangshan and Nandakumar, Karthik and Li, Yitong and Ma, Xingjun and Jin, Jiong and Yu, Han and Ng, Kee Siong},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {31},
  number = {11},
  pages = {2524--2541},
  issn = {1045-9219, 1558-2183, 2161-9883},
  urldate = {2024-03-27},
  abstract = {The current standalone deep learning framework tends to result in overfitting and low utility. This problem can be addressed by either a centralized framework that deploys a central server to train a global model on the joint data from all parties, or a distributed framework that leverages a parameter server to aggregate local model updates. Server-based solutions are prone to the problem of a single-point-of-failure. In this respect, collaborative learning frameworks, such as federated learning (FL), are more robust. Existing federated learning frameworks overlook an important aspect of participation: fairness. All parties are given the same final model without regard to their contributions. To address these issues, we propose a decentralized Fair and Privacy-Preserving Deep Learning (FPPDL) framework to incorporate fairness into federated deep learning models. In particular, we design a local credibility mutual evaluation mechanism to guarantee fairness, and a three-layer onion-style encryption scheme to guarantee both accuracy and privacy. Different from existing FL paradigm, under FPPDL, each participant receives a different version of the FL model with performance commensurate with his contributions. Experiments on benchmark datasets demonstrate that FPPDL balances fairness, privacy and accuracy. It enables federated learning ecosystems to detect and isolate low-contribution parties, thereby promoting responsible participation.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  language = {en}
}

@article{lyuHowDemocratiseProtect2020,
  title = {How to {{Democratise}} and {{Protect AI}}: {{Fair}} and {{Differentially Private Decentralised Deep Learning}}},
  shorttitle = {How to {{Democratise}} and {{Protect AI}}},
  author = {Lyu, Lingjuan and Li, Yitong and Nandakumar, Karthik and Yu, Jiangshan and Ma, Xingjun},
  year = {2020},
  journal = {IEEE Transactions on Dependable and Secure Computing},
  pages = {1--1},
  issn = {1545-5971, 1941-0018, 2160-9209},
  urldate = {2024-04-03},
  abstract = {This article first considers the research problem of fairness in collaborative deep learning, while ensuring privacy. A novel reputation system is proposed through digital tokens and local credibility to ensure fairness, in combination with differential privacy to guarantee privacy. In particular, we build a fair and differentially private decentralised deep learning framework called FDPDDL, which enables parties to derive more accurate local models in a fair and private manner by using our developed two-stage scheme: during the initialisation stage, artificial samples generated by Differentially Private Generative Adversarial Network (DPGAN) are used to mutually benchmark the local credibility of each party and generate initial tokens; during the update stage, Differentially Private SGD (DPSGD) is used to facilitate collaborative privacy-preserving deep learning, and local credibility and tokens of each party are updated according to the quality and quantity of individually released gradients. Experimental results on benchmark datasets under three realistic settings demonstrate that FDPDDL achieves high fairness, yields comparable accuracy to the centralised and distributed frameworks, and delivers better accuracy than the standalone framework.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  language = {en}
}

@article{maFederatedDataCleaning2021,
  title = {Federated {{Data Cleaning}}: {{Collaborative}} and {{Privacy-Preserving Data Cleaning}} for {{Edge Intelligence}}},
  shorttitle = {Federated {{Data Cleaning}}},
  author = {Ma, Lichuan and Pei, Qingqi and Zhou, Lu and Zhu, Haojin and Wang, Licheng and Ji, Yusheng},
  year = {2021},
  month = apr,
  journal = {IEEE Internet of Things Journal},
  volume = {8},
  number = {8},
  pages = {6757--6770},
  issn = {2327-4662},
  urldate = {2024-04-12},
  abstract = {As an important driving factor of emerging Internet-of-Things (IoT) applications, machine learning algorithms are currently facing the challenge of how to ``clean'' data noise, that is introduced during the training process (e.g., asynchronous execution and lossy data compression and quantization). In an attempt to guarantee data quality, various data cleaning approaches have been proposed to filter out abnormal data entries based on the global data distribution. However, most existing data cleaning approaches are based on a centralized paradigm and thus cannot be applied to future edge-based IoT applications, where each edge node (EN) has only a limited view of the global data distribution. Moreover, the increasing demand for privacy preservation largely prevents ENs from combining their data for centralized cleaning. In this study, we propose a federated data cleaning protocol, coined as FedClean, for edge intelligence (EI) scenarios that is designed to achieve data cleaning without compromising data privacy. More specifically, different ENs first generate Boolean shares of their data and distribute them to two noncolluding servers. These two servers then run the FedClean protocol to privately and efficiently compute the attribute value frequency (AVF) scores of the collected data entries, which are then sorted in ascending order via a bitonic sorting network without revealing their values. As a result, data entries with lower AVF scores are considered as abnormal and filtered out. The security, efficiency, and effectiveness of the proposed approach are then demonstrated via concrete security analysis and comprehensive experiments.},
  keywords = {Cleaning,Collaboration,Data cleaning,Data models,Data privacy,edge intelligence (EI),Internet of Things,privacy preserving,Protocols,Servers}
}

@book{mahantiDataQualityDimensions2019,
  title = {Data Quality: Dimensions, Measurement, Strategy, Management, and Governance},
  shorttitle = {Data Quality},
  author = {Mahanti, Rupa},
  year = {2019},
  publisher = {ASQ Quality Press},
  address = {Milwaukee, Wisconsin},
  isbn = {978-0-87389-977-2},
  langid = {english},
  language = {eng}
}

@inproceedings{mahloujifarPropertyInferencePoisoning2022,
  title = {Property {{Inference}} from {{Poisoning}}},
  booktitle = {2022 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Mahloujifar, Saeed and Ghosh, Esha and Chase, Melissa},
  year = {2022},
  month = may,
  pages = {1120--1137},
  issn = {2375-1207},
  urldate = {2024-03-11},
  abstract = {Property inference attacks consider an adversary who has access to a trained ML model and tries to extract some global statistics of the training data. In this work, we study property inference in scenarios where the adversary can maliciously control a part of the training data (poisoning data) with the goal of increasing the leakage. Previous works on poisoning attacks focused on trying to decrease the accuracy of models. Here, for the first time, we study poisoning attacks where the goal of the adversary is to increase the information leakage of the model. We show that poisoning attacks can boost the information leakage significantly and should be considered as a stronger threat model in sensitive applications where some of the data sources may be malicious.We theoretically prove that our attack can always succeed as long as the learning algorithm used has good generalization properties. Then we experimentally evaluate our on different datasets (Census dataset, Enron email dataset, MNIST and CelebA), properties (that are present in the training data as features, that are not present as features, and properties that are uncorrelated with the rest of the training data or classification task) and model architectures (including Resnet-18 and Resnet-50). We were able to achieve high attack accuracy with relatively low poisoning rate, namely, 2--3\% poisoning in most of our experiments. We also evaluated our attacks on models trained with DP and we show that even with very small values for {$\epsilon$}, the attack is still quite successful1.1Code is available at https://github.com/smahloujifar/PropertyInferenceFromPoisoning.git},
  keywords = {Data mining,Data models,Machine learning,Poisoning-attacks,privacy,Privacy,property-inference-attacks,Security,Soft sensors,Training data}
}

@article{makhoulReviewDataQuality2022,
  title = {Review of Data Quality Indicators and Metrics, and Suggestions for Indicators and Metrics for Structural Health Monitoring},
  author = {Makhoul, Nisrine},
  year = {2022},
  month = nov,
  journal = {Advances in Bridge Engineering},
  volume = {3},
  number = {1},
  pages = {17},
  issn = {2662-5407},
  urldate = {2023-11-15},
  abstract = {Structural Health Monitoring (SHM) systems have been extensively implemented to deliver data support and safeguard structural safety in structural integrity management context. SHM relies on data that can be noisy in large amounts or scarce. Little work has been done on SHM data quality (DQ). Therefore, this article suggests SHM DQ indicators and recommends deterministic and probabilistic SHM DQ metrics to address uncertainties. This will allow better decision-making for structural integrity management.},
  keywords = {Data quality,Indicators,Metrics,Structural Health Monitoring}
}

@article{malanSurveyAdvancesLandscape2021,
  title = {A {{Survey}} of {{Advances}} in {{Landscape Analysis}} for {{Optimisation}}},
  author = {Malan, Katherine Mary},
  year = {2021},
  month = feb,
  journal = {Algorithms},
  volume = {14},
  number = {2},
  pages = {40},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1999-4893},
  urldate = {2024-05-13},
  abstract = {Fitness landscapes were proposed in 1932 as an abstract notion for understanding biological evolution and were later used to explain evolutionary algorithm behaviour. The last ten years has seen the field of fitness landscape analysis develop from a largely theoretical idea in evolutionary computation to a practical tool applied in optimisation in general and more recently in machine learning. With this widened scope, new types of landscapes have emerged such as multiobjective landscapes, violation landscapes, dynamic and coupled landscapes and error landscapes. This survey is a follow-up from a 2013 survey on fitness landscapes and includes an additional 11 landscape analysis techniques. The paper also includes a survey on the applications of landscape analysis for understanding complex problems and explaining algorithm behaviour, as well as algorithm performance prediction and automated algorithm configuration and selection. The extensive use of landscape analysis in a broad range of areas highlights the wide applicability of the techniques and the paper discusses some opportunities for further research in this growing field.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {automated algorithm selection,error landscape,fitness landscape,landscape analysis,violation landscape}
}

@article{manwaniNoiseToleranceRisk2013,
  title = {Noise {{Tolerance Under Risk Minimization}}},
  author = {Manwani, Naresh and Sastry, P. S.},
  year = {2013},
  month = jun,
  journal = {IEEE Transactions on Cybernetics},
  volume = {43},
  number = {3},
  pages = {1146--1151},
  issn = {2168-2275},
  urldate = {2024-04-13},
  abstract = {In this paper, we explore noise-tolerant learning of classifiers. We formulate the problem as follows. We assume that there is an unobservable training set that is noise free. The actual training set given to the learning algorithm is obtained from this ideal data set by corrupting the class label of each example. The probability that the class label of an example is corrupted is a function of the feature vector of the example. This would account for most kinds of noisy data one encounters in practice. We say that a learning method is noise tolerant if the classifiers learnt with noise-free data and with noisy data, both have the same classification accuracy on the noise-free data. In this paper, we analyze the noise-tolerance properties of risk minimization (under different loss functions). We show that risk minimization under 0-1 loss function has impressive noise-tolerance properties and that under squared error loss is tolerant only to uniform noise; risk minimization under other loss functions is not noise tolerant. We conclude this paper with some discussion on the implications of these theoretical results.},
  keywords = {Fasteners,Label noise,loss functions,Noise,Noise measurement,noise tolerance,Risk management,risk minimization,Training,Training data,Vectors}
}

@misc{maQuadraticApproximationMultiscale2022,
  title = {Beyond the {{Quadratic Approximation}}: The {{Multiscale Structure}} of {{Neural Network Loss Landscapes}}},
  shorttitle = {Beyond the {{Quadratic Approximation}}},
  author = {Ma, Chao and Kunin, Daniel and Wu, Lei and Ying, Lexing},
  year = {2022},
  month = jun,
  number = {arXiv:2204.11326},
  eprint = {2204.11326},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-13},
  abstract = {A quadratic approximation of neural network loss landscapes has been extensively used to study the optimization process of these networks. Though, it usually holds in a very small neighborhood of the minimum, it cannot explain many phenomena observed during the optimization process. In this work, we study the structure of neural network loss functions and its implication on optimization in a region beyond the reach of a good quadratic approximation. Numerically, we observe that neural network loss functions possesses a multiscale structure, manifested in two ways: (1) in a neighborhood of minima, the loss mixes a continuum of scales and grows subquadratically, and (2) in a larger region, the loss shows several separate scales clearly. Using the subquadratic growth, we are able to explain the Edge of Stability phenomenon [5] observed for the gradient descent (GD) method. Using the separate scales, we explain the working mechanism of learning rate decay by simple examples. Finally, we study the origin of the multiscale structure and propose that the non-convexity of the models and the non-uniformity of training data is one of the causes. By constructing a two-layer neural network problem we show that training data with different magnitudes give rise to different scales of the loss function, producing subquadratic growth and multiple separate scales.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{marcuImportanceDataQuality2023,
  title = {The {{Importance}} of {{Data Quality}} in {{Training}} a {{Deep Convolutional Neural Network}}},
  booktitle = {2023 17th {{International Conference}} on {{Engineering}} of {{Modern Electric Systems}} ({{EMES}})},
  author = {Marcu, David C. and Grava, Cristian},
  year = {2023},
  month = jun,
  pages = {1--4},
  urldate = {2024-05-11},
  abstract = {It is an acknowledged fact that good quality and varied training data samples are essential for achieving high rates of success in the operation of a convolutional neural network. The performance of the trained network is highly dependent on the training data being as diverse and representative as possible and each learnable feature being present in a large number of training data samples. There are situations when it is not possible to accrue a sufficient number of original samples and artificial sample generation is needed. The current work studies the impact on performance of a convolutional neural network of original training data supplemented with artificially generated samples. Multiple batches of training data consisting of varied combinations of fractions between original and artificial samples have been used to train a convolutional neural network in order to determine the percentage of original data needed to achieve minimal loss of performance.},
  keywords = {artificial data,augmented training data,convolutional neural network,Convolutional neural networks,Data integrity,deep learning,Deep learning,Training,Training data}
}

@misc{martinRethinkingGeneralizationRequires2019,
  title = {Rethinking Generalization Requires Revisiting Old Ideas: Statistical Mechanics Approaches and Complex Learning Behavior},
  shorttitle = {Rethinking Generalization Requires Revisiting Old Ideas},
  author = {Martin, Charles H. and Mahoney, Michael W.},
  year = {2019},
  month = feb,
  number = {arXiv:1710.09553},
  eprint = {1710.09553},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-18},
  abstract = {We describe an approach to understand the peculiar and counterintuitive generalization properties of deep neural networks. The approach involves going beyond worst-case theoretical capacity control frameworks that have been popular in machine learning in recent years to revisit old ideas in the statistical mechanics of neural networks. Within this approach, we present a prototypical Very Simple Deep Learning (VSDL) model, whose behavior is controlled by two control parameters, one describing an effective amount of data, or load, on the network (that decreases when noise is added to the input), and one with an effective temperature interpretation (that increases when algorithms are early stopped). Using this model, we describe how a very simple application of ideas from the statistical mechanics theory of generalization provides a strong qualitative description of recently-observed empirical results regarding the inability of deep neural networks not to overfit training data, discontinuous learning and sharp transitions in the generalization properties of learning algorithms, etc.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{masihaLearningDistributionMismatch2022,
  title = {Learning under {{Distribution Mismatch}} and {{Model Misspecification}}},
  author = {Masiha, Saeed and Gohari, Amin and Yassaee, Mohammad Hossein and Aref, Mohammad Reza},
  year = {2022},
  month = aug,
  number = {arXiv:2102.05695},
  eprint = {2102.05695},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-03-08},
  abstract = {We study learning algorithms when there is a mismatch between the distributions of the training and test datasets of a learning algorithm. The effect of this mismatch on the generalization error and model misspecification are quantified. Moreover, we provide a connection between the generalization error and the rate-distortion theory, which allows one to utilize bounds from the rate-distortion theory to derive new bounds on the generalization error and vice versa. In particular, the rate-distortion based bound strictly improves over the earlier bound by Xu and Raginsky even when there is no mismatch. We also discuss how "auxiliary loss functions" can be utilized to obtain upper bounds on the generalization error.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory}
}

@article{maStateoftheartSurveySolving2022,
  title = {A State-of-the-Art Survey on Solving Non-{{IID}} Data in {{Federated Learning}}},
  author = {Ma, Xiaodong and Zhu, Jia and Lin, Zhihao and Chen, Shanxuan and Qin, Yangjie},
  year = {2022},
  month = oct,
  journal = {Future Generation Computer Systems},
  volume = {135},
  pages = {244--258},
  issn = {0167-739X},
  urldate = {2024-04-16},
  abstract = {Federated Learning (FL) proposed in recent years has received significant attention from researchers in that it can enable multiple clients to cooperatively train global models without revealing private data. This training mode protects users' privacy without violating the supervision, and aggregates scattered data to exert great potential. However, the data samples on each participating device of FL are usually not independent and identically distributed (IID), which leads to serious statistical heterogeneity challenges for FL. In this article, we analyze and establish the definition of non-IID data problems, and put forward a series of challenges that this problem may bring to FL. We classify existing methods to solve this problem from the researcher's entry point and subsequent sub-methods, aiming to provide a comprehensive study for solving the problem of non-IID data in FL. Our research shows that non-IID data will not only reduce the performance of the FL model, but also damage the active participation of users in the FL process. Compared with methods based on data-side sharing, enhancement, and selection, it is more common for researchers to improve federated learning algorithms from models, algorithms, and frameworks to solve non-IID problems. To the best of our knowledge, although many efforts have been made to address the problem of non-IID data, there are currently few authoritative systematic reviews in this field and are not up-to-date. In this article, we will fill the gaps in FL and provide researchers with the state-of-the-art research results to solve non-IID problems in FL and promote the further implementation of FL.},
  keywords = {Federated Learning,Machine learning,Non-IID data,Statistical heterogeneity}
}

@inproceedings{mcmahanCommunicationEfficientLearningDeep2017,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  year = {2017},
  month = apr,
  pages = {1273--1282},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-01-20},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  langid = {english},
  language = {en}
}

@article{mehrabiSurveyBiasFairness2022,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2022},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  urldate = {2023-12-16},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  langid = {english},
  language = {en},
  keywords = {Deep learning,Fairness and bias in artificial intelligence,Machine learning,Natural language processing,Representation learning}
}

@article{mehtaLossSurfaceDeep2022,
  title = {The {{Loss Surface}} of {{Deep Linear Networks Viewed Through}} the {{Algebraic Geometry Lens}}},
  author = {Mehta, Dhagash and Chen, Tianran and Tang, Tingting and Hauenstein, Jonathan D.},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {9},
  pages = {5664--5680},
  issn = {1939-3539},
  urldate = {2024-05-18},
  abstract = {By using the viewpoint of modern computational algebraic geometry, we explore properties of the optimization landscapes of deep linear neural network models. After providing clarification on the various definitions of ``flat'' minima, we show that the geometrically flat minima, which are merely artifacts of residual continuous symmetries of the deep linear networks, can be straightforwardly removed by a generalized L\_2L2-regularization. Then, we establish upper bounds on the number of isolated stationary points of these networks with the help of algebraic geometry. Combining these upper bounds with a method in numerical algebraic geometry, we find all stationary points for modest depth and matrix size. We demonstrate that, in the presence of the non-zero regularization, deep linear networks can indeed possess local minima which are not global minima. Finally, we show that even though the number of stationary points increases as the number of neurons (regularization parameters) increases (decreases), higher index saddles are surprisingly rare.},
  keywords = {Analytical models,Deep learning,Deep linear network,Geometry,global optimization,Mathematical model,Neurons,numerical algebraic geometry,regularization,Task analysis,Upper bound}
}

@inproceedings{melisExploitingUnintendedFeature2019,
  title = {Exploiting {{Unintended Feature Leakage}} in {{Collaborative Learning}}},
  booktitle = {2019 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Melis, Luca and Song, Congzheng and De Cristofaro, Emiliano and Shmatikov, Vitaly},
  year = {2019},
  month = may,
  pages = {691--706},
  publisher = {IEEE},
  address = {San Francisco, CA, USA},
  urldate = {2024-03-11},
  abstract = {Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates.},
  isbn = {978-1-5386-6660-9},
  langid = {english},
  language = {en}
}

@inproceedings{mendietaLocalLearningMatters2022,
  title = {Local {{Learning Matters}}: {{Rethinking Data Heterogeneity}} in {{Federated Learning}}},
  shorttitle = {Local {{Learning Matters}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mendieta, Matias and Yang, Taojiannan and Wang, Pu and Lee, Minwoo and Ding, Zhengming and Chen, Chen},
  year = {2022},
  pages = {8397--8406},
  urldate = {2024-06-06},
  langid = {english},
  language = {en}
}

@inproceedings{mFedAvgMinimizingPolyakLojasiewicz2023,
  title = {{{FedAvg}} for {{Minimizing Polyak-{\L}ojasiewicz Objectives}}: {{The Interpolation Regime}}},
  shorttitle = {{{FedAvg}} for {{Minimizing Polyak-{\L}ojasiewicz Objectives}}},
  booktitle = {2023 57th {{Asilomar Conference}} on {{Signals}}, {{Systems}}, and {{Computers}}},
  author = {M, Shruti and Khanduri, Prashant and Bharath, B. N.},
  year = {2023},
  month = oct,
  pages = {607--613},
  issn = {2576-2303},
  urldate = {2024-04-29},
  abstract = {Federated Learning (FL) is a distributed learning paradigm where multiple clients each having access to a local dataset collaborate with a server to solve a joint problem. Federated Averaging (FedAvg) the algorithm of choice for many FL applications is characterized by partial client participation and local updates at each client. Regardless of its popularity, the performance of FedAvg is not very well understood, especially in the interpolation regime, a common phenomenon observed in modern overparameterized neural networks. In this work, we address this challenge and perform a thorough theoretical performance analysis of FedAvg. We consider a class of non-convex functions satisfying the Polyak-Lojasiewicz (PL) inequality, a condition satisfied by overparameterized neural networks. For the first time, we establish that FedAvg with partial client participation achieves a linear convergence rate of, where ∊ is the solution accuracy. In contrast to the standard FedAvg analyses, our work does not require bounded heterogeneity, variance, and gradient assumptions. Instead, we show that sample-wise (and local) smoothness of the local objectives suffice to capture the effect of heterogeneity. Experiments on multiple real datasets corroborate our theoretical findings.},
  keywords = {Computers,Distance learning,Federated learning,Federated Learning (FL),Interpolation,Interpolation regime,Neural networks,Overparameterized models,Performance analysis,Polyak-Lojasiewicz (PL) inequality,Training}
}

@inproceedings{mhamdiHiddenVulnerabilityDistributed2018,
  title = {The {{Hidden Vulnerability}} of {{Distributed Learning}} in {{Byzantium}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Mhamdi, El Mahdi El and Guerraoui, Rachid and Rouault, S{\'e}bastien},
  year = {2018},
  month = jul,
  pages = {3521--3530},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-06},
  abstract = {While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these approaches have been proven Byzantine--resilient: they ensure the convergence of SGD despite the presence of a minority of adversarial workers. We show in this paper that convergence is not enough. In high dimension \$d {\textbackslash}gg 1\$, an adver{\textbackslash}-sary can build on the loss function's non--convexity to make SGD converge to ineffective models. More precisely, we bring to light that existing Byzantine--resilient schemes leave a margin of poisoning of \${\textbackslash}bigOmega{\textbackslash}left(f(d){\textbackslash}right)\$, where \$f(d)\$ increases at least like \${\textbackslash}sqrt[p]\{d~\}\$. Based on this leeway, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR--10 and MNIST. We introduce Bulyan, and prove it significantly reduces the attackers leeway to a narrow \${\textbackslash}bigO{\textbackslash},( {\textbackslash}sfrac\{1\}\{{\textbackslash}sqrt\{d~\}\})\$ bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence as if only non--Byzantine gradients had been used to update the model.},
  langid = {english},
  language = {en}
}

@article{mignaccoEffectiveNoiseStochastic2022,
  title = {The Effective Noise of Stochastic Gradient Descent},
  author = {Mignacco, Francesca and Urbani, Pierfrancesco},
  year = {2022},
  month = aug,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2022},
  number = {8},
  pages = {083405},
  publisher = {{IOP Publishing and SISSA}},
  issn = {1742-5468},
  urldate = {2024-05-25},
  abstract = {Stochastic gradient descent (SGD) is the workhorse algorithm of deep learning technology. At each step of the training phase, a mini batch of samples is drawn from the training dataset and the weights of the neural network are adjusted according to the performance on this specific subset of examples. The mini-batch sampling procedure introduces a stochastic dynamics to the gradient descent, with a non-trivial state-dependent noise. We characterize the stochasticity of SGD and a recently-introduced variant, persistent SGD, in a prototypical neural network model. In the under-parametrized regime, where the final training error is positive, the SGD dynamics reaches a stationary state and we define an effective temperature from the fluctuation--dissipation theorem, computed from dynamical mean-field theory. We use the effective temperature to quantify the magnitude of the SGD noise as a function of the problem parameters. In the over-parametrized regime, where the training error vanishes, we measure the noise magnitude of SGD by computing the average distance between two replicas of the system with the same initialization and two different realizations of SGD noise. We find that the two noise measures behave similarly as a function of the problem parameters. Moreover, we observe that noisier algorithms lead to wider decision boundaries of the corresponding constraint satisfaction problem.},
  langid = {english},
  language = {en}
}

@article{mingardSGDBayesianSampler2021,
  title = {Is {{SGD}} a {{Bayesian}} Sampler? {{Well}}, Almost},
  shorttitle = {Is {{SGD}} a {{Bayesian}} Sampler?},
  author = {Mingard, Chris and {Valle-P{\'e}rez}, Guillermo and Skalse, Joar and Louis, Ard A.},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {79},
  pages = {1--64},
  issn = {1533-7928},
  urldate = {2024-05-21},
  abstract = {Deep neural networks (DNNs) generalise remarkably well in the overparameterised regime, suggesting a strong inductive bias towards functions with low generalisation error. We empirically investigate this bias by calculating, for a range of architectures and datasets, the probability  P SGD (f{$\mid$}S) {$P$} {$S$} {$G$} {$D$} ( {$f$} {$\mid$} {$S$} )  that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function  f {$f$}  consistent with a training set  S {$S$} . We also use Gaussian processes to estimate the Bayesian posterior probability  P B (f{$\mid$}S) {$P$} {$B$} ( {$f$} {$\mid$} {$S$} )  that the DNN expresses  f {$f$}  upon random sampling of its parameters, conditioned on  S {$S$} . Our main findings are that  P SGD (f{$\mid$}S) {$P$} {$S$} {$G$} {$D$} ( {$f$} {$\mid$} {$S$} )  correlates remarkably well with  P B (f{$\mid$}S) {$P$} {$B$} ( {$f$} {$\mid$} {$S$} )  and that  P B (f{$\mid$}S) {$P$} {$B$} ( {$f$} {$\mid$} {$S$} )  is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines  P B (f{$\mid$}S) {$P$} {$B$} ( {$f$} {$\mid$} {$S$} ) ), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior  P B (f{$\mid$}S) {$P$} {$B$} ( {$f$} {$\mid$} {$S$} )  is the first order determinant of  P SGD (f{$\mid$}S) {$P$} {$S$} {$G$} {$D$} ( {$f$} {$\mid$} {$S$} ) , there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on  P SGD (f{$\mid$}S) {$P$} {$S$} {$G$} {$D$} ( {$f$} {$\mid$} {$S$} )  and/or  P B (f{$\mid$}S) {$P$} {$B$} ( {$f$} {$\mid$} {$S$} ) , can shed light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.}
}

@article{minskerGeometricMedianRobust2015,
  title = {Geometric Median and Robust Estimation in {{Banach}} Spaces},
  author = {Minsker, Stanislav},
  year = {2015},
  month = nov,
  journal = {Bernoulli},
  volume = {21},
  number = {4},
  pages = {2308--2335},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  urldate = {2024-04-06},
  abstract = {In many real-world applications, collected data are contaminated by noise with heavy-tailed distribution and might contain outliers of large magnitude. In this situation, it is necessary to apply methods which produce reliable outcomes even if the input contains corrupted measurements. We describe a general method which allows one to obtain estimators with tight concentration around the true parameter of interest taking values in a Banach space. Suggested construction relies on the fact that the geometric median of a collection of independent ``weakly concentrated'' estimators satisfies a much stronger deviation bound than each individual element in the collection. Our approach is illustrated through several examples, including sparse linear regression and low-rank matrix recovery problems.},
  keywords = {distributed computing,heavy-tailed noise,large deviations,linear models,low-rank matrix estimation,Principal Component Analysis,robust estimation}
}

@book{misraArtificialIntelligenceCloud2022,
  title = {Artificial {{Intelligence}} for {{Cloud}} and {{Edge Computing}}},
  editor = {Misra, Sanjay and Kumar Tyagi, Amit and Piuri, Vincenzo and Garg, Lalit},
  year = {2022},
  series = {Internet of {{Things}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2023-08-08},
  isbn = {978-3-030-80820-4 978-3-030-80821-1},
  langid = {english},
  language = {english},
  keywords = {Artificial Intelligence,Cloud Computing,Cyber Security,Edge Computing,Opportunities and Challenges in Industry 4.0,Opportunities in Society 5.0}
}

@phdthesis{mitraMachineLearningMethods2022,
  title = {Machine {{Learning Methods}} for {{Data Quality Aspects}} in {{Edge Computing Platforms}}},
  author = {Mitra, Alakananda},
  year = {2022},
  address = {United States -- Texas},
  urldate = {2023-12-01},
  abstract = {In this research, three aspects of data quality with regard to artifical intelligence (AI) have been investigated: detection of misleading fake data, especially deepfakes, data scarcity, and data insufficiency, especially how much training data is required for an AI application. Different application domains where the selected aspects pose issues have been chosen. To address the issues of data privacy, security, and regulation, these solutions are targeted for edge devices. In Chapter 3, two solutions have been proposed that aim to preempt such misleading deepfake videos and images on social media. These solutions are deployable at edge devices. In Chapter 4, a deepfake resilient digital ID system has been described. Another data quality aspect, data scarcity, has been addressed in Chapter 5. One of such agricultural problems is estimating crop damage due to natural disasters. Data insufficiency is another aspect of data quality. The amount of data required to achieve acceptable accuracy in a machine learning (ML) model has been studied in Chapter 6. As the data scarcity problem is studied in the agriculture domain, a similar scenario---plant disease detection and damage estimation---has been chosen for this verification. This research aims to provide ML or deep learning (DL)-based methods to solve several data quality-related issues in different application domains and achieve high accuracy. We hope that this work will contribute to research on the application of machine learning techniques in domains where data quality is a barrier to success.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798374401790},
  langid = {english},
  language = {English},
  school = {University of North Texas},
  keywords = {Agricultural cyber,Crop damage estimation,Damage estimation,Data quality,Edge computing,Machine learning,Physical system,Smart city}
}

@article{moConvergenceConsistencyERM2016,
  title = {Convergence and Consistency of {{ERM}} Algorithm with Uniformly Ergodic {{Markov}} Chain Samples},
  author = {Mo, Xiaomei and Xu, Jie},
  year = {2016},
  month = may,
  journal = {International Journal of Wavelets, Multiresolution and Information Processing},
  volume = {14},
  number = {03},
  pages = {1650013},
  publisher = {World Scientific Publishing Co.},
  issn = {0219-6913},
  urldate = {2024-04-30},
  abstract = {This paper studies the convergence rate and consistency of Empirical Risk Minimization algorithm, where the samples need not be independent and identically distributed (i.i.d.) but can come from uniformly ergodic Markov chain (u.e.M.c.). We firstly establish the generalization bounds of Empirical Risk Minimization algorithm with u.e.M.c. samples. Then we deduce that the Empirical Risk Minimization algorithm on the base of u.e.M.c. samples is consistent and owns a fast convergence rate.},
  keywords = {consistency,convergence rate,Empirical Risk Minimization algorithm,generalization bound,uniform ergodic Markov chain samples (u.e.M.c.)}
}

@inproceedings{mohriAgnosticFederatedLearning2019,
  title = {Agnostic {{Federated Learning}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
  year = {2019},
  month = may,
  pages = {4615--4625},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-03-30},
  abstract = {A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.},
  langid = {english},
  language = {en}
}

@article{moQueryingLittleEnough2021,
  title = {Querying Little Is Enough: {{Model}} Inversion Attack via Latent Information},
  shorttitle = {Querying Little Is Enough},
  author = {Mo, Kanghua and Liu, Xiaozhang and Huang, Teng and Yan, Anli},
  year = {2021},
  journal = {International Journal of Intelligent Systems},
  volume = {36},
  number = {2},
  pages = {681--690},
  issn = {1098-111X},
  urldate = {2024-03-12},
  abstract = {As machine learning (ML) technologies evolve, various online intelligent services use ML models to provide predictions. Unfortunately, attackers can obtain the private information of the model by interacting with the online service, namely model inversion attack (MIA). However, MIA requires large data sets to be transferred to an online service to obtain the predictive value of the inference model. Besides, the huge transmission may cause the administrator's active defense. To overcome this drawback, we propose a novel MIA scheme, which leverages latent information extracted by an auxiliary neural network as high-dimensional features to simplify what inversion model should learn. The core idea of our scheme is to reuse some parameters of the local pretraining model. Extensive experiments have verified the effectiveness of our method in convolutional neural networks on LFW, pubFig, MNIST data sets. Experimental results show that even with a few queries, our inversion method still work accurately and is superior to other technologies. It is worth mentioning that our method makes it more difficult for administrators to defend against the attack and elicit more investigations for privacy-preserving.},
  copyright = {{\copyright} 2020 Wiley Periodicals LLC},
  langid = {english},
  language = {en},
  keywords = {latent information,machine learning,model inversion attack,privacy-preserving,query times}
}

@inproceedings{morrisTextEmbeddingsReveal2023,
  title = {Text {{Embeddings Reveal}} ({{Almost}}) {{As Much As Text}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Morris, John and Kuleshov, Volodymyr and Shmatikov, Vitaly and Rush, Alexander},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {12448--12460},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  urldate = {2024-02-24},
  abstract = {How much private information do text embeddings reveal about the original text? We investigate the problem of embedding inversion, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a naive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover 92\% of 32-token text inputs exactly. We train our model to decode text embeddings from two state-of-the-art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes.}
}

@article{mothukuriSurveySecurityPrivacy2021,
  title = {A Survey on Security and Privacy of Federated Learning},
  author = {Mothukuri, Viraaji and Parizi, Reza M. and Pouriyeh, Seyedamin and Huang, Yan and Dehghantanha, Ali and Srivastava, Gautam},
  year = {2021},
  month = feb,
  journal = {Future Generation Computer Systems},
  volume = {115},
  pages = {619--640},
  issn = {0167-739X},
  urldate = {2024-02-11},
  abstract = {Federated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL's security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.},
  keywords = {Artificial intelligence,Distributed learning,Federated learning,Federated machine learning,Machine learning,Privacy,Security}
}

@misc{mullerLocalizedShortcutRemoval2023,
  title = {Localized {{Shortcut Removal}}},
  author = {M{\"u}ller, Nicolas M. and Jacobs, Jochen and Williams, Jennifer and B{\"o}ttinger, Konstantin},
  year = {2023},
  month = may,
  number = {arXiv:2211.15510},
  eprint = {2211.15510},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2023-12-01},
  abstract = {Machine learning is a data-driven field, and the quality of the underlying datasets plays a crucial role in learning success. However, high performance on held-out test data does not necessarily indicate that a model generalizes or learns anything meaningful. This is often due to the existence of machine learning shortcuts - features in the data that are predictive but unrelated to the problem at hand. To address this issue for datasets where the shortcuts are smaller and more localized than true features, we propose a novel approach to detect and remove them. We use an adversarially trained lens to detect and eliminate highly predictive but semantically unconnected clues in images. In our experiments on both synthetic and real-world data, we show that our proposed approach reliably identifies and neutralizes such shortcuts without causing degradation of model performance on clean data. We believe that our approach can lead to more meaningful and generalizable machine learning models, especially in scenarios where the quality of the underlying datasets is crucial.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@article{murshedMachineLearningNetwork2022,
  title = {Machine {{Learning}} at the {{Network Edge}}: {{A Survey}}},
  shorttitle = {Machine {{Learning}} at the {{Network Edge}}},
  author = {Murshed, M. G. Sarwar and Murphy, Christopher and Hou, Daqing and Khan, Nazar and Ananthanarayanan, Ganesh and Hussain, Faraz},
  year = {2022},
  month = nov,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {8},
  eprint = {1908.00080},
  primaryclass = {cs, stat},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  urldate = {2023-07-20},
  abstract = {Resource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e close to the IoT devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Statistics - Machine Learning}
}

@inproceedings{naikIntroductionFederatedLearning2024,
  title = {An {{Introduction}} to~{{Federated Learning}}: {{Working}}, {{Types}}, {{Benefits}} and~{{Limitations}}},
  shorttitle = {An {{Introduction}} to~{{Federated Learning}}},
  booktitle = {Advances in {{Computational Intelligence Systems}}},
  author = {Naik, Dishita and Naik, Nitin},
  editor = {Naik, Nitin and Jenkins, Paul and Grace, Paul and Yang, Longzhi and Prajapat, Shaligram},
  year = {2024},
  pages = {3--17},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  abstract = {Machine learning has been constantly evolving and revolutionizing every aspect of our lives. There is ongoing research to enhance and modify machine learning models where scientists and researchers are finding ways to improve the effectiveness and adaptability of models with the changing technology moulding to user requirements for real life application. The main challenges in this endeavour of enhancing machine learning models are obtaining quality data, selecting an appropriate model, and ensuring the data privacy. Federated learning has been developed to address the aforementioned challenges, which is an effective way to train machine learning models in a collaborative manner by using the local data from a large number of devices without directly exchanging their raw data whilst simultaneously delivering on model performance. Federated learning is not just a type of machine learning, it is an amalgamation of several technologies and techniques. To fully understand its concepts a comprehensive study is required. This paper aims to simplify the fundamentals of federated learning in order to provide a better understanding of it. It explains federated learning in a step-by-step manner covering its comprehensive definition, detailed working, different types, benefits and limitations.},
  isbn = {978-3-031-47508-5},
  langid = {english},
  language = {en},
  keywords = {Centralized federated learning,Cross-device federated learning,Cross-silo federated learning,Decentralized federated learning,Distributed machine learning (DML),Federated learning (FL),Federated machine learning (FML),Horizontal federated learning,Vertical federated learning}
}

@article{nakhodnovLossFunctionDynamics2022,
  title = {Loss {{Function Dynamics}} and {{Landscape}} for {{Deep Neural Networks Trained}} with {{Quadratic Loss}}},
  author = {Nakhodnov, M. S. and Kodryan, M. S. and Lobacheva, E. M. and Vetrov, D. S.},
  year = {2022},
  month = dec,
  journal = {Doklady Mathematics},
  volume = {106},
  number = {1},
  pages = {S43-S62},
  issn = {1531-8362},
  urldate = {2024-05-18},
  abstract = {Knowledge of the loss landscape geometry makes it possible to successfully explain the behavior of neural networks, the dynamics of their training, and the relationship between resulting solutions and hyperparameters, such as the regularization method, neural network architecture, or learning rate schedule. In this paper, the dynamics of learning and the surface of the standard cross-entropy loss function and the currently popular mean squared error (MSE) loss function for scale-invariant networks with normalization are studied. Symmetries are eliminated via the transition to optimization on a sphere. As a result, three learning phases with fundamentally different properties are revealed depending on the learning step on the sphere, namely, convergence phase, phase of chaotic equilibrium, and phase of destabilized learning. These phases are observed for both loss functions, but larger networks and longer learning for the transition to the convergence phase are required in the case of MSE loss.},
  langid = {english},
  language = {en},
  keywords = {batch normalization,MSE loss function,optimization,scale invariance,training of neural networks}
}

@inproceedings{namkoongStochasticGradientMethods2016,
  title = {Stochastic {{Gradient Methods}} for {{Distributionally Robust Optimization}} with F-Divergences},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Namkoong, Hongseok and Duchi, John C},
  year = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-12-03},
  abstract = {We develop efficient solution methods for a robust empirical risk minimization problem designed to give calibrated confidence intervals on performance and provide optimal tradeoffs between bias and variance. Our methods apply to distributionally robust optimization problems proposed by Ben-Tal et al., which put more weight on observations inducing high loss via a worst-case approach over a non-parametric uncertainty set on the underlying data distribution. Our algorithm solves the resulting minimax problems with nearly the same computational cost of stochastic gradient descent through the use of several carefully designed data structures. For a sample of size n, the per-iteration cost of our method scales as O(log n), which allows us to give optimality certificates that distributionally robust optimization provides at little extra cost compared to empirical risk minimization and stochastic gradient methods.}
}

@inproceedings{navazFederatedQualityProfiling2022,
  title = {Federated {{Quality Profiling}}: {{A}} Quality Evaluation of Patient Monitoring at the {{Edge}}},
  shorttitle = {Federated {{Quality Profiling}}},
  booktitle = {2022 {{International Wireless Communications}} and {{Mobile Computing}} ({{IWCMC}})},
  author = {Navaz, Alramzana Nujum and Serhani, Mohamed Adel and El Kassabi, Hadeel T.},
  year = {2022},
  month = may,
  pages = {1015--1021},
  issn = {2376-6506},
  urldate = {2024-01-23},
  abstract = {Continuous monitoring of patients involves collecting and analyzing sensory data from a multitude of sources. To overcome communication overhead, ensure data privacy and security, reduce data loss, and maintain efficient resource usage the processing and analytics are moved close to where the data is located (e.g., the Edge). Data quality (DQ) can be degraded because of imprecise or malfunctioning sensors, dynamic changes in the environment, transmission failures, or delays. Therefore, can mislead clinical judgments and cause incorrect actions, if not managed properly. In this paper, inspired by Federated Learning (FL), we propose a novel approach using Federated Data Quality (FDQ) Profiling to assess DQ at the edge considering a global quality profile aggregated based on local profiles. We conducted experiments to evaluate the effect of FDQ profiling on DQ improvement considering the assessment of outlier detection and unbalanced data. The results demonstrated that the improved DQ has a positive impact on the accuracy of four conventional machine learning models.}
}

@article{nayeriApplicationPlacementFog2021,
  title = {Application Placement in {{Fog}} Computing with {{AI}} Approach: {{Taxonomy}} and a State of the Art Survey},
  shorttitle = {Application Placement in {{Fog}} Computing with {{AI}} Approach},
  author = {Nayeri, Zahra Makki and Ghafarian, Toktam and Javadi, Bahman},
  year = {2021},
  month = jul,
  journal = {Journal of Network and Computer Applications},
  volume = {185},
  pages = {103078},
  issn = {1084-8045},
  urldate = {2023-07-28},
  abstract = {With the increasing use of the Internet of Things (IoT) in various fields and the need to process and store huge volumes of generated data, Fog computing was introduced to complement Cloud computing services. Fog computing offers basic services at the network for supporting IoT applications with low response time requirements. However, Fogs are distributed, heterogeneous, and their resources are limited, therefore efficient distribution of IoT applications tasks in Fog nodes, in order to meet quality of service (QoS) and quality of experience (QoE) constraints is challenging. In this survey, at first, we have an overview of basic concepts of Fog computing, and then review the application placement problem in Fog computing with focus on Artificial intelligence (AI) techniques. We target three main objectives with considering a characteristics of AI-based methods in Fog application placement problem: (i) categorizing evolutionary algorithms, (ii) categorizing machine learning algorithms, and (iii) categorizing combinatorial algorithms into subcategories includes a combination of machine learning and heuristic, a combination of evolutionary and heuristic, and a combinations of evolutionary and machine learning. Then the security considerations of application placement have been reviewed. Finally, we provide a number of open questions and issues as future works.},
  langid = {english},
  language = {english},
  keywords = {Application placement,Artificial intelligence,Edge computing,Fog computing,Resource management,Service placement,Task scheduling}
}

@incollection{neweyChapter36Large1994,
  title = {Chapter 36 {{Large}} Sample Estimation and Hypothesis Testing},
  booktitle = {Handbook of {{Econometrics}}},
  author = {Newey, Whitney K. and McFadden, Daniel},
  year = {1994},
  month = jan,
  volume = {4},
  pages = {2111--2245},
  publisher = {Elsevier},
  urldate = {2024-04-25},
  abstract = {Asymptotic distribution theory is the primary method used to examine the properties of econometric estimators and tests. We present conditions for obtaining cosistency and asymptotic normality of a very general class of estimators (extremum estimators). Consistent asymptotic variance estimators are given to enable approximation of the asymptotic distribution. Asymptotic efficiency is another desirable property then considered. Throughout the chapter, the general results are also specialized to common econometric estimators (e.g. MLE and GMM), and in specific examples we work through the conditions for the various results in detail. The results are also extended to two-step estimators (with finite-dimensional parameter estimation in the first step), estimators derived from nonsmooth objective functions, and semiparametric two-step estimators (with nonparametric estimation of an infinite-dimensional parameter in the first step). Finally, the trinity of test statistics is considered within the quite general setting of GMM estimation, and numerous examples are given.}
}

@misc{neyshaburImplicitRegularizationDeep2017,
  title = {Implicit {{Regularization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam},
  year = {2017},
  month = sep,
  number = {arXiv:1709.01953},
  eprint = {1709.01953},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-25},
  abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{nguyenLossSurfaceDeep2017,
  title = {The {{Loss Surface}} of {{Deep}} and {{Wide Neural Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Nguyen, Quynh and Hein, Matthias},
  year = {2017},
  month = jul,
  pages = {2603--2612},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-22},
  abstract = {While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.},
  langid = {english},
  language = {en}
}

@article{nguyenQualityNotQuantity2022,
  title = {Quality {{Not Quantity}}: {{On}} the {{Interaction}} between {{Dataset Design}} and {{Robustness}} of {{CLIP}}},
  shorttitle = {Quality {{Not Quantity}}},
  author = {Nguyen, Thao and Ilharco, Gabriel and Wortsman, Mitchell and Oh, Sewoong and Schmidt, Ludwig},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {21455--21469},
  urldate = {2023-11-29},
  langid = {english},
  language = {en}
}

@phdthesis{niroomandMachineLearningLoss2024,
  title = {On {{Machine Learning Loss Landscapes}}},
  author = {Niroomand, Maximilian},
  year = {2024},
  month = may,
  urldate = {2024-06-23},
  abstract = {Loss functions are pivotal to the training of every machine learning model. When evaluating the loss function over a large range of parameters, a loss landscape is obtained. In this thesis, loss landscapes for various classes of machine learning models are explored. Geometric properties of the loss landscape can provide deep insights into machine learning models and their decision making process. Today, the most critical questions in machine learning are matters of model performance, robustness and interpretability. Here, tools from the energy landscapes field in theoretical chemistry are used to study these questions for neural networks and Gaussian processes. This cross-disciplinary approach is facilitated by a collection of analogues between both fields, such as the concept of heat capacity, monotonic sequence basins, or catastrophe theory, as developed in this work. The energy landscape perspective of loss landscape proves to be a helpful approach to examine critical questions in the machine learning field. Energy landscapes tools are applied to neural network ensemble generation and reveal that different minima of the loss landscape specialise on different sections of the input data, improving classification accuracy when combined. A second application is the evaluation and selection of loss functions, guiding hyperparameter choices. By comparing the appAUC loss function with conventional alternatives, I identify that the appAUC, while more accurate, is less robust and therefore inferior for real-life problems. Lastly, monotonic sequence basins can be used to group minima for which conserved weights can be identified to decode algorithmic decision making. Using this approach, I am able to identify and quantify input feature relevance for classification problems, a fundamental step towards trustworthy and accountable machine learning systems. For Gaussian processes, I present an analysis of the loss function hyperparameter space over different kernel choices, specifically over changes in the Mat{\'e}rn smoothness parameter \${\textbackslash}nu\$, which is equivalent to different kernel choices. I identify fold catastrophes in the landscape as \${\textbackslash}nu\$ changes around critical points and highlight the non-optimality of half-integer parameterisations of \${\textbackslash}nu\$ that are commonly employed in the field. Towards the end of this dissertation, a comparison of loss landscapes between machine learning models is provided, and suggestions for future work are highlighted.},
  langid = {english},
  language = {eng},
  school = {University of Cambridge}
}

@inproceedings{nokhwalRTRARapidTraining2023,
  title = {{{RTRA}}: {{Rapid Training}} of {{Regularization-based Approaches}} in {{Continual Learning}}},
  shorttitle = {{{RTRA}}},
  booktitle = {2023 10th {{International Conference}} on {{Soft Computing}} \& {{Machine Intelligence}} ({{ISCMI}})},
  author = {Nokhwal, Sahil and Kumar, Nirman},
  year = {2023},
  month = nov,
  pages = {188--192},
  issn = {2640-0146},
  urldate = {2024-08-08},
  abstract = {Catastrophic forgetting(CF) is a significant challenge in continual learning (CL). In regularization-based approaches to mitigate CF, modifications to important training parameters are penalized in subsequent tasks using an appropriate loss function. We propose the RTRA, a modification to the widely used Elastic Weight Consolidation (EWC) regularization scheme, using the Natural Gradient for loss function optimization. Our approach improves the training of regularization-based methods without sacrificing test-data performance. We compare the proposed RTRA approach against EWC using the iFood251 dataset. We show that RTRA has a clear edge over the state-of-the-art approaches.},
  keywords = {Adaptive learning,Catastrophic forgetting,Class-incremental learning,Continual learning,Dynamic learning,Incremental learning,Learning from non-stationary distributions,Learning from streaming data,Learning on the fly,Learning with limited data,Learning without forgetting,Lifelong learning,Machine intelligence,Memory-aware learning,Neural networks,Never-ending learning,Online learning,Optimization,Plasticity in neural networks,Sequential learning,Task analysis,Testing,Training}
}

@article{okaforImprovingDataQuality2020,
  title = {Improving {{Data Quality}} of {{Low-cost IoT Sensors}} in {{Environmental Monitoring Networks Using Data Fusion}} and {{Machine Learning Approach}}},
  author = {Okafor, Nwamaka U. and Alghorani, Yahia and Delaney, Declan T.},
  year = {2020},
  month = sep,
  journal = {ICT Express},
  volume = {6},
  number = {3},
  pages = {220--228},
  issn = {2405-9595},
  urldate = {2023-10-24},
  abstract = {Environmental monitoring has become an active research area due to the current rise in the global climate change crises. Current environmental monitoring solutions, however, are characterized by high cost of acquisition and complexity of installation; often requiring extensive resources, infrastructure and expertise. It is infeasible to achieve with these solutions, high density in-situ networks such as are required to build refined scale models to facilitate robust monitoring, thus, leaving large gaps within the collected dataset. Low-Cost Sensors (LCS) can offer high-resolution spatiotemporal measurements which could be used to supplement existing dataset from current environmental monitoring solutions. LCS however, require frequent calibration in order to provide accurate and reliable data as they are often affected by environmental conditions when deployed on the field. Calibrating LCS can help to improve their data quality and ensure they are collecting accurate data. Achieving effective calibration, however, requires identifying factors that affect sensor's data quality for a given measurement. This study evaluates the performance of three Feature Selection (FS) algorithms including Forward Feature Selection (FFS), Backward Elimination (BE) and Exhaustive Feature Selection (EFS) in identifying factors that affect data quality of low-cost IoT sensors in environmental monitoring networks. Applying the concept of data fusion, sensors data were merged with environmental factors and integrated into a single calibration equation to calibrate cairclipO3/NO2 and cairclipNO2 sensors using Linear Regression (LR) and Artificial Neural Networks (ANN). The study showed the effectiveness of calibration in improving low-cost IoT sensor data quality and also demonstrated the convenience of feature selection and the ability of data fusion to provide more consistent, accurate and reliable information for calibration models. The analysis showed that the cairclipO3/NO2 sensor provided measurements that have good correlation with reference measurements whereas the cairclipNO2 sensor showed no reasonable correlation with the reference data. Calibrating the cairclipO3/NO2 yielded good improvement in its measurement outputs when compared to reference measurements (R2=0.83). However, calibrating the cairclipNO2 sensor data yielded no significant improvement in its data quality.},
  keywords = {Data fusion,Environmental monitoring,Feature selection,Internet of Things (ioT),Machine learning}
}

@article{ollivierInformationGeometricOptimizationAlgorithms2017,
  title = {Information-{{Geometric Optimization Algorithms}}: {{A Unifying Picture}} via {{Invariance Principles}}},
  shorttitle = {Information-{{Geometric Optimization Algorithms}}},
  author = {Ollivier, Yann and Arnold, Ludovic and Auger, Anne and Hansen, Nikolaus},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {18},
  pages = {1--65},
  issn = {1533-7928},
  urldate = {2024-08-17},
  abstract = {We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space XXX into a continuous-time black-box optimization method on XXX, the information-geometric optimization (IGO) method. Invariance as a major design principle keeps the number of arbitrary choices to a minimum. The resulting IGO flow is the flow of an ordinary differential equation conducting the natural gradient ascent of an adaptive, time-dependent transformation of the objective function. It makes no particular assumptions on the objective function to be optimized. The IGO method produces explicit IGO algorithms through time discretization. It naturally recovers versions of known algorithms and offers a systematic way to derive new ones. In continuous search spaces, IGO algorithms take a form related to natural evolution strategies (NES). The cross-entropy method is recovered in a particular case with a large time step, and can be extended into a smoothed, parametrization-independent maximum likelihood update (IGO-ML). When applied to the family of Gaussian distributions on {\textbackslash}Rd{\textbackslash}Rd{\textbackslash}R{\textasciicircum}d, the IGO framework recovers a version of the well-known CMA-ES algorithm and of xNES. For the family of Bernoulli distributions on \{0,1\}d\{0,1\}d{\textbackslash}\{0,1{\textbackslash}\}{\textasciicircum}d, we recover the seminal PBIL algorithm and cGA. For the distributions of restricted Boltzmann machines, we naturally obtain a novel algorithm for discrete optimization on \{0,1\}d\{0,1\}d{\textbackslash}\{0,1{\textbackslash}\}{\textasciicircum}d. All these algorithms are natural instances of, and unified under, the single information-geometric optimization framework. The IGO method achieves, thanks to its intrinsic formulation, maximal invariance properties: invariance under reparametrization of the search space XXX, under a change of parameters of the probability distribution, and under increasing transformation of the function to be optimized. The latter is achieved through an adaptive, quantile-based formulation of the objective. Theoretical considerations strongly suggest that IGO algorithms are essentially characterized by a minimal change of the distribution over time. Therefore they have minimal loss in diversity through the course of optimization, provided the initial diversity is high. First experiments using restricted Boltzmann machines confirm this insight. As a simple consequence, IGO seems to provide, from information theory, an elegant way to simultaneously explore several valleys of a fitness landscape in a single run.}
}

@book{olsonDataQualityAccuracy2003,
  title = {Data Quality: The Accuracy Dimension},
  shorttitle = {Data Quality},
  author = {Olson, Jack E.},
  year = {2003},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  abstract = {"Data Quality: The Accuracy Dimension is about assessing the quality of corporate data and improving its accuracy using the data profiling method. Corporate data is increasingly important as companies continue to find new ways to use it Likewise, improving the accuracy of data in information systems is fast becoming a major goal as companies realize how much it affects their bottom line. Data profiling is a new technology that supports and enhances the accuracy of databases throu Jack Olson explains data profiling and shows how it fits into the large picture of data quality."--Jacket},
  isbn = {978-1-55860-891-7},
  langid = {english},
  language = {eng},
  lccn = {006.74 (XML)}
}

@article{olteanuSocialDataBiases2019,
  title = {Social {{Data}}: {{Biases}}, {{Methodological Pitfalls}}, and {{Ethical Boundaries}}},
  shorttitle = {Social {{Data}}},
  author = {Olteanu, Alexandra and Castillo, Carlos and Diaz, Fernando and K{\i}c{\i}man, Emre},
  year = {2019},
  journal = {Frontiers in Big Data},
  volume = {2},
  issn = {2624-909X},
  urldate = {2023-12-26},
  abstract = {Social data in digital form---including user-generated content, expressed or implicit relations between people, and behavioral traces---are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding ``what the world thinks'' about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the na{\"i}ve usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.``For your own sanity, you have to remember that not all problems can be solved. Not all problems can be solved, but all problems can be illuminated.'' --Ursula Franklin1}
}

@article{onetoImprovedAnalysisRademacher2013,
  title = {An Improved Analysis of the {{Rademacher}} Data-Dependent Bound Using Its Self Bounding Property},
  author = {Oneto, Luca and Ghio, Alessandro and Anguita, Davide and Ridella, Sandro},
  year = {2013},
  month = aug,
  journal = {Neural Networks},
  volume = {44},
  pages = {107--111},
  issn = {0893-6080},
  urldate = {2024-05-07},
  abstract = {The problem of assessing the performance of a classifier, in the finite-sample setting, has been addressed by Vapnik in his seminal work by using data-independent measures of complexity. Recently, several authors have addressed the same problem by proposing data-dependent measures, which tighten previous results by taking in account the actual data distribution. In this framework, we derive some data-dependent bounds on the generalization ability of a classifier by exploiting the Rademacher Complexity and recent concentration results: in addition of being appealing for practical purposes, as they exploit empirical quantities only, these bounds improve previously known results.},
  keywords = {Concentration of measure,Data-dependent bounds,Error estimation,Rademacher complexity}
}

@inproceedings{orvietoShadowingPropertiesOptimization2019,
  title = {Shadowing {{Properties}} of {{Optimization Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Orvieto, Antonio and Lucchi, Aurelien},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-09-26},
  abstract = {Ordinary differential equation (ODE) models of gradient-based optimization methods can provide insights into the dynamics of learning and inspire the design of new algorithms. Unfortunately, this thought-provoking perspective is weakened by the fact that, in the worst case, the error between the algorithm steps and its ODE approximation grows exponentially with the number of iterations. In an attempt to encourage the use of continuous-time methods in optimization, we show that, if some additional regularity on the objective is assumed, the ODE representations of Gradient Descent and Heavy-ball do not suffer from the aforementioned problem, once we allow for a small perturbation on the algorithm initial condition. In the dynamical systems literature, this phenomenon is called shadowing. Our analysis relies on the concept of hyperbolicity, as well as on tools from numerical analysis.}
}

@article{ouadrhiriDifferentialPrivacyDeep2022,
  title = {Differential {{Privacy}} for {{Deep}} and {{Federated Learning}}: {{A Survey}}},
  shorttitle = {Differential {{Privacy}} for {{Deep}} and {{Federated Learning}}},
  author = {Ouadrhiri, Ahmed El and Abdelhadi, Ahmed},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {22359--22380},
  issn = {2169-3536},
  urldate = {2024-04-11},
  abstract = {Users' privacy is vulnerable at all stages of the deep learning process. Sensitive information of users may be disclosed during data collection, during training, or even after releasing the trained learning model. Differential privacy (DP) is one of the main approaches proven to ensure strong privacy protection in data analysis. DP protects the users' privacy by adding noise to the original dataset or the learning parameters. Thus, an attacker could not retrieve the sensitive information of an individual involved in the training dataset. In this survey paper, we analyze and present the main ideas based on DP to guarantee users' privacy in deep and federated learning. In addition, we illustrate all types of probability distributions that satisfy the DP mechanism, with their properties and use cases. Furthermore, we bridge the gap in the literature by providing a comprehensive overview of the different variants of DP, highlighting their advantages and limitations. Our study reveals the gap between theory and application, accuracy, and robustness of DP. Finally, we provide several open problems and future research directions.},
  keywords = {Computational modeling,Cryptography,Deep learning,differential privacy,Differential privacy,federated learning,Privacy,privacy protection,probability distribution,Remuneration,Servers,Training}
}

@article{owusu-agyemangGuaranteedDistributedMachine2021,
  title = {Guaranteed Distributed Machine Learning: {{Privacy-preserving}} Empirical Risk Minimization},
  shorttitle = {Guaranteed Distributed Machine Learning},
  author = {{Owusu-Agyemang}, Kwabena and Qin, Zhen and Benjamin, Appiah and Xiong, Hu and Qin, Zhiguang and {University of Electronic Science and Technology of China, School of Information and Software Engineering, China}},
  year = {2021},
  journal = {Mathematical Biosciences and Engineering},
  volume = {18},
  number = {4},
  pages = {4772--4796},
  issn = {1551-0018},
  urldate = {2023-12-02}
}

@article{papyanTracesClassCrossClass2020,
  title = {Traces of {{Class}}/{{Cross-Class Structure Pervade Deep Learning Spectra}}},
  author = {Papyan, Vardan},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {252},
  pages = {1--64},
  issn = {1533-7928},
  urldate = {2024-07-10},
  abstract = {Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deep neural network spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, ``spikes'', and small but distinct continuous distributions, ``bumps'', often seen beyond the edge of a ``main bulk''.}
}

@inproceedings{parkDifferentiallyPrivateSharpnessAware2023,
  title = {Differentially {{Private Sharpness-Aware Training}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Park, Jinseong and Kim, Hoki and Choi, Yujin and Lee, Jaewook},
  year = {2023},
  month = jul,
  pages = {27204--27224},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-07-07},
  abstract = {Training deep learning models with differential privacy (DP) results in a degradation of performance. The training dynamics of models with DP show a significant difference from standard training, whereas understanding the geometric properties of private learning remains largely unexplored. In this paper, we investigate sharpness, a key factor in achieving better generalization, in private learning. We show that flat minima can help reduce the negative effects of per-example gradient clipping and the addition of Gaussian noise. We then verify the effectiveness of Sharpness-Aware Minimization (SAM) for seeking flat minima in private learning. However, we also discover that SAM is detrimental to the privacy budget and computational time due to its two-step optimization. Thus, we propose a new sharpness-aware training method that mitigates the privacy-optimization trade-off. Our experimental results demonstrate that the proposed method improves the performance of deep learning models with DP from both scratch and fine-tuning. Code is available at https://github.com/jinseongP/DPSAT.},
  langid = {english},
  language = {en}
}

@inproceedings{pasquiniEludingSecureAggregation2022,
  title = {Eluding {{Secure Aggregation}} in {{Federated Learning}} via {{Model Inconsistency}}},
  booktitle = {Proceedings of the {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Pasquini, D. and Francati, D. and Ateniese, G.},
  year = {2022},
  pages = {2429--2443},
  issn = {1543-7221},
  abstract = {Secure aggregation is a cryptographic protocol that securely computes the aggregation of its inputs. It is pivotal in keeping model updates private in federated learning. Indeed, the use of secure aggregation prevents the server from learning the value and the source of the individual model updates provided by the users, hampering inference and data attribution attacks. In this work, we show that a malicious server can easily elude secure aggregation as if the latter were not in place. We devise two different attacks capable of inferring information on individual private training datasets, independently of the number of users participating in the secure aggregation. This makes them concrete threats in large-scale, real-world federated learning applications. The attacks are generic and equally effective regardless of the secure aggregation protocol used They exploit a vulnerability of the federated learning protocol caused by incorrect usage of secure aggregation and lack of parameter validation. Our work demonstrates that current implementations of federated learning with secure aggregation offer only a ''false sense of security.'' {\copyright} 2022 ACM.},
  isbn = {978-1-4503-9450-5},
  langid = {english},
  language = {English},
  keywords = {federated learning,model inconsistency,secure aggregation}
}

@inproceedings{patel2ndInternationalWorkshop2021,
  title = {2nd {{International Workshop}} on {{Data Quality Assessment}} for {{Machine Learning}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Patel, Hima and Ishikawa, Fuyuki and {Berti-Equille}, Laure and Gupta, Nitin and Mehta, Sameep and Masuda, Satoshi and Mujumdar, Shashank and Afzal, Shazia and Bedathur, Srikanta and Nishi, Yasuharu},
  year = {2021},
  month = aug,
  pages = {4147--4148},
  publisher = {ACM},
  address = {Virtual Event Singapore},
  urldate = {2023-12-03},
  isbn = {978-1-4503-8332-5},
  langid = {english},
  language = {en},
  keywords = {data assessment,data quality,machine learning}
}

@inproceedings{paulDeepLearningData2021,
  title = {Deep {{Learning}} on a {{Data Diet}}: {{Finding Important Examples Early}} in {{Training}}},
  shorttitle = {Deep {{Learning}} on a {{Data Diet}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  year = {2021},
  volume = {34},
  pages = {20596--20607},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-07},
  abstract = {Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores---the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores---and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generalize to other configurations. Compared to recent work that prunes data by discarding examples that are rarely forgotten over the course of training, our scores use only local information early in training. We also use our scores to detect noisy examples and study training dynamics through the lens of important examples---we investigate how the data distribution shapes the loss surface and identify subspaces of the model's data representation that are relatively stable over training.}
}

@book{pearlCausalityModelsReasoning2000,
  title = {Causality: Models, Reasoning, and Inference},
  shorttitle = {Causality},
  author = {Pearl, Judea},
  year = {2000},
  publisher = {Cambridge University Press},
  address = {Cambridge, U.K. ; New York},
  isbn = {978-0-521-89560-6 978-0-521-77362-1},
  lccn = {BD541 .P43 2000},
  keywords = {Causation,Probabilities}
}

@inproceedings{penningtonGeometryNeuralNetwork2017,
  title = {Geometry of {{Neural Network Loss Surfaces}} via {{Random Matrix Theory}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Pennington, Jeffrey and Bahri, Yasaman},
  year = {2017},
  month = jul,
  pages = {2798--2806},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-18},
  abstract = {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, {$\phi\phi\backslash$}phi, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1/2(1-{$\phi$})21/2(1-{$\phi$})21/2(1-{\textbackslash}phi){\textasciicircum}2.},
  langid = {english},
  language = {en}
}

@inproceedings{penningtonNonlinearRandomMatrix2017,
  title = {Nonlinear Random Matrix Theory for Deep Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pennington, Jeffrey and Worah, Pratik},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-19}
}

@inproceedings{petersenPostprocessingIndividualFairness2021,
  title = {Post-Processing for {{Individual Fairness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Petersen, Felix and Mukherjee, Debarghya and Sun, Yuekai and Yurochkin, Mikhail},
  year = {2021},
  volume = {34},
  pages = {25944--25955},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-01-30},
  abstract = {Post-processing in algorithmic fairness is a versatile approach for correcting bias in ML systems that are already used in production. The main appeal of post-processing is that it avoids expensive retraining. In this work, we propose general post-processing algorithms for individual fairness (IF). We consider a setting where the learner only has access to the predictions of the original model and a similarity graph between individuals, guiding the desired fairness constraints. We cast the IF post-processing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired "treat similar individuals similarly" interpretation. Our theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, our post-processing algorithms correct individual biases in large-scale NLP models such as BERT, while preserving accuracy.}
}

@inproceedings{petzkaRelativeFlatnessGeneralization2021,
  title = {Relative {{Flatness}} and {{Generalization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Petzka, Henning and Kamp, Michael and Adilova, Linara and Sminchisescu, Cristian and Boley, Mario},
  year = {2021},
  volume = {34},
  pages = {18420--18432},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-30},
  abstract = {Flatness of the loss curve is conjectured to be connected to the generalization ability of machine learning models, in particular neural networks. While it has been empirically observed that flatness measures consistently correlate strongly with generalization, it is still an open theoretical problem why and under which circumstances flatness is connected to generalization, in particular in light of reparameterizations that change certain flatness measures but leave generalization unchanged. We investigate the connection between flatness and generalization by relating it to the interpolation from representative data, deriving notions of representativeness, and feature robustness. The notions allow us to rigorously connect flatness and generalization and to identify conditions under which the connection holds. Moreover, they give rise to a novel, but natural relative flatness measure that correlates strongly with generalization, simplifies to ridge regression for ordinary least squares, and solves the reparameterization issue.}
}

@article{peyvandiPrivacypreservingFederatedLearning2022,
  title = {Privacy-Preserving Federated Learning for Scalable and High Data Quality Computational-Intelligence-as-a-Service in {{Society}} 5.0},
  author = {Peyvandi, Amirhossein and Majidi, Babak and Peyvandi, Soodeh and Patra, Jagdish C.},
  year = {2022},
  month = jul,
  journal = {Multimedia Tools and Applications},
  volume = {81},
  number = {18},
  pages = {25029--25050},
  issn = {1573-7721},
  urldate = {2023-12-01},
  abstract = {Training supervised machine learning models like deep learning requires high-quality labelled datasets that contain enough samples from various categories and specific cases. The Data as a Service (DaaS) can provide this high-quality data for training efficient machine learning models. However, the issue of privacy can minimize the participation of the data owners in DaaS provision. In this paper, a blockchain-based decentralized federated learning framework for secure, scalable, and privacy-preserving computational intelligence, called Decentralized Computational Intelligence as a Service (DCIaaS), is proposed. The proposed framework is able to improve data quality, computational intelligence quality, data equality, and computational intelligence equality for complex machine learning tasks. The proposed framework uses the blockchain network for secure decentralized transfer and sharing of data and machine learning models on the cloud. As a case study for multimedia applications, the performance of DCIaaS framework for biomedical image classification and hazardous litter management is analysed. Experimental results show an increase in the accuracy of the models trained using the proposed framework compared to decentralized training. The proposed framework addresses the issue of privacy-preserving in DaaS using the distributed ledger technology and acts as a platform for crowdsourcing the training process of machine learning models.},
  langid = {english},
  language = {en},
  keywords = {Blockchain,Data as a service,Decentralized machine learning,Federated learning,Privacy preserving,Society 5.0}
}

@phdthesis{philipsDataDependentAnalysisLearning2005,
  title = {Data-{{Dependent Analysis}} of {{Learning Algorithms}}},
  author = {Philips, Petra},
  year = {2005},
  urldate = {2024-05-07},
  abstract = {This thesis studies the generalization ability of machine learning algorithms in a statistical setting. It focuses on the data-dependent analysis of the generalization performance of learning algorithms in order to make full use of the potential of the actual training sample from which these algorithms learn.{\P} First, we propose an extension of the standard framework for the derivation of generalization bounds for algorithms taking their hypotheses from random classes of functions. ... {\P} Second, we study in more detail generalization bounds for a specific algorithm which is of central importance in learning theory, namely the Empirical Risk Minimization algorithm (ERM). ...},
  copyright = {The Australian National University},
  langid = {english},
  language = {en},
  school = {The Australian National University and Research School of Information Sciences and Engineering},
  annotation = {Accepted: 2009-03-17T05:35:02Z\\
Last Modified: 2022-11-17}
}

@article{phongPrivacyPreservingDeepLearning2018,
  title = {Privacy-{{Preserving Deep Learning}} via {{Additively Homomorphic Encryption}}},
  author = {Phong, Le Trieu and Aono, Yoshinori and Hayashi, Takuya and Wang, Lihua and Moriai, Shiho},
  year = {2018},
  month = may,
  journal = {IEEE Transactions on Information Forensics and Security},
  volume = {13},
  number = {5},
  pages = {1333--1345},
  issn = {1556-6021},
  urldate = {2024-04-11},
  abstract = {We present a privacy-preserving deep learning system in which many learning participants perform neural network-based deep learning over a combined dataset of all, without revealing the participants' local data to a central server. To that end, we revisit the previous work by Shokri and Shmatikov (ACM CCS 2015) and show that, with their method, local data information may be leaked to an honest-but-curious server. We then fix that problem by building an enhanced system with the following properties: 1) no information is leaked to the server and 2) accuracy is kept intact, compared with that of the ordinary deep learning system also over the combined dataset. Our system bridges deep learning and cryptography: we utilize asynchronous stochastic gradient descent as applied to neural networks, in combination with additively homomorphic encryption. We show that our usage of encryption adds tolerable overhead to the ordinary deep learning system.},
  keywords = {additively homomorphic encryption,deep learning,Encryption,LWE-based encryption,Machine learning,neural network,Neural networks,Paillier encryption,Privacy,Servers}
}

@inproceedings{pleissIdentifyingMislabeledData2020,
  title = {Identifying {{Mislabeled Data}} Using the {{Area Under}} the {{Margin Ranking}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pleiss, Geoff and Zhang, Tianyi and Elenberg, Ethan and Weinberger, Kilian Q},
  year = {2020},
  volume = {33},
  pages = {17044--17056},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-11-29},
  abstract = {Not all data in a typical training set help with generalization; some samples can be overly ambiguous or outrightly mislabeled. This paper introduces a new method to identify such samples and mitigate their impact when training neural networks. At the heart of our algorithm is the Area Under the Margin (AUM) statistic, which exploits differences in the training dynamics of clean and mislabeled samples. A simple procedure - adding an extra class populated with purposefully mislabeled threshold samples - learns a AUM upper bound that isolates mislabeled data. This approach consistently improves upon prior work on synthetic and real-world datasets. On the WebVision50 classification task our method removes 17\% of training data, yielding a 1.6\% (absolute) improvement in test error. On CIFAR100 removing 13\% of the data leads to a 1.2\% drop in error.}
}

@article{poulinakisMachineLearningMethodsNoisy2023,
  title = {Machine-{{Learning Methods}} on {{Noisy}} and {{Sparse Data}}},
  author = {Poulinakis, Konstantinos and Drikakis, Dimitris and Kokkinakis, Ioannis W. and Spottswood, Stephen Michael},
  year = {2023},
  month = jan,
  journal = {Mathematics},
  volume = {11},
  number = {1},
  pages = {236},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  urldate = {2024-01-10},
  abstract = {Experimental and computational data and field data obtained from measurements are often sparse and noisy. Consequently, interpolating unknown functions under these restrictions to provide accurate predictions is very challenging. This study compares machine-learning methods and cubic splines on the sparsity of training data they can handle, especially when training samples are noisy. We compare deviation from a true function f using the mean square error, signal-to-noise ratio and the Pearson R2 coefficient. We show that, given very sparse data, cubic splines constitute a more precise interpolation method than deep neural networks and multivariate adaptive regression splines. In contrast, machine-learning models are robust to noise and can outperform splines after a training data threshold is met. Our study aims to provide a general framework for interpolating one-dimensional signals, often the result of complex scientific simulations or laboratory experiments.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {deep neural networks,feedforward neural networks,interpolation,machine learning,MARS,noisy data,sparse data,splines}
}

@misc{qiImpactsDirtyData2021,
  title = {Impacts of {{Dirty Data}}: And {{Experimental Evaluation}}},
  shorttitle = {Impacts of {{Dirty Data}}},
  author = {Qi, Zhixin and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
  year = {2021},
  month = apr,
  number = {arXiv:1803.06071},
  eprint = {1803.06071},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-01},
  abstract = {Data quality issues have attracted widespread attention due to the negative impacts of dirty data on data mining and machine learning results. The relationship between data quality and the accuracy of results could be applied on the selection of the appropriate algorithm with the consideration of data quality and the determination of the data share to clean. However, rare research has focused on exploring such relationship. Motivated by this, this paper conducts an experimental comparison for the effects of missing, inconsistent and conflicting data on classification and clustering algorithms. Based on the experimental findings, we provide guidelines for algorithm selection and data cleaning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{rafiFairnessPrivacyPreserving2024,
  title = {Fairness and Privacy Preserving in Federated Learning: {{A}} Survey},
  shorttitle = {Fairness and Privacy Preserving in Federated Learning},
  author = {Rafi, Taki Hasan and Noor, Faiza Anan and Hussain, Tahmid and Chae, Dong-Kyu},
  year = {2024},
  month = may,
  journal = {Information Fusion},
  volume = {105},
  pages = {102198},
  issn = {1566-2535},
  urldate = {2024-03-30},
  abstract = {Federated Learning (FL) is an increasingly popular form of distributed machine learning that addresses privacy concerns by allowing participants to collaboratively train machine learning models without exchanging their private data. Although FL emerged as a privacy-preserving alternative to centralized machine learning approaches, it faces significant challenges in preserving the privacy of its clients and mitigating potential bias against clients or disadvantaged groups. Most existing research in FL has addressed these two ethical notions separately, whereas ensuring privacy and fairness simultaneously in FL systems is of paramount importance. Moreover, current research efforts fail to balance privacy, fairness, and model performance, leaving systems vulnerable to various problems. To provide a comprehensive overview of these critical challenges, this work presents an integrated study of privacy and fairness concerns in the context of FL. In addition to providing an extensive review of the current literature on privacy and fairness issues, we also examine the existing approaches for achieving a balance between these two ethical notions to develop robust FL systems. Finally, we highlight potential research directions related to the challenges of implementing privacy-preserving and fairness-aware FL systems.},
  keywords = {Distributed machine learning,Fairness,Federated learning,Privacy-preserving}
}

@misc{rahimianDistributionallyRobustOptimization2019,
  title = {Distributionally {{Robust Optimization}}: {{A Review}}},
  shorttitle = {Distributionally {{Robust Optimization}}},
  author = {Rahimian, Hamed and Mehrotra, Sanjay},
  year = {2019},
  month = aug,
  eprint = {1908.05659},
  primaryclass = {cs, math, stat},
  urldate = {2023-12-03},
  abstract = {The concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. Statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. A modeling framework, called distributionally robust optimization (DRO), has recently received significant attention in both the operations research and statistical learning communities. This paper surveys main concepts and contributions to DRO, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@article{rahmanChallengesApplicationsDesign2021,
  title = {Challenges, {{Applications}} and {{Design Aspects}} of {{Federated Learning}}: {{A Survey}}},
  shorttitle = {Challenges, {{Applications}} and {{Design Aspects}} of {{Federated Learning}}},
  author = {Rahman, K. M. Jawadur and Ahmed, Faisal and Akhter, Nazma and Hasan, Mohammad and Amin, Ruhul and Aziz, Kazi Ehsan and Islam, A. K. M. Muzahidul and Mukta, Md. Saddam Hossain and Islam, A. K. M. Najmul},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {124682--124700},
  issn = {2169-3536},
  urldate = {2024-02-23},
  abstract = {Federated learning (FL) is a new technology that has been a hot research topic. It enables the training of an algorithm across multiple decentralized edge devices or servers holding local data samples without exchanging them. There are many application domains in which considerable properly labeled and complete data are not available in a centralized location (e.g., doctors' diagnoses from medical image analysis). There are also growing concerns over data and user privacy, as artificial intelligence is becoming ubiquitous in new application domains. As such, much research has recently been conducted in several areas within the nascent field of FL. Various surveys on different subtopics exist in the current literature, focusing on specific challenges, design aspects, and application domains. In this paper, we review existing contemporary works in related areas to understand the challenges and topics emphasized by each type of FL survey. Furthermore, we categorize FL research in terms of challenges, design factors, and applications, conducting a holistic review of each and outlining promising research directions.},
  langid = {english},
  language = {en}
}

@article{raiClientSelectionFederated2022,
  title = {Client {{Selection}} in {{Federated Learning}} under {{Imperfections}} in {{Environment}}},
  author = {Rai, Sumit and Kumari, Arti and Prasad, Dilip K.},
  year = {2022},
  month = mar,
  journal = {AI},
  volume = {3},
  number = {1},
  pages = {124--145},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-2688},
  urldate = {2024-04-07},
  abstract = {Federated learning promises an elegant solution for learning global models across distributed and privacy-protected datasets. However, challenges related to skewed data distribution, limited computational and communication resources, data poisoning, and free riding clients affect the performance of federated learning. Selection of the best clients for each round of learning is critical in alleviating these problems. We propose a novel sampling method named the irrelevance sampling technique. Our method is founded on defining a novel irrelevance score that incorporates the client characteristics in a single floating value, which can elegantly classify the client into three numerical sign defined pools for easy sampling. It is a computationally inexpensive, intuitive and privacy preserving sampling technique that selects a subset of clients based on quality and quantity of data on edge devices. It achieves 50--80\% faster convergence even in highly skewed data distribution in the presence of free riders based on lack of data and severe class imbalance under both Independent and Identically Distributed (IID) and Non-IID conditions. It shows good performance on practical application datasets.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  language = {en},
  keywords = {active learning,class imbalance,client selection,COOP,faster convergence,FedAvg,federated learning,free-riders,FSVRG}
}

@article{ramirez-gallegoSurveyDataPreprocessing2017,
  title = {A Survey on Data Preprocessing for Data Stream Mining: {{Current}} Status and Future Directions},
  shorttitle = {A Survey on Data Preprocessing for Data Stream Mining},
  author = {{Ram{\'i}rez-Gallego}, Sergio and Krawczyk, Bartosz and Garc{\'i}a, Salvador and Wo{\'z}niak, Micha{\l} and Herrera, Francisco},
  year = {2017},
  month = may,
  journal = {Neurocomputing},
  volume = {239},
  pages = {39--57},
  issn = {0925-2312},
  urldate = {2024-01-30},
  abstract = {Data preprocessing and reduction have become essential techniques in current knowledge discovery scenarios, dominated by increasingly large datasets. These methods aim at reducing the complexity inherent to real-world datasets, so that they can be easily processed by current data mining solutions. Advantages of such approaches include, among others, a faster and more precise learning process, and more understandable structure of raw data. However, in the context of data preprocessing techniques for data streams have a long road ahead of them, despite online learning is growing in importance thanks to the development of Internet and technologies for massive data collection. Throughout this survey, we summarize, categorize and analyze those contributions on data preprocessing that cope with streaming data. This work also takes into account the existing relationships between the different families of methods (feature and instance selection, and discretization). To enrich our study, we conduct thorough experiments using the most relevant contributions and present an analysis of their predictive performance, reduction rates, computational time, and memory usage. Finally, we offer general advices about existing data stream preprocessing algorithms, as well as discuss emerging future challenges to be faced in the domain of data stream preprocessing.},
  keywords = {Concept drift,Data discretization,Data mining,Data preprocessing,Data reduction,Data stream,Feature selection,Instance selection,Online learning}
}

@book{raschkaMachineLearningPyTorch2022,
  title = {Machine Learning with {{PyTorch}} and {{Scikit-Learn}}: {{Develop}} Machine Learning and Deep Learning Models with {{Python}}},
  shorttitle = {Machine Learning with {{PyTorch}} and {{Scikit-Learn}}},
  author = {Raschka, Sebastian and Liu, Yuxi and Mirjalili, Vahid and Dzhulgakov, Dmytro},
  year = {2022},
  series = {Expert Insight},
  publisher = {Packt},
  address = {Birmingham Mumbai},
  abstract = {This book of the bestselling and widely acclaimed Python Machine Learning series is a comprehensive guide to machine and deep learning using PyTorch's simple to code framework.Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesLearn applied machine learning with a solid foundation in theoryClear, intuitive explanations take you deep into the theory and practice of Python machine learningFully updated and expanded to cover PyTorch, transformers, XGBoost, graph neural networks, and best practicesBook DescriptionMachine Learning with PyTorch and Scikit-Learn is a comprehensive guide to machine learning and deep learning with PyTorch. It acts as both a step-by-step tutorial and a reference you'll keep coming back to as you build your machine learning systems.Packed with clear explanations, visualizations, and examples, the book covers all the essential machine learning techniques in depth. While some books teach you only to follow instructions, with this machine learning book, we teach the principles allowing you to build models and applications for yourself.Why PyTorch?PyTorch is the Pythonic way to learn machine learning, making it easier to learn and simpler to code with. This book explains the essential parts of PyTorch and how to create models using popular libraries, such as PyTorch Lightning and PyTorch Geometric.You will also learn about generative adversarial networks (GANs) for generating new data and training intelligent agents with reinforcement learning. Finally, this new edition is expanded to cover the latest trends in deep learning, including graph neural networks and large-scale transformers used for natural language processing (NLP).This PyTorch book is your companion to machine learning with Python, whether you're a Python developer new to machine learning or want to deepen your knowledge of the latest developments.What you will learnExplore frameworks, models, and techniques for machines to 'learn' from dataUse scikit-learn for machine learning and PyTorch for deep learningTrain machine learning classifiers on images, text, and moreBuild and train neural networks, transformers, and boosting algorithmsDiscover best practices for evaluating and tuning modelsPredict continuous target outcomes using regression analysisDig deeper into textual and social media data using sentiment analysisWho this book is forIf you have a good grasp of Python basics and want to start learning about machine learning and deep learning, then this is the book for you. This is an essential resource written for developers and data scientists who want to create practical machine learning and deep learning applications using scikit-learn and PyTorch.Before you get started with this book, you'll need a good understanding of calculus, as well as linear algebra},
  isbn = {978-1-80181-638-0 978-1-80181-931-2},
  langid = {english},
  language = {english}
}

@article{raychaudhuryFairnessFederatedLearning2022,
  title = {Fairness in {{Federated Learning}} via {{Core-Stability}}},
  author = {Ray Chaudhury, Bhaskar and Li, Linyi and Kang, Mintong and Li, Bo and Mehta, Ruta},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {5738--5750},
  urldate = {2023-08-31},
  abstract = {Federated learning provides an effective paradigm to jointly optimize a model benefited from rich distributed data while protecting data privacy. Nonetheless, the heterogeneity nature of distributed data makes it challenging to define and ensure fairness among local agents. For instance, it is intuitively "unfair" for agents with data of high quality to sacrifice their performance due to other agents with low quality data. Currently popular egalitarian and weighted equity-based fairness measures suffer from the aforementioned pitfall. In this work, we aim to formally represent this problem and address these fairness issues using concepts from co-operative game theory and social choice theory. We model the task of learning a shared predictor in the federated setting as a fair public decision making problem, and then define the notion of core-stable fairness: Given \$N\$ agents, there is no subset of agents \$S\$ that can benefit significantly by forming a coalition among themselves based on their utilities \$U\_N\$ and \$U\_S\$ (i.e., \${\textbackslash}frac\{{\textbar}S{\textbar}\}\{N\} U\_S {\textbackslash}geq U\_N\$). Core-stable predictors are robust to low quality local data from some agents, and additionally they satisfy Proportionality and Pareto-optimality, two well sought-after fairness and efficiency notions within social choice. We then propose an efficient federated learning protocol CoreFed to optimize a core stable predictor. CoreFed determines a core-stable predictor when the loss functions of the agents are convex. CoreFed also determines approximate core-stable predictors when the loss functions are not convex, like smooth neural networks. We further show the existence of core-stable predictors in more general settings using Kakutani's fixed point theorem. Finally, we empirically validate our analysis on two real-world datasets, and we show that CoreFed achieves higher core-stability fairness than FedAvg while having similar accuracy.},
  langid = {english},
  language = {english},
  keywords = {DQ effect}
}

@inproceedings{reisizadehRobustFederatedLearning2020,
  title = {Robust {{Federated Learning}}: {{The Case}} of {{Affine Distribution Shifts}}},
  shorttitle = {Robust {{Federated Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Reisizadeh, Amirhossein and Farnia, Farzan and Pedarsani, Ramtin and Jadbabaie, Ali},
  year = {2020},
  volume = {33},
  pages = {21554--21565},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-12-16},
  abstract = {Federated learning is a distributed  paradigm that aims at training  models using samples distributed across multiple users in a network while keeping the samples on users' devices with the aim of efficiency and  protecting users privacy. In such settings, the training data is often statistically heterogeneous and manifests various distribution shifts across users, which degrades the performance of the learnt model. The primary goal of this paper is to develop a robust federated learning algorithm that achieves satisfactory performance against distribution shifts in users' samples. To achieve this goal, we first consider a structured affine distribution shift in users' data that captures the device-dependent data heterogeneity in federated settings. This perturbation model is applicable to various federated learning problems such as image classification where the images undergo device-dependent imperfections, e.g. different intensity, contrast, and brightness. To address affine distribution shifts across users, we propose a Federated Learning framework Robust to Affine distribution shifts (FLRA) that is provably robust against affine Wasserstein shifts to the distribution of observed samples. To solve the FLRA's distributed minimax optimization problem, we propose a fast and efficient optimization method and provide convergence and performance  guarantees via a gradient Descent Ascent (GDA) method. We further prove generalization error bounds for the learnt classifier to show proper generalization from empirical distribution of samples to the true underlying distribution. We perform several numerical experiments to empirically support FLRA. We show that an affine distribution shift indeed suffices to significantly decrease the performance of the learnt classifier in a new test user, and our proposed algorithm achieves a significant gain in comparison to standard federated learning and adversarial training methods.}
}

@misc{renggliDataQualityDrivenView2021,
  title = {A {{Data Quality-Driven View}} of {{MLOps}}},
  author = {Renggli, Cedric and Rimanic, Luka and G{\"u}rel, Nezihe Merve and Karla{\v s}, Bojan and Wu, Wentao and Zhang, Ce},
  year = {2021},
  month = feb,
  number = {arXiv:2102.07750},
  eprint = {2102.07750},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-29},
  abstract = {Developing machine learning models can be seen as a process similar to the one established for traditional software development. A key difference between the two lies in the strong dependency between the quality of a machine learning model and the quality of the data used to train or perform evaluations. In this work, we demonstrate how different aspects of data quality propagate through various stages of machine learning development. By performing a joint analysis of the impact of well-known data quality dimensions and the downstream machine learning process, we show that different components of a typical MLOps pipeline can be efficiently designed, providing both a technical and theoretical perspective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning}
}

@article{restucciaQualityInformationMobile2017,
  title = {Quality of {{Information}} in {{Mobile Crowdsensing}}: {{Survey}} and {{Research Challenges}}},
  shorttitle = {Quality of {{Information}} in {{Mobile Crowdsensing}}},
  author = {Restuccia, Francesco and Ghosh, Nirnay and Bhattacharjee, Shameek and Das, Sajal K. and Melodia, Tommaso},
  year = {2017},
  month = nov,
  journal = {ACM Transactions on Sensor Networks},
  volume = {13},
  number = {4},
  pages = {34:1--34:43},
  issn = {1550-4859},
  urldate = {2023-11-25},
  abstract = {Smartphones have become the most pervasive devices in people's lives and are clearly transforming the way we live and perceive technology. Today's smartphones benefit from almost ubiquitous Internet connectivity and come equipped with a plethora of inexpensive yet powerful embedded sensors, such as an accelerometer, a gyroscope, a microphone, and a camera. This unique combination has enabled revolutionary applications based on the mobile crowdsensing paradigm, such as real-time road traffic monitoring, air and noise pollution, crime control, and wildlife monitoring, just to name a few. Differently from prior sensing paradigms, humans are now the primary actors of the sensing process, since they become fundamental in retrieving reliable and up-to-date information about the event being monitored. As humans may behave unreliably or maliciously, assessing and guaranteeing Quality of Information (QoI) becomes more important than ever. In this article, we provide a new framework for defining and enforcing the QoI in mobile crowdsensing and analyze in depth the current state of the art on the topic. We also outline novel research challenges, along with possible directions of future work.},
  keywords = {challenges,framework,information,Quality,reputation,survey,trust,truth discovery}
}

@article{rodriguesStudyGeneralizationFitness2020,
  title = {A {{Study}} of {{Generalization}} and {{Fitness Landscapes}} for {{Neuroevolution}}},
  author = {Rodrigues, Nuno M. and Silva, Sara and Vanneschi, Leonardo},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {108216--108234},
  issn = {2169-3536},
  urldate = {2024-05-13},
  abstract = {Fitness landscapes are a useful concept for studying the dynamics of meta-heuristics. In the last two decades, they have been successfully used for estimating the optimization capabilities of different flavors of evolutionary algorithms, including genetic algorithms and genetic programming. However, so far they have not been used for studying the performance of machine learning algorithms on unseen data, and they have not been applied to studying neuroevolution landscapes. This paper fills these gaps by applying fitness landscapes to neuroevolution, and using this concept to infer useful information about the learning and generalization ability of the machine learning method. For this task, we use a grammar-based approach to generate convolutional neural networks, and we study the dynamics of three different mutations used to evolve them. To characterize fitness landscapes, we study autocorrelation, entropic measure of ruggedness, and fitness clouds. Also, we propose the use of two additional evaluation measures: density clouds and overfitting measure. The results show that these measures are appropriate for estimating both the learning and the generalization ability of the considered neuroevolution configurations.},
  keywords = {Atmospheric measurements,Autocorrelation,convolutional neural networks,Correlation,density clouds,Density measurement,entropic measure of ruggedness,Evolutionary computation,fitness clouds,fitness landscapes,generalization,neuroevolution,Optimization,overfitting,Task analysis,Training}
}

@article{rodriguez-barrosoSurveyFederatedLearning2023,
  title = {Survey on Federated Learning Threats: {{Concepts}}, Taxonomy on Attacks and Defences, Experimental Study and Challenges},
  shorttitle = {Survey on Federated Learning Threats},
  author = {{Rodr{\'i}guez-Barroso}, Nuria and {Jim{\'e}nez-L{\'o}pez}, Daniel and Luz{\'o}n, M. Victoria and Herrera, Francisco and {Mart{\'i}nez-C{\'a}mara}, Eugenio},
  year = {2023},
  month = feb,
  journal = {Information Fusion},
  volume = {90},
  pages = {148--173},
  issn = {1566-2535},
  urldate = {2024-03-24},
  abstract = {Federated learning is a machine learning paradigm that emerges as a solution to the privacy-preservation demands in artificial intelligence. As machine learning, federated learning is threatened by adversarial attacks against the integrity of the learning model and the privacy of data via a distributed approach to tackle local and global learning. This weak point is exacerbated by the inaccessibility of data in federated learning, which makes the protection against adversarial attacks harder and evidences the need to furtherance the research on defence methods to make federated learning a real solution for safeguarding data privacy. In this paper, we present an extensive review of the threats of federated learning, as well as as their corresponding countermeasures, attacks versus defences. This survey provides a taxonomy of adversarial attacks and a taxonomy of defence methods that depict a general picture of this vulnerability of federated learning and how to overcome it. Likewise, we expound guidelines for selecting the most adequate defence method according to the category of the adversarial attack. Besides, we carry out an extensive experimental study from which we draw further conclusions about the behaviour of attacks and defences and the guidelines for selecting the most adequate defence method according to the category of the adversarial attack. Finally, we present our learned lessons and challenges.},
  keywords = {Adversarial attacks,Defences,Federated learning,Privacy attacks}
}

@inproceedings{royEmpiricalRiskMinimization2021,
  title = {On {{Empirical Risk Minimization}} with {{Dependent}} and {{Heavy-Tailed Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Roy, Abhishek and Balasubramanian, Krishnakumar and Erdogdu, Murat A},
  year = {2021},
  volume = {34},
  pages = {8913--8926},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-11-28},
  abstract = {In this work, we establish risk bounds for the Empirical Risk Minimization (ERM) with both dependent and heavy-tailed data-generating processes. We do so by extending the seminal works [Men15, Men18] on the analysis of ERM with heavytailed but independent and identically distributed observations, to the strictly stationary exponentially {$\beta$}-mixing case. Our analysis is based on explicitly controlling the multiplier process arising from the interaction between the noise and the function evaluations on inputs. It allows for the interaction to be even polynomially heavy-tailed, which covers a significantly large class of heavy-tailed models beyond what is analyzed in the learning theory literature. We illustrate our results by deriving rates of convergence for the high-dimensional linear regression problem with dependent and heavy-tailed data.}
}

@book{sadiqHandbookDataQuality2013,
  title = {Handbook of {{Data Quality}}: {{Research}} and {{Practice}}},
  shorttitle = {Handbook of {{Data Quality}}},
  author = {Sadiq, Shazia},
  year = {2013},
  series = {Handbook of Data Quality},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  abstract = {The issue of data quality is as old as data itself. However, the proliferation of diverse, large-scale and often publically available data on the Web has increased the risk of poor data quality and misleading data interpretations. On the other hand, data is now exposed at a much more strategic level e.g. through business intelligence systems, increasing manifold the stakes involved for individuals, corporations as well as government agencies. There, the lack of knowledge about data accuracy, currency or completeness can have erroneous and even catastrophic results. With these changes, traditional approaches to data management in general, and data quality control specifically, are challenged. There is an evident need to incorporate data quality considerations into the whole data cycle, encompassing managerial/governance as well as technical aspects. Data quality experts from research and industry agree that a unified framework for data quality management should bring together organizational, architectural and computational approaches. Accordingly, Sadiq structured this handbook in four parts: Part I is on organizational solutions, i.e. the development of data quality objectives for the organization, and the development of strategies to establish roles, processes, policies, and standards required to manage and ensure data quality. Part II, on architectural solutions, covers the technology landscape required to deploy developed data quality management processes, standards and policies. Part III, on computational solutions, presents effective and efficient tools and techniques related to record linkage, lineage and provenance, data uncertainty, and advanced integrity constraints. Finally, Part IV is devoted to case studies of successful data quality initiatives that highlight the various aspects of data quality in action. The individual chapters present both an overview of the respective topic in terms of historical research and/or practice and state of the},
  isbn = {978-3-642-36257-6},
  langid = {english},
  language = {eng},
  lccn = {006.74 (XML)}
}

@misc{sahooLearningBiasedSample2023,
  title = {Learning from a {{Biased Sample}}},
  author = {Sahoo, Roshni and Lei, Lihua and Wager, Stefan},
  year = {2023},
  month = jan,
  number = {arXiv:2209.01754},
  eprint = {2209.01754},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-25},
  abstract = {The empirical risk minimization approach to data-driven decision making assumes that we can learn a decision rule from training data drawn under the same conditions as the ones we want to deploy it in. However, in a number of settings, we may be concerned that our training sample is biased, and that some groups (characterized by either observable or unobservable attributes) may be under- or over-represented relative to the general population; and in this setting empirical risk minimization over the training set may fail to yield rules that perform well at deployment. We propose a model of sampling bias called \${\textbackslash}Gamma\$-biased sampling, where observed covariates can affect the probability of sample selection arbitrarily much but the amount of unexplained variation in the probability of sample selection is bounded by a constant factor. Applying the distributionally robust optimization framework, we propose a method for learning a decision rule that minimizes the worst-case risk incurred under a family of test distributions that can generate the training distribution under \${\textbackslash}Gamma\$-biased sampling. We apply a result of Rockafellar and Uryasev to show that this problem is equivalent to an augmented convex risk minimization problem. We give statistical guarantees for learning a model that is robust to sampling bias via the method of sieves, and propose a deep learning algorithm whose loss function captures our robust learning target. We empirically validate our proposed method in simulations and a case study on ICU length of stay prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology}
}

@misc{saleiroAequitasBiasFairness2019,
  title = {Aequitas: {{A Bias}} and {{Fairness Audit Toolkit}}},
  shorttitle = {Aequitas},
  author = {Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T. and Ghani, Rayid},
  year = {2019},
  month = apr,
  number = {arXiv:1811.05577},
  eprint = {1811.05577},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-10},
  abstract = {Recent work has raised concerns on the risk of unintended bias in AI systems being used nowadays that can affect individuals unfairly based on race, gender or religion, among other possible characteristics. While a lot of bias metrics and fairness definitions have been proposed in recent years, there is no consensus on which metric/definition should be used and there are very few available resources to operationalize them. Therefore, despite recent awareness, auditing for bias and fairness when developing and deploying AI systems is not yet a standard practice. We present Aequitas, an open source bias and fairness audit toolkit that is an intuitive and easy to use addition to the machine learning workflow, enabling users to seamlessly test models for several bias and fairness metrics in relation to multiple population sub-groups. Aequitas facilitates informed and equitable decisions around developing and deploying algorithmic decision making systems for both data scientists, machine learning researchers and policymakers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@article{sanchezsanchezFederatedTrustSolutionTrustworthy2024,
  title = {{{FederatedTrust}}: {{A}} Solution for Trustworthy Federated Learning},
  shorttitle = {{{FederatedTrust}}},
  author = {S{\'a}nchez S{\'a}nchez, Pedro Miguel and Huertas Celdr{\'a}n, Alberto and Xie, Ning and Bovet, G{\'e}r{\^o}me and Mart{\'i}nez P{\'e}rez, Gregorio and Stiller, Burkhard},
  year = {2024},
  month = mar,
  journal = {Future Generation Computer Systems},
  volume = {152},
  pages = {83--98},
  issn = {0167-739X},
  urldate = {2024-04-07},
  abstract = {The rapid expansion of the Internet of Things (IoT) and Edge Computing has presented challenges for centralized Machine and Deep Learning (ML/DL) methods due to the presence of distributed data silos that hold sensitive information. To address concerns regarding data privacy, collaborative and privacy-preserving ML/DL techniques like Federated Learning (FL) have emerged. FL ensures data privacy by design, as the local data of participants remains undisclosed during the creation of a global and collaborative model. However, data privacy and performance are insufficient since a growing need demands trust in model predictions. Existing literature has proposed various approaches dealing with trustworthy ML/DL (excluding data privacy), identifying robustness, fairness, explainability, and accountability as important pillars. Nevertheless, further research is required to identify trustworthiness pillars and evaluation metrics specifically relevant to FL models, as well as to develop solutions that can compute the trustworthiness level of FL models. This work examines the existing requirements for evaluating trustworthiness in FL and introduces a comprehensive taxonomy consisting of six pillars (privacy, robustness, fairness, explainability, accountability, and federation), along with over 30 metrics for computing the trustworthiness of FL models. Subsequently, an algorithm named FederatedTrust is designed based on the pillars and metrics identified in the taxonomy to compute the trustworthiness score of FL models. A prototype of FederatedTrust is implemented and integrated into the learning process of FederatedScope, a well-established FL framework. Finally, five experiments are conducted using different configurations of FederatedScope (with different participants, selection rates, training rounds, and differential privacy) to demonstrate the utility of FederatedTrust in computing the trustworthiness of FL models. Three experiments employ the FEMNIST dataset, and two utilize the N-BaIoT dataset, considering a real-world IoT security use case.},
  keywords = {Accountability,AI governance,Explainability,Fairness,Privacy,Robustness,Trust assessment,Trustworthy federated learning}
}

@book{sasaneOptimizationFunctionSpaces2016,
  title = {Optimization in Function Spaces},
  author = {Sasane, Amol},
  year = {2016},
  publisher = {Dover Publications},
  address = {Mineola, New York},
  abstract = {This highly readable volume on optimization in function spaces is based on author Amol Sasane's lecture notes, which he developed over several years while teaching a course for third-year undergraduates at the London School of Economics. The classroom-tested text is written in an informal but precise style that emphasizes clarity and detail, taking students step by step through each subject. Numerous examples throughout the text clarify methods, and a substantial number of exercises provide reinforcement. Detailed solutions to all of the exercises make this book ideal for self-study. The topics are relevant to students in engineering and economics as well as mathematics majors. Prerequisites include multivariable calculus and basic linear algebra. The necessary background in differential equations and elementary functional analysis is developed within the text, offering students a self-contained treatment},
  isbn = {978-0-486-81096-6},
  langid = {english},
  language = {eng},
  annotation = {OCLC: 947084298}
}

@article{sattlerClusteredFederatedLearning2021,
  title = {Clustered {{Federated Learning}}: {{Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints}}},
  shorttitle = {Clustered {{Federated Learning}}},
  author = {Sattler, Felix and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2021},
  month = aug,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {8},
  pages = {3710--3722},
  issn = {2162-2388},
  urldate = {2024-04-17},
  abstract = {Federated learning (FL) is currently the most widely adopted framework for collaborative training of (deep) machine learning models under privacy constraints. Albeit its popularity, it has been observed that FL yields suboptimal results if the local clients' data distributions diverge. To address this issue, we present clustered FL (CFL), a novel federated multitask learning (FMTL) framework, which exploits geometric properties of the FL loss surface to group the client population into clusters with jointly trainable data distributions. In contrast to existing FMTL approaches, CFL does not require any modifications to the FL communication protocol to be made, is applicable to general nonconvex objectives (in particular, deep neural networks), does not require the number of clusters to be known a priori, and comes with strong mathematical guarantees on the clustering quality. CFL is flexible enough to handle client populations that vary over time and can be implemented in a privacy-preserving way. As clustering is only performed after FL has converged to a stationary point, CFL can be viewed as a postprocessing method that will always achieve greater or equal performance than conventional FL by allowing clients to arrive at more specialized models. We verify our theoretical analysis in experiments with deep convolutional and recurrent neural networks on commonly used FL data sets.},
  keywords = {Clustering,Data models,distributed learning,federated learning,multi-task learning,Optimization,Privacy,Servers,Sociology,Statistics,Training}
}

@article{saxeMathematicalTheorySemantic2019,
  title = {A Mathematical Theory of Semantic Development in Deep Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2019},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {23},
  pages = {11537--11546},
  issn = {0027-8424, 1091-6490},
  urldate = {2024-07-18},
  abstract = {Significance             Over the course of development, humans learn myriad facts about items in the world, and naturally group these items into useful categories and structures. This semantic knowledge is essential for diverse behaviors and inferences in adulthood. How is this richly structured semantic knowledge acquired, organized, deployed, and represented by neuronal networks in the brain? We address this question by studying how the nonlinear learning dynamics of deep linear networks acquires information about complex environmental structures. Our results show that this deep learning dynamics can self-organize emergent hidden representations in a manner that recapitulates many empirical phenomena in human semantic development. Such deep networks thus provide a mathematically tractable window into the development of internal neural representations through experience.           ,              An extensive body of empirical research has revealed remarkable regularities in the acquisition, organization, deployment, and neural representation of human semantic knowledge, thereby raising a fundamental conceptual question: What are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences? We address this question by mathematically analyzing the nonlinear dynamics of learning in deep linear networks. We find exact solutions to this learning dynamics that yield a conceptual explanation for the prevalence of many disparate phenomena in semantic cognition, including the hierarchical differentiation of concepts through rapid developmental transitions, the ubiquity of semantic illusions between such transitions, the emergence of item typicality and category coherence as factors controlling the speed of semantic processing, changing patterns of inductive projection over development, and the conservation of semantic similarity in neural representations across species. Thus, surprisingly, our simple neural model qualitatively recapitulates many diverse regularities underlying semantic development, while providing analytic insight into how the statistical structure of an environment can interact with nonlinear deep-learning dynamics to give rise to these regularities.},
  langid = {english},
  language = {en}
}

@incollection{sayedComprehensiveReviewData2020,
  title = {A Comprehensive Review of Data Quality Management-and-Insurance and Respective Machine Learning and Deep Learning Based Techniques; Case Study, Class Imbalance (in the Context of {{MNIST}} Character Classification)},
  booktitle = {Developments of {{Artificial Intelligence Technologies}} in {{Computation}} and {{Robotics}}},
  author = {Sayed, Mohamad Al and Silva, Meluka De and Abhiram, Kolli and Kyandoghere, Kyamakya},
  year = {2020},
  month = jun,
  series = {World {{Scientific Proceedings Series}} on {{Computer Engineering}} and {{Information Science}}},
  volume = {Volume 12},
  pages = {1059--1068},
  publisher = {WORLD SCIENTIFIC},
  urldate = {2023-11-12},
  isbn = {9789811223327},
  keywords = {benchmarking selected neuro-computing based classifiers performance w.r.t. class imbalance,Data imperfections}
}

@inproceedings{schechtmanOrthogonalDirectionsConstrained2023,
  title = {Orthogonal {{Directions Constrained Gradient Method}}: From Non-Linear Equality Constraints to {{Stiefel}} Manifold},
  shorttitle = {Orthogonal {{Directions Constrained Gradient Method}}},
  booktitle = {Proceedings of {{Thirty Sixth Conference}} on {{Learning Theory}}},
  author = {Schechtman, Sholom and Tiapkin, Daniil and Muehlebach, Michael and Moulines, {\'E}ric},
  year = {2023},
  month = jul,
  pages = {1228--1258},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-08-21},
  abstract = {We consider the problem of minimizing a non-convex function over a smooth manifold M. We propose a novel algorithm, the Orthogonal Directions Constrained Gradient Method (ODCGM), which only requires computing a projection onto a vector space. ODCGM is infeasible but the iterates are constantly pulled towards the manifold, ensuring the convergence of ODCGM towards M. ODCGM is much simpler to implement than the classical methods which require the computation of a retraction. Moreover, we show that ODCGM exhibits the near-optimal oracle complexities O(1/{$\varepsilon$}{\textasciicircum}\{-2\}) and O(1/{$\varepsilon$}{\textasciicircum}\{-4\}) in the deterministic and stochastic cases, respectively. Furthermore, we establish that, under an appropriate choice of the projection metric, our method recovers the landing algorithm of Ablin and Peyr{\'e} (2022), a recently introduced algorithm for optimization over the Stiefel manifold. As a result, we significantly extend the analysis of Ablin and Peyr{\'e} (2022), establishingnear-optimal rates both in deterministic and stochastic frameworks. Finally, we perform numerical experiments, which shows the efficiency of ODCGM in a high-dimensional setting.},
  langid = {english},
  language = {en}
}

@article{schelterAutomatingLargescaleData2018,
  title = {Automating Large-Scale Data Quality Verification},
  author = {Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas},
  year = {2018},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {12},
  pages = {1781--1794},
  issn = {2150-8097},
  urldate = {2023-11-29},
  abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.},
  langid = {english},
  language = {en}
}

@misc{scheuermannEdgeIntroductionEdge2019,
  title = {The {{Edge}} Is {{Near}}: {{An Introduction}} to {{Edge Computing}}! - Inovex {{GmbH}}},
  author = {Scheuermann, Johannes M. and Bischoff, Maximilian},
  year = {2019},
  month = jun,
  urldate = {2023-09-27},
  howpublished = {https://www.inovex.de/de/blog/edge-computing-introduction/},
  keywords = {figure}
}

@article{seedatRethinkingPseudolabelingDatacentric2023,
  title = {Rethinking Pseudo-Labeling: {{Data-centric}} Insights Improve Semi-Supervised Learning},
  shorttitle = {Rethinking Pseudo-Labeling},
  author = {Seedat, Nabeel and Huynh, Nicolas and Imrie, Fergus and van der Schaar, Mihaela},
  year = {2023},
  month = oct,
  urldate = {2024-05-11},
  abstract = {Pseudo-labeling is a popular semi-supervised learning technique to leverage unlabeled data when labeled samples are scarce. The generation and selection of pseudo-labels heavily rely on labeled data. Existing approaches implicitly assume that the labeled data is gold standard and ``perfect''. However, this can be violated in reality with issues such as mislabeling or ambiguity. We address this overlooked aspect and show the importance of investigating labeled data quality to improve any pseudo-labeling method. Specifically, we introduce a novel data characterization and selection framework called DIPS to extend pseudo-labeling. We select useful labeled and pseudo-labeled samples via analysis of learning dynamics. We empirically demonstrate that DIPS improves the performance of various pseudo-labeling methods on real-world datasets across multiple modalities, including tabular and images, with minimal computational overhead. Additionally, DIPS improves data efficiency and reduces the performance distinctions between different pseudo-labelers. Overall, we highlight the significant benefits of a data-centric rethinking of pseudo-labeling in real-world settings.},
  langid = {english},
  language = {en}
}

@article{senTamingDataQuality2022,
  title = {Taming {{Data Quality}} in {{AI-Enabled Industrial Internet}} of {{Things}}},
  author = {Sen, Sagar and Husom, Erik Johannes and Goknil, Arda and Tverdal, Simeon and Nguyen, Phu and Mancisidor, Iker},
  year = {2022},
  month = nov,
  journal = {IEEE Software},
  volume = {39},
  number = {6},
  pages = {35--42},
  issn = {1937-4194},
  abstract = {We address the problem of taming data quality in artificial intelligence (AI)-enabled Industrial Internet of Things systems by devising machine learning pipelines as part of a decentralized edge-to-cloud architecture. We present the design and deployment of our approach from an AI engineering perspective using two industrial case studies.},
  keywords = {Cloud computing,Data integrity,Image edge detection,Industrial Internet of Things,Maintenance engineering,Soft sensors}
}

@inproceedings{shafahiPoisonFrogsTargeted2018,
  title = {Poison {{Frogs}}! {{Targeted Clean-Label Poisoning Attacks}} on {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shafahi, Ali and Huang, W. Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-25},
  abstract = {Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use ``clean-labels''; they don't require the attacker to have any control over the labeling of training data.  They are also targeted; they control the behavior of the classifier on a specific test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by putting them online and waiting for them to be scraped by a data collection bot.}
}

@article{shanthiniTaxonomyImpactLabel2019,
  title = {A Taxonomy on Impact of Label Noise and Feature Noise Using Machine Learning Techniques},
  author = {Shanthini, A. and Vinodhini, G. and Chandrasekaran, R. M. and Supraja, P.},
  year = {2019},
  month = sep,
  journal = {Soft Computing},
  volume = {23},
  number = {18},
  pages = {8597--8607},
  issn = {1433-7479},
  urldate = {2024-01-23},
  abstract = {Soft computing techniques are effective techniques that are used in prediction of noise in the dataset which causes misclassification. In classification, it is expected to have perfect labeling, but the noise present in data has impact on the label mapped and influences the input values by affecting the input feature values of the instances. Existence of noise complicates prediction in the real-world data which leads to vicious effect of the classifier. Present study aims at quantitative assessment of label noise and feature noise through machine learning, and classification performance in medical datasets as noise handling has become an important aspect in the research work related to data mining and its application. Weak classifier boosting provides high standard accuracy levels in classification problems. This study explores the performance of most recent soft computing technique in machine learning which includes weak learner-based boosting algorithms, such as adaptive boosting, generalized tree boosting and extreme gradient boosting. Current study was made to compare and analyze disparate boosting algorithms in divergent noise and feature levels (5\%, 10\%, 15\% and 20\%) on distinct medical datasets. The performances of weak learners are measured in terms of accuracy and equalized loss of accuracy.},
  langid = {english},
  language = {en},
  keywords = {Boosting,Feature noise,Label noise,Machine learning}
}

@article{shapleyMarketGames1969,
  title = {On Market Games},
  author = {Shapley, Lloyd S and Shubik, Martin},
  year = {1969},
  month = jun,
  journal = {Journal of Economic Theory},
  volume = {1},
  number = {1},
  pages = {9--25},
  issn = {0022-0531},
  urldate = {2024-03-30},
  abstract = {The ``market games''---games that derive from an exchange economy in which the traders have continuous concave monetary utility functions, are shown to be the same as the ``totally balanced games''---games which with all their subgames possess cores. (The core of a game is the set of out-comes that no coalition can profitably block.) The coincidence of these two classes of games is established with the aid of explicit transformations that generate a game from a market and vice versa. It is further shown that any game with a core has the same solutions, in the von Neumann-Morgenstern sense, as some totally balanced game. Thus, a market may be found that reproduces the solution behavior of any game that has a core. In particular, using a recent result of Lucas, a ten-trader tencommodity market is described that has no solution.}
}

@article{shayestehAutomatedConceptDrift2022,
  title = {Automated {{Concept Drift Handling}} for {{Fault Prediction}} in {{Edge Clouds Using Reinforcement Learning}}},
  author = {Shayesteh, Behshid and Fu, Chunyan and Ebrahimzadeh, Amin and Glitho, Roch H.},
  year = {2022},
  month = jun,
  journal = {IEEE Transactions on Network and Service Management},
  volume = {19},
  number = {2},
  pages = {1321--1335},
  issn = {1932-4537},
  urldate = {2023-11-27},
  abstract = {Fault management systems that use real-time analytics based on Machine Learning (ML) help provide the reliability required in edge clouds, though they suffer from frequent changes in data distribution (concept drift) caused by highly dynamic traffic in edge clouds, requiring frequent adaptations of the ML model. We propose an automated concept drift handling framework for fault prediction in edge clouds using Reinforcement Learning (RL) to select the most appropriate drift adaptation method as well as the amount of data needed for adaptation, while considering edge cloud operator's requirements. We implemented an edge cloud testbed and introduced infrastructure and network faults to it in the presence of abrupt and incremental concept drift. According to the obtained results, our proposed framework achieves up to 40\% higher accuracy compared to a system without drift handling, and up to 13{\textbackslash}times and 30{\textbackslash}times less regret for selecting adaptation methods and amount of data, respectively, compared to other approaches.}
}

@inproceedings{shejwalkarBackDrawingBoard2022,
  title = {Back to the {{Drawing Board}}: {{A Critical Evaluation}} of {{Poisoning Attacks}} on {{Production Federated Learning}}},
  shorttitle = {Back to the {{Drawing Board}}},
  booktitle = {2022 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Shejwalkar, Virat and Houmansadr, Amir and Kairouz, Peter and Ramage, Daniel},
  year = {2022},
  month = may,
  pages = {1354--1371},
  issn = {2375-1207},
  urldate = {2024-03-26},
  abstract = {While recent works have indicated that federated learning (FL) may be vulnerable to poisoning attacks by compromised clients, their real impact on production FL systems is not fully understood. In this work, we aim to develop a comprehensive systemization for poisoning attacks on FL by enumerating all possible threat models, variations of poisoning, and adversary capabilities. We specifically put our focus on un-targeted poisoning attacks, as we argue that they are significantly relevant to production FL deployments. We present a critical analysis of untargeted poisoning attacks under practical, production FL environments by carefully characterizing the set of realistic threat models and adversarial capabilities. Our findings are rather surprising: contrary to the established belief, we show that FL is highly robust in practice even when using simple, low-cost defenses. We go even further and propose novel, state-of-the-art data and model poisoning attacks, and show via an extensive set of experiments across three benchmark datasets how (in)effective poisoning attacks are in the presence of simple defense mechanisms. We aim to correct previous misconceptions and offer concrete guidelines to conduct more accurate (and more realistic) research on this topic.},
  keywords = {Benchmark testing,Collaborative work,Data models,poisoning-attacks,Privacy,Production,production-federated-learning,robust-aggregation-rules,Robustness,Security}
}

@inproceedings{shejwalkarManipulatingByzantineOptimizing2021,
  title = {Manipulating the {{Byzantine}}: {{Optimizing Model Poisoning Attacks}} and {{Defenses}} for {{Federated Learning}}},
  shorttitle = {Manipulating the {{Byzantine}}},
  booktitle = {Proceedings 2021 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Shejwalkar, Virat and Houmansadr, Amir},
  year = {2021},
  publisher = {Internet Society},
  address = {Virtual},
  urldate = {2024-02-24},
  abstract = {Federated learning (FL) enables many data owners (e.g., mobile devices) to train a joint ML model (e.g., a next-word prediction classifier) without the need of sharing their private training data. However, FL is known to be susceptible to model poisoning attacks by malicious participants (e.g., adversaryowned mobile devices), who aim at hampering the accuracy of the jointly trained model through sending malicious inputs during the federated training process. In this paper, we present a general framework for model poisoning attacks on FL. We show that our framework leads to poisoning attacks that substantially outperform the state-of-the-art model poisoning attacks by large margins. For instance, our attacks result in 1.5{\texttimes} to 60{\texttimes} more reductions in the accuracy of FL compared to the strongest of existing poisoning attacks.},
  isbn = {978-1-891562-66-2},
  langid = {english},
  language = {en}
}

@inproceedings{shiChallengesApproachesMitigating2022,
  title = {Challenges and {{Approaches}} for {{Mitigating Byzantine Attacks}} in {{Federated Learning}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Trust}}, {{Security}} and {{Privacy}} in {{Computing}} and {{Communications}} ({{TrustCom}})},
  author = {Shi, Junyu and Wan, Wei and Hu, Shengshan and Lu, Jianrong and Yu Zhang, Leo},
  year = {2022},
  month = dec,
  pages = {139--146},
  issn = {2324-9013},
  urldate = {2024-01-25},
  abstract = {Recently emerged federated learning (FL) is an attractive distributed learning framework in which numerous wireless end-user devices can train a global model with the data remained autochthonous. Compared with the traditional machine learning framework that collects user data for centralized storage, which brings huge communication burden and concerns about data privacy, this approach can not only save the network bandwidth but also protect the data privacy. Despite the promising prospect, Byzantine attack, an intractable threat in conventional distributed network, is discovered to be rather efficacious against FL as well. In this paper, we conduct a comprehensive investigation of the state-of-the-art strategies for defending against Byzantine attacks in FL. We first provide a taxonomy for the existing defense solutions according to the techniques they used, followed by an across-the-board comparison and discussion. Then we propose a new Byzantine attack method called weight attack to defeat those defense schemes, and conduct experiments to demonstrate its threat. The results show that existing defense solutions, although abundant, are still far from fully protecting FL. Finally, we indicate possible countermeasures for weight attack, and highlight several challenges and future research directions for mitigating Byzantine attacks in FL.},
  keywords = {Byzantine attack,Data privacy,Distance learning,distributed network,Federated learning,Privacy,security,Taxonomy,Training,Wireless communication}
}

@article{shiCommunicationEfficientEdgeAI2020,
  title = {Communication-{{Efficient Edge AI}}: {{Algorithms}} and {{Systems}}},
  shorttitle = {Communication-{{Efficient Edge AI}}},
  author = {Shi, Yuanming and Yang, Kai and Jiang, Tao and Zhang, Jun and Letaief, Khaled B.},
  year = {2020},
  journal = {IEEE Communications Surveys \& Tutorials},
  volume = {22},
  number = {4},
  pages = {2167--2191},
  issn = {1553-877X},
  abstract = {Artificial intelligence (AI) has achieved remarkable breakthroughs in a wide range of fields, ranging from speech processing, image classification to drug discovery. This is driven by the explosive growth of data, advances in machine learning (especially deep learning), and the easy access to powerful computing resources. Particularly, the wide scale deployment of edge devices (e.g., IoT devices) generates an unprecedented scale of data, which provides the opportunity to derive accurate models and develop various intelligent applications at the network edge. However, such enormous data cannot all be sent to the cloud for processing, due to the varying channel quality, traffic congestion and/or privacy concerns, and the enormous energy consumption. By pushing inference and training processes of AI models to edge nodes, edge AI has emerged as a promising alternative. AI at the edge requires close cooperation among edge devices, such as smart phones and smart vehicles, and edge servers at the wireless access points and base stations, which however result in heavy communication overheads. In this paper, we present a comprehensive survey of the recent developments in various techniques for overcoming these communication challenges. Specifically, we first identify key communication challenges in edge AI systems. We then introduce communication-efficient techniques, from both algorithmic and system perspectives for training and inference tasks at the network edge. Potential future research directions are also highlighted.},
  keywords = {Artificial intelligence,communication efficiency,Computational modeling,Data models,edge AI,edge intelligence,Neural networks,Servers,Task analysis,Training data,Wireless communication}
}

@misc{shiEfficientFederatedLearning2023,
  title = {Efficient {{Federated Learning}} with {{Enhanced Privacy}} via {{Lottery Ticket Pruning}} in {{Edge Computing}}},
  author = {Shi, Yifan and Wei, Kang and Shen, Li and Li, Jun and Wang, Xueqian and Yuan, Bo and Guo, Song},
  year = {2023},
  month = may,
  number = {arXiv:2305.01387},
  eprint = {2305.01387},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-11},
  abstract = {Federated learning (FL) is a collaborative learning paradigm for decentralized private data from mobile terminals (MTs). However, it suffers from issues in terms of communication, resource of MTs, and privacy. Existing privacy-preserving FL methods usually adopt the instance-level differential privacy (DP), which provides a rigorous privacy guarantee but with several bottlenecks: severe performance degradation, transmission overhead, and resource constraints of edge devices such as MTs. To overcome these drawbacks, we propose Fed-LTP, an efficient and privacy-enhanced FL framework with {\textbackslash}underline\{{\textbackslash}textbf\{L\}\}ottery {\textbackslash}underline\{{\textbackslash}textbf\{T\}\}icket {\textbackslash}underline\{{\textbackslash}textbf\{H\}\}ypothesis (LTH) and zero-concentrated D{\textbackslash}underline\{{\textbackslash}textbf\{P\}\} (zCDP). It generates a pruned global model on the server side and conducts sparse-to-sparse training from scratch with zCDP on the client side. On the server side, two pruning schemes are proposed: (i) the weight-based pruning (LTH) determines the pruned global model structure; (ii) the iterative pruning further shrinks the size of the pruned model's parameters. Meanwhile, the performance of Fed-LTP is also boosted via model validation based on the Laplace mechanism. On the client side, we use sparse-to-sparse training to solve the resource-constraints issue and provide tighter privacy analysis to reduce the privacy budget. We evaluate the effectiveness of Fed-LTP on several real-world datasets in both independent and identically distributed (IID) and non-IID settings. The results clearly confirm the superiority of Fed-LTP over state-of-the-art (SOTA) methods in communication, computation, and memory efficiencies while realizing a better utility-privacy trade-off.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning}
}

@article{shiFairnessAwareFederatedLearning2023,
  title = {Towards {{Fairness-Aware Federated Learning}}},
  author = {Shi, Yuxin and Yu, Han and Leung, Cyril},
  year = {2023},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--17},
  issn = {2162-2388},
  urldate = {2023-11-28},
  abstract = {Recent advances in federated learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL and overlook the interests of the FL clients. This may result in unfair treatment of clients, which discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse fairness-aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This article aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by the existing literature in this field, we propose a taxonomy of FAFL approaches covering major steps in FL, including client selection, optimization, contribution evaluation, and incentive distribution. In addition, we discuss the main metrics for experimentally evaluating the performance of FAFL approaches and suggest promising future research directions toward FAFL.}
}

@inproceedings{shokriMembershipInferenceAttacks2017,
  title = {Membership {{Inference Attacks Against Machine Learning Models}}},
  booktitle = {2017 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  year = {2017},
  month = may,
  pages = {3--18},
  issn = {2375-1207},
  urldate = {2024-02-24},
  abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
  keywords = {Data models,Google,Predictive models,Privacy,Sociology,Statistics,Training}
}

@inproceedings{sidiDataQualitySurvey2012,
  title = {Data Quality: {{A}} Survey of Data Quality Dimensions},
  shorttitle = {Data Quality},
  booktitle = {2012 {{International Conference}} on {{Information Retrieval}} \& {{Knowledge Management}}},
  author = {Sidi, Fatimah and Shariat Panahy, Payam Hassany and Affendey, Lilly Suriani and Jabar, Marzanah A. and Ibrahim, Hamidah and Mustapha, Aida},
  year = {2012},
  month = mar,
  pages = {300--304},
  urldate = {2023-12-01},
  abstract = {Nowadays, activities and decisions making in an organization is based on data and information obtained from data analysis, which provides various services for constructing reliable and accurate process. As data are significant resources in all organizations the quality of data is critical for managers and operating processes to identify related performance issues. Moreover, high quality data can increase opportunity for achieving top services in an organization. However, identifying various aspects of data quality from definition, dimensions, types, strategies, techniques are essential to equip methods and processes for improving data. This paper focuses on systematic review of data quality dimensions in order to use at proposed framework which combining data mining and statistical techniques to measure dependencies among dimensions and illustrate how extracting knowledge can increase process quality.},
  keywords = {Accuracy,Data mining,Data Quality,Data Quality Dimensions,Information systems,Organizations,Process control,Reliability,Types of Data}
}

@inproceedings{simsekGeometryLossLandscape2021,
  title = {Geometry of the {{Loss Landscape}} in {{Overparameterized Neural Networks}}: {{Symmetries}} and {{Invariances}}},
  shorttitle = {Geometry of the {{Loss Landscape}} in {{Overparameterized Neural Networks}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Simsek, Berfin and Ged, Fran{\c c}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Clement and Gerstner, Wulfram and Brea, Johanni},
  year = {2021},
  month = jul,
  pages = {9722--9732},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-13},
  abstract = {We study how permutation symmetries in overparameterized multi-layer neural networks generate `symmetry-induced' critical points. Assuming a network with LL L  layers of minimal widths r{$\ast$}1,{\dots},r{$\ast$}L-1r1{$\ast$},{\dots},rL-1{$\ast$} r\_1{\textasciicircum}*, {\textbackslash}ldots, r\_\{L-1\}{\textasciicircum}*  reaches a zero-loss minimum at r{$\ast$}1!{$\cdots$}r{$\ast$}L-1!r1{$\ast$}!{$\cdots$}rL-1{$\ast$}! r\_1{\textasciicircum}*! {\textbackslash}cdots r\_\{L-1\}{\textasciicircum}*!  isolated points that are permutations of one another, we show that adding one extra neuron to each layer is sufficient to connect all these previously discrete minima into a single manifold. For a two-layer overparameterized network of width r{$\ast$}+h=:mr{$\ast$}+h=:m r{\textasciicircum}*+ h =: m  we explicitly describe the manifold of global minima: it consists of T(r{$\ast$},m)T(r{$\ast$},m) T(r{\textasciicircum}*, m)  affine subspaces of dimension at least hh h  that are connected to one another. For a network of width mmm, we identify the number G(r,m)G(r,m)G(r,m) of affine subspaces containing only symmetry-induced critical points that are related to the critical points of a smaller network of width \$r},
  langid = {english},
  language = {en}
}

@article{singhEdgeAISurvey2023,
  title = {Edge {{AI}}: {{A}} Survey},
  shorttitle = {Edge {{AI}}},
  author = {Singh, Raghubir and Gill, Sukhpal Singh},
  year = {2023},
  journal = {Internet of Things and Cyber-Physical Systems},
  volume = {3},
  pages = {71--92},
  issn = {26673452},
  urldate = {2023-07-20},
  abstract = {Artificial Intelligence (AI) at the edge is the utilization of AI in real-world devices. Edge AI refers to the practice of doing AI computations near the users at the network's edge, instead of centralised location like a cloud service provider's data centre. With the latest innovations in AI efficiency, the proliferation of Internet of Things (IoT) devices, and the rise of edge computing, the potential of edge AI has now been unlocked. This study provides a thorough analysis of AI approaches and capabilities as they pertain to edge computing, or Edge AI. Further, a detailed survey of edge computing and its paradigms including transition to Edge AI is presented to explore the background of each variant proposed for implementing Edge Computing. Furthermore, we discussed the Edge AI approach to deploying AI algorithms and models on edge devices, which are typically resource-constrained devices located at the edge of the network. We also presented the technology used in various modern IoT applications, including autonomous vehicles, smart homes, industrial automation, healthcare, and surveillance. Moreover, the discussion of leveraging machine learning algorithms optimized for resource-constrained environments is presented. Finally, important open challenges and potential research directions in the field of edge computing and edge AI have been identified and investigated. We hope that this article will serve as a common goal for a future blueprint that will unite important stakeholders and facilitates to accelerate development in the field of Edge AI.},
  langid = {english},
  language = {english}
}

@article{slowikRelationDistributionallyRobust2022,
  title = {On the {{Relation}} between {{Distributionally Robust Optimization}} and {{Data Curation}} ({{Student Abstract}})},
  author = {S{\l}owik, Agnieszka and Bottou, L{\'e}on and Holden, Sean B. and Jamnik, Mateja},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {11},
  pages = {13053--13054},
  issn = {2374-3468},
  urldate = {2023-12-03},
  abstract = {Machine learning systems based on minimizing average error have been shown to perform inconsistently across notable subsets of the data, which is not exposed by a low average error for the entire dataset. In consequential social and economic applications, where data represent people, this can lead to discrimination of underrepresented gender and ethnic groups. Distributionally Robust Optimization (DRO) seemingly addresses this problem by minimizing the worst expected risk across subpopulations. We establish theoretical results that clarify the relation between DRO and the optimization of the same loss averaged on an adequately weighted training dataset. A practical implication of our results is that neither DRO nor curating the training set should be construed as a complete solution for bias mitigation.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {Adversarial Learning}
}

@inproceedings{smithFederatedMultiTaskLearning2017,
  title = {Federated {{Multi-Task Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-17},
  abstract = {Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.}
}

@book{sole-casalsMachineLearningMethods2021,
  title = {Machine {{Learning Methods}} with {{Noisy}}, {{Incomplete}} or {{Small Datasets}}},
  year = {2021},
  publisher = {MDPI - Multidisciplinary Digital Publishing Institute},
  address = {Basel, Switzerland},
  abstract = {In many machine learning applications, available datasets are sometimes incomplete, noisy or affected by artifacts. In supervised scenarios, it could happen that label information has low quality, which might include unbalanced training sets, noisy labels and other problems. Moreover, in practice, it is very common that available data samples are not enough to derive useful supervised or unsupervised classifiers. All these issues are commonly referred to as the low-quality data problem. This book collects novel contributions on machine learning methods for low-quality datasets, to contribute to the dissemination of new ideas to solve this challenging problem, and to provide clear examples of application in real scenarios},
  collaborator = {{Sol{\'e}-Casals}, Jordi and Sun, Zhe and Caiafa, Cesar F. and {Marti-Puig}, Pere and Tanaka, Toshihisa},
  isbn = {978-3-0365-1288-4},
  langid = {english},
  language = {eng},
  annotation = {OCLC: 1302684138}
}

@article{soltanolkotabiTheoreticalInsightsOptimization2019,
  title = {Theoretical {{Insights Into}} the {{Optimization Landscape}} of {{Over-Parameterized Shallow Neural Networks}}},
  author = {Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D.},
  year = {2019},
  month = feb,
  journal = {IEEE Transactions on Information Theory},
  volume = {65},
  number = {2},
  pages = {742--769},
  issn = {1557-9654},
  urldate = {2024-08-31},
  abstract = {In this paper, we study the problem of learning a shallow artificial neural network that best fits a training data set. We study this problem in the over-parameterized regime where the numbers of observations are fewer than the number of parameters in the model. We show that with the quadratic activations, the optimization landscape of training, such shallow neural networks, has certain favorable characteristics that allow globally optimal models to be found efficiently using a variety of local search heuristics. This result holds for an arbitrary training data of input/output pairs. For differentiable activation functions, we also show that gradient descent, when suitably initialized, converges at a linear rate to a globally optimal model. This result focuses on a realizable model where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted weight coefficients.},
  keywords = {Biological neural networks,Convergence,Data models,Nonconvex optimization,Numerical models,Optimization,over-parametrized neural networks,random matrix theory,Training}
}

@article{songLearningNoisyLabels2023,
  title = {Learning {{From Noisy Labels With Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Learning {{From Noisy Labels With Deep Neural Networks}}},
  author = {Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  year = {2023},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {11},
  pages = {8135--8153},
  issn = {2162-2388},
  urldate = {2024-01-10},
  abstract = {Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.}
}

@misc{sontagRemarkPolynomialsSpecified2013,
  title = {A Remark about Polynomials with Specified Local Minima and No Other Critical Points},
  author = {Sontag, Eduardo D.},
  year = {2013},
  month = feb,
  number = {arXiv:1302.0759},
  eprint = {1302.0759},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2024-09-20},
  abstract = {The following observation must surely be "well-known", but it seems worth giving a simple and quite explicit proof. Take any finite subset X of Rn, n{$>$}1. Then, there is a polynomial function P:Rn -{$>$} R which has local minima on the set X, and has no other critical points. Applied to the negative gradient flow of P, this implies that there is a polynomial vector field with asymptotically stable equilibria on X and no other equilibria. Some trajectories of this vector field are not pre-compact; a complementary observation says that, again for arbitrary X, one can find a vector field with asymptotically stable equilibria on X, no other equilibria except saddles, and all omega-limit sets consisting of singletons.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Dynamical Systems}
}

@misc{stanfordcsaffiliatesTatsunoriHashimotoAverage2023,
  title = {Tatsunori {{Hashimoto}}, {{Beyond}} the {{Average Case Machine Learning}} for {{Atypical Examples}}},
  author = {{Stanford CS Affiliates}},
  year = {2023},
  month = nov,
  urldate = {2023-12-17},
  abstract = {Tatsunori (Tatsu) Hashimoto is an Assistant Professor in the Computer Science department at Stanford. Prior to this, he was a post-doc for Professors Percy Liang and John Duchi at Stanford. He holds a Ph.D from MIT where he studied random walks and computational biology under Professors Tommi Jaakkola and David Gifford, and a B.S. from Harvard in Statistics and Math. His work has been recognized in NeurIPS 2018 (Oral), ICML 2018 (Best paper runner-up), and NeurIPS 2014 Workshop on Networks (Best student paper).}
}

@article{sternbergBiomedicalImageProcessing1983,
  title = {Biomedical {{Image Processing}}},
  author = {{Sternberg}},
  year = {1983},
  month = jan,
  journal = {Computer},
  volume = {16},
  number = {1},
  pages = {22--34},
  issn = {1558-0814},
  urldate = {2024-09-09},
  keywords = {Biomedical image processing,Biomedical imaging,Electrokinetics,Genetic mutations,Humans,Image analysis,Image processing,Medical diagnostic imaging,Pediatrics,Proteins}
}

@article{sunDecentralizedFederatedAveraging2023,
  title = {Decentralized {{Federated Averaging}}},
  author = {Sun, Tao and Li, Dongsheng and Wang, Bao},
  year = {2023},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {4},
  pages = {4289--4301},
  issn = {1939-3539},
  urldate = {2024-04-20},
  abstract = {Federated averaging (FedAvg) is a communication-efficient algorithm for distributed training with an enormous number of clients. In FedAvg, clients keep their data locally for privacy protection; a central parameter server is used to communicate between clients. This central server distributes the parameters to each client and collects the updated parameters from clients. FedAvg is mostly studied in centralized fashions, requiring massive communications between the central server and clients, which leads to possible channel blocking. Moreover, attacking the central server can break the whole system's privacy. Indeed, decentralization can significantly reduce the communication of the busiest node (the central one) because all nodes only communicate with their neighbors. To this end, in this paper, we study the decentralized FedAvg with momentum (DFedAvgM), implemented on clients that are connected by an undirected graph. In DFedAvgM, all clients perform stochastic gradient descent with momentum and communicate with their neighbors only. To further reduce the communication cost, we also consider the quantized DFedAvgM. The proposed algorithm involves the mixing matrix, momentum, client training with multiple local iterations, and quantization, introducing extra items in the Lyapunov analysis. Thus, the analysis of this paper is much more challenging than previous decentralized (momentum) SGD or FedAvg. We prove convergence of the (quantized) DFedAvgM under trivial assumptions; the convergence rate can be improved to sublinear when the loss function satisfies the P{\L} property. Numerically, we find that the proposed algorithm outperforms FedAvg in both convergence speed and communication cost.},
  keywords = {Collaborative work,Convergence,Costs,Decentralized optimization,federated averaging,momentum,Peer-to-peer computing,Privacy,Servers,stochastic gradient descent,Training}
}

@misc{sunOptimizationDeepLearning2019,
  title = {Optimization for Deep Learning: Theory and Algorithms},
  shorttitle = {Optimization for Deep Learning},
  author = {Sun, Ruoyu},
  year = {2019},
  month = dec,
  number = {arXiv:1912.08957},
  eprint = {1912.08957},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-06-29},
  abstract = {When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and existing theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinitewidth analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@inproceedings{sureshFrameworkUnderstandingSources2021,
  title = {A {{Framework}} for {{Understanding Sources}} of {{Harm}} throughout the {{Machine Learning Life Cycle}}},
  booktitle = {Proceedings of the 1st {{ACM Conference}} on {{Equity}} and {{Access}} in {{Algorithms}}, {{Mechanisms}}, and {{Optimization}}},
  author = {Suresh, Harini and Guttag, John},
  year = {2021},
  month = nov,
  series = {{{EAAMO}} '21},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-12-25},
  abstract = {As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
  isbn = {978-1-4503-8553-4},
  keywords = {AI ethics,algorithmic bias,allocative harm,fairness in machine learning,representational harm,societal implications of machine learning}
}

@inproceedings{suRobustMultiagentOptimization2016,
  title = {Robust {{Multi-agent Optimization}}: {{Coping}} with {{Byzantine Agents}} with {{Input Redundancy}}},
  shorttitle = {Robust {{Multi-agent Optimization}}},
  booktitle = {Stabilization, {{Safety}}, and {{Security}} of {{Distributed Systems}}},
  author = {Su, Lili and Vaidya, Nitin H.},
  editor = {Bonakdarpour, Borzoo and Petit, Franck},
  year = {2016},
  pages = {368--382},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {This paper addresses the multi-agent optimization problem in which the agents try to collaboratively minimize \$\${\textbackslash}frac\{1\}\{k\}{\textbackslash}sum \_\{i=1\}{\textasciicircum}k h\_i\$\$for a given choice of k input functions \$\$h\_1, {\textbackslash}ldots , h\_k\$\$. This problem finds its application in distributed machine learning, where the data set is too large to be processed and stored by a single machine. It has been shown that when the networked agents may suffer Byzantine faults, it is impossible to minimize \$\${\textbackslash}frac\{1\}\{k\}{\textbackslash}sum \_\{i=1\}{\textasciicircum}k h\_i\$\$with no redundancy in the local cost functions.},
  isbn = {978-3-319-49259-9},
  langid = {english},
  language = {en},
  keywords = {Byzantine faults,Coding theory,Distributed optimization,Multi-agent network,Redundancy,Security}
}

@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year = {2013},
  month = may,
  pages = {1139--1147},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-09-25},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
  langid = {english},
  language = {en}
}

@inproceedings{tahmasebiUniversalClassSharpnessAware2024,
  title = {A {{Universal Class}} of {{Sharpness-Aware Minimization Algorithms}}},
  booktitle = {High-Dimensional {{Learning Dynamics}} 2024: {{The Emergence}} of {{Structure}} and {{Reasoning}}},
  author = {Tahmasebi, Behrooz and Soleymani, Ashkan and Bahri, Dara and Jegelka, Stefanie and Jaillet, Patrick},
  year = {2024},
  month = jun,
  urldate = {2024-08-14},
  abstract = {Recently, there has been a surge in interest in developing optimization algorithms for overparameterized models as achieving generalization is believed to require algorithms with suitable biases. This interest centers on minimizing sharpness of the original loss function; the Sharpness-Aware Minimization (SAM) algorithm has proven effective. However, existing literature focuses on only a few sharpness measures (such as the maximum eigenvalue/trace of the training loss Hessian), which may not necessarily yield meaningful insights for non-convex optimization scenarios (e.g., neural networks). Moreover, many sharpness measures show sensitivity to parameter invariances in neural networks, e.g., they magnify significantly under rescaling parameters. Hence, here we introduce a new class of sharpness measures leading to sharpness-aware objective functions. We prove that these measures are universally expressive, allowing any function of the training loss Hessian matrix to be represented by choosing appropriate hyperparameters. Furthermore, we show that the proposed objective functions explicitly bias towards minimizing their corresponding sharpness measures. Finally, as an example of our proposed general framework, we present Frob-SAM and Det-SAM, which are specifically designed to minimize the Frobenius norm and the determinant of the Hessian of the training loss, respectively. We also demonstrate the advantages of our general framework through an extensive series of experiments.},
  langid = {english},
  language = {en}
}

@inproceedings{taikDataQualityBasedScheduling2021,
  title = {Data-{{Quality Based Scheduling}} for {{Federated Edge Learning}}},
  booktitle = {2021 {{IEEE}} 46th {{Conference}} on {{Local Computer Networks}} ({{LCN}})},
  author = {Taik, Afaf and Moudoud, Hajar and Cherkaoui, Soumaya},
  year = {2021},
  month = oct,
  eprint = {2201.11247},
  primaryclass = {cs},
  pages = {17--23},
  urldate = {2023-08-01},
  abstract = {FEderated Edge Learning (FEEL) has emerged as a leading technique for privacy-preserving distributed training in wireless edge networks, where edge devices collaboratively train machine learning (ML) models with the orchestration of a server. However, due to frequent communication, FEEL needs to be adapted to the limited communication bandwidth. Furthermore, the statistical heterogeneity of local datasets' distributions, and the uncertainty about the data quality pose important challenges to the training's convergence. Therefore, a meticulous selection of the participating devices and an analogous bandwidth allocation are necessary. In this paper, we propose a data-quality based scheduling (DQS) algorithm for FEEL. DQS prioritizes reliable devices with rich and diverse datasets. In this paper, we define the different components of the learning algorithm and the data-quality evaluation. Then, we formulate the device selection and the bandwidth allocation problem. Finally, we present our DQS algorithm for FEEL, and we evaluate it in different data poisoning scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing}
}

@article{tamFederatedNoisyClient2023,
  title = {Federated {{Noisy Client Learning}}},
  author = {Tam, Kahou and Li, Li and Han, Bo and Xu, Chengzhong and Fu, Huazhu},
  year = {2023},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--14},
  issn = {2162-2388},
  urldate = {2024-04-12},
  abstract = {Federated learning (FL) collaboratively trains a shared global model depending on multiple local clients, while keeping the training data decentralized to preserve data privacy. However, standard FL methods ignore the noisy client issue, which may harm the overall performance of the shared model. We first investigate the critical issue caused by noisy clients in FL and quantify the negative impact of the noisy clients in terms of the representations learned by different layers. We have the following two key observations: 1) the noisy clients can severely impact the convergence and performance of the global model in FL and 2) the noisy clients can induce greater bias in the deeper layers than the former layers of the global model. Based on the above observations, we propose federated noisy client learning (Fed-NCL), a framework that conducts robust FL with noisy clients. Specifically, Fed-NCL first identifies the noisy clients through well estimating the data quality and model divergence. Then robust layerwise aggregation is proposed to adaptively aggregate the local models of each client to deal with the data heterogeneity caused by the noisy clients. We further perform label correction on the noisy clients to improve the generalization of the global model. Experimental results on various datasets demonstrate that our algorithm boosts the performances of different state-of-the-art systems with noisy clients. Our code is available at https://github.com/TKH666/Fed-NCL.},
  keywords = {Adaptation models,Computational modeling,Data models,Federated learning (FL),label noise,Noise measurement,noisy client,noisy learning,Servers,Training,Training data}
}

@inproceedings{tanakaNoetherLearningDynamics2021,
  title = {Noether's {{Learning Dynamics}}: {{Role}} of {{Symmetry Breaking}} in {{Neural Networks}}},
  shorttitle = {Noether's {{Learning Dynamics}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tanaka, Hidenori and Kunin, Daniel},
  year = {2021},
  month = nov,
  urldate = {2024-07-16},
  abstract = {In nature, symmetry governs regularities, while symmetry breaking brings texture. In artificial neural networks, symmetry has been a central design principle to efficiently capture regularities in the world, but the role of symmetry breaking is not well understood. Here, we develop a theoretical framework to study the "geometry of learning dynamics" in neural networks, and reveal a key mechanism of explicit symmetry breaking behind the efficiency and stability of modern neural networks. To build this understanding, we model the discrete learning dynamics of gradient descent using a continuous-time Lagrangian formulation, in which the learning rule corresponds to the kinetic energy and the loss function corresponds to the potential energy. Then, we identify "kinetic symmetry breaking" (KSB), the condition when the kinetic energy explicitly breaks the symmetry of the potential function. We generalize Noether's theorem known in physics to take into account KSB and derive the resulting motion of the Noether charge: "Noether's Learning Dynamics" (NLD). Finally, we apply NLD to neural networks with normalization layers and reveal how KSB introduces a mechanism of implicit adaptive optimization, establishing an analogy between learning dynamics induced by normalization layers and RMSProp. Overall, through the lens of Lagrangian mechanics, we have established a theoretical foundation to discover geometric design principles for the learning dynamics of neural networks.},
  langid = {english},
  language = {en}
}

@inproceedings{tarmounUnderstandingDynamicsGradient2021,
  title = {Understanding the {{Dynamics}} of {{Gradient Flow}} in {{Overparameterized Linear}} Models},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Tarmoun, Salma and Franca, Guilherme and Haeffele, Benjamin D. and Vidal, Rene},
  year = {2021},
  month = jul,
  pages = {10153--10161},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-19},
  abstract = {We provide a detailed analysis of the dynamics ofthe gradient flow in overparameterized two-layerlinear models. A particularly interesting featureof this model is that its nonlinear dynamics can beexactly solved as a consequence of a large num-ber of conservation laws that constrain the systemto follow particular trajectories. More precisely,the gradient flow preserves the difference of theGramian matrices of the input and output weights,and its convergence to equilibrium depends onboth the magnitude of that difference (which isfixed at initialization) and the spectrum of the data.In addition, and generalizing prior work, we proveour results without assuming small, balanced orspectral initialization for the weights. Moreover,we establish interesting mathematical connectionsbetween matrix factorization problems and differ-ential equations of the Riccati type.},
  langid = {english},
  language = {en}
}

@article{tianAsynchronousFederatedLearning2021,
  title = {Towards Asynchronous Federated Learning Based Threat Detection: {{A DC-Adam}} Approach},
  shorttitle = {Towards Asynchronous Federated Learning Based Threat Detection},
  author = {Tian, Pu and Chen, Zheyi and Yu, Wei and Liao, Weixian},
  year = {2021},
  month = sep,
  journal = {Computers \& Security},
  volume = {108},
  pages = {102344},
  issn = {0167-4048},
  urldate = {2024-04-17},
  abstract = {The increasing popularity and widespread use of Internet of Things (IoT) and Cyber-Physical Systems (CPS) technologies have produced a significant need for the integration of cloud and edge computing with distributed detection solutions to handle the growing volume of distributed security threats. While deep learning-based approaches have been used to detect anomalous behaviors in complex data patterns, the heterogeneity in IoT networks still poses paramount challenges to update synchronization across learning nodes in distributed training. Particularly, the non-independent and identically distributed (non-IID) data patterns over remote nodes significantly affect the performance of model training provisioned on cloud and edge computing servers, and most existing works have assumed a homogeneous setting. The heterogeneity brings the gradient delay problem, causing the gradient inconsistency in the barrier-free asynchronous mode. In this paper, we propose a Delay Compensated Adam (DC-Adam) approach, an asynchronous federated learning-based detection approach, for IoT devices with limited resources. To overcome the notorious gradient delay problem, we develop a Taylor Expansion-based scheme to compensate for the inconsistency caused by asynchronous communication. Moreover, a pre-shared data training strategy for non-IID data is developed to avoid the convergence divergence under the non-IID patterns. After the collaborative model training procedure, we append an additional local training process at each client to fit respective patterns. Via a combination of theoretical analysis of convergence and practical experimental results, we validate the efficacy of our proposed approach compared to the other state-of-the-art approaches. Compared with benchmark approaches, we demonstrate that our proposed method can converge stably, and that it outperforms the barrier-free asynchronous federated learning by 12.8\% (accuracy), 14\% (precision). 8.71\% (recall), and 11.16\% (F1 score) on average.},
  keywords = {Cloud computing security,Distributed threat detection,Edge computing security,Federated learning,Non-IID data}
}

@inproceedings{tolpeginDataPoisoningAttacks2020,
  title = {Data {{Poisoning Attacks Against Federated Learning Systems}}},
  booktitle = {Computer {{Security}} -- {{ESORICS}} 2020},
  author = {Tolpegin, Vale and Truex, Stacey and Gursoy, Mehmet Emre and Liu, Ling},
  editor = {Chen, Liqun and Li, Ninghui and Liang, Kaitai and Schneider, Steve},
  year = {2020},
  pages = {480--501},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {Federated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants' data remains on their own devices with only model updates being shared with a central server. However, the distributed nature of FL gives rise to new threats caused by potentially malicious participants. In this paper, we study targeted data poisoning attacks against FL systems in which a malicious subset of the participants aim to poison the global model by sending model updates derived from mislabeled data. We first demonstrate that such data poisoning attacks can cause substantial drops in classification accuracy and recall, even with a small percentage of malicious participants. We additionally show that the attacks can be targeted, i.e., they have a large negative impact only on classes that are under attack. We also study attack longevity in early/late round training, the impact of malicious participant availability, and the relationships between the two. Finally, we propose a defense strategy that can help identify malicious participants in FL to circumvent poisoning attacks, and demonstrate its effectiveness.},
  isbn = {978-3-030-58951-6},
  langid = {english},
  language = {en},
  keywords = {Adversarial machine learning,Data poisoning,Deep learning,Federated learning,Label flipping}
}

@article{tragerPURESPURIOUSCRITICAL2020,
  title = {{{PURE AND SPURIOUS CRITICAL POINTS}}: {{A GEOMETRIC STUDY OF LINEAR NETWORKS}}},
  author = {Trager, Matthew and Kohn, Kathlen and Bruna, Joan},
  year = {2020},
  abstract = {The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network's weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. Our analysis clearly illustrates that the absence of ``bad'' local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (``filling architectures'') but it holds only for the quadratic loss when the functional space is a determinantal variety (``non-filling architectures''). Without any assumption on the architecture, smooth convex losses may lead to landscapes with many bad minima.},
  langid = {english},
  language = {en}
}

@inproceedings{truexHybridApproachPrivacyPreserving2019,
  title = {A {{Hybrid Approach}} to {{Privacy-Preserving Federated Learning}}},
  booktitle = {Proceedings of the 12th {{ACM Workshop}} on {{Artificial Intelligence}} and {{Security}}},
  author = {Truex, Stacey and Baracaldo, Nathalie and Anwar, Ali and Steinke, Thomas and Ludwig, Heiko and Zhang, Rui and Zhou, Yi},
  year = {2019},
  month = nov,
  series = {{{AISec}}'19},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-03-18},
  abstract = {Federated learning facilitates the collaborative training of models without the sharing of raw data. However, recent attacks demonstrate that simply maintaining data locality during training processes does not provide sufficient privacy guarantees. Rather, we need a federated learning system capable of preventing inference over both the messages exchanged during training and the final trained model while ensuring the resulting model also has acceptable predictive accuracy. Existing federated learning approaches either use secure multiparty computation (SMC) which is vulnerable to inference or differential privacy which can lead to low accuracy given a large number of parties with relatively small amounts of data each. In this paper, we present an alternative approach that utilizes both differential privacy and SMC to balance these trade-offs. Combining differential privacy with secure multiparty computation enables us to reduce the growth of noise injection as the number of parties increases without sacrificing privacy while maintaining a pre-defined rate of trust. Our system is therefore a scalable approach that protects against inference threats and produces models with high accuracy. Additionally, our system can be used to train a variety of machine learning models, which we validate with experimental results on 3 different machine learning algorithms. Our experiments demonstrate that our approach out-performs state of the art solutions.},
  isbn = {978-1-4503-6833-9},
  keywords = {differential privacy,federated learning,privacy,privacy-preserving machine learning,secure multiparty computation}
}

@article{truongPrivacyPreservationFederated2021,
  title = {Privacy Preservation in Federated Learning: {{An}} Insightful Survey from the {{GDPR}} Perspective},
  shorttitle = {Privacy Preservation in Federated Learning},
  author = {Truong, Nguyen and Sun, Kai and Wang, Siyao and Guitton, Florian and Guo, YiKe},
  year = {2021},
  month = nov,
  journal = {Computers \& Security},
  volume = {110},
  pages = {102402},
  issn = {0167-4048},
  urldate = {2024-02-11},
  abstract = {In recent years, along with the blooming of Machine Learning (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML-based service providers not only confront with difficulties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Furthermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning without disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not sufficient for privacy-guarantee because model parameters exchanged among participants conceal sensitive information that can be exploited in privacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing challenges are examined along with the prospective approaches following the GDPR regulatory guidelines that FL-based systems shall implement to fully comply with the GDPR.},
  keywords = {Data protection regulation,Federated learning,GDPR,Personal data,Privacy,Privacy preservation}
}

@inproceedings{tsaiFormalizingGeneralizationAdversarial2021,
  title = {Formalizing {{Generalization}} and {{Adversarial Robustness}} of {{Neural Networks}} to {{Weight Perturbations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tsai, Yu-Lin and Hsu, Chia-Yi and Yu, Chia-Mu and Chen, Pin-Yu},
  year = {2021},
  volume = {34},
  pages = {19692--19704},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-30},
  abstract = {Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.}
}

@article{tsouvalasLabelingChaosLearning2024,
  title = {Labeling {{Chaos}} to {{Learning Harmony}}: {{Federated Learning}} with {{Noisy Labels}}},
  shorttitle = {Labeling {{Chaos}} to {{Learning Harmony}}},
  author = {Tsouvalas, Vasileios and Saeed, Aaqib and Ozcelebi, Tanir and Meratnia, Nirvana},
  year = {2024},
  month = feb,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {15},
  number = {2},
  pages = {22:1--22:26},
  issn = {2157-6904},
  urldate = {2024-04-12},
  abstract = {Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized private datasets where the labeling effort is entrusted to the clients. While most existing FL approaches assume high-quality labels are readily available on users' devices, in reality, label noise can naturally occur in FL and is closely related to clients' characteristics. Due to scarcity of available data and significant label noise variations among clients in FL, existing state-of-the-art centralized approaches exhibit unsatisfactory performance, whereas prior FL studies rely on excessive on-device computational schemes or additional clean data available on the server. We propose FedLN, a framework to deal with label noise across different FL training stages, namely FL initialization, on-device model training, and server model aggregation, able to accommodate the diverse computational capabilities of devices in an FL system. Specifically, FedLN computes per-client noise level estimation in a single federated round and improves the models' performance by either correcting or mitigating the effect of noisy samples. Our evaluation on various publicly available vision and audio datasets demonstrates a 22\% improvement on average compared to other existing methods for a label noise level of 60\%. We further validate the efficiency of FedLN in human-annotated real-world noisy datasets and report a 4.8\% increase on average in models' recognition performance, highlighting that FedLN can be useful for improving FL services provided to everyday users.},
  keywords = {deep learning,Federated learning,knowledge distillation,label correction,noisy labels}
}

@inproceedings{tuorOvercomingNoisyIrrelevant2021,
  title = {Overcoming {{Noisy}} and {{Irrelevant Data}} in {{Federated Learning}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Tuor, Tiffany and Wang, Shiqiang and Ko, Bong Jun and Liu, Changchang and Leung, Kin K.},
  year = {2021},
  month = jan,
  pages = {5020--5027},
  issn = {1051-4651},
  urldate = {2023-12-19},
  abstract = {Many image and vision applications require a large amount of data for model training. Collecting all such data at a central location can be challenging due to data privacy and communication bandwidth restrictions. Federated learning is an effective way of training a machine learning model in a distributed manner from local data collected by client devices, which does not require exchanging the raw data among clients. A challenge is that among the large variety of data collected at each client, it is likely that only a subset is relevant for a learning task while the rest of data has a negative impact on model training. Therefore, before starting the learning process, it is important to select the subset of data that is relevant to the given federated learning task. In this paper, we propose a method for distributedly selecting relevant data, where we use a benchmark model trained on a small benchmark dataset that is task-specific, to evaluate the relevance of individual data samples at each client and select the data with sufficiently high relevance. Then, each client only uses the selected subset of its data in the federated learning process. The effectiveness of our proposed approach is evaluated on multiple real-world image datasets in a simulated system with a large number of clients, showing up to 25\% improvement in model accuracy compared to training with all data.}
}

@misc{ujvaryRethinkingSharpnessAwareMinimization2022,
  title = {Rethinking {{Sharpness-Aware Minimization}} as {{Variational Inference}}},
  author = {Ujv{\'a}ry, Szilvia and Telek, Zsigmond and Kerekes, Anna and M{\'e}sz{\'a}ros, Anna and Husz{\'a}r, Ferenc},
  year = {2022},
  month = oct,
  urldate = {2024-05-12},
  abstract = {Sharpness-aware minimization (SAM) aims to improve the generalisation of gradient-based learning by seeking out flat minima. In this work, we establish connections between SAM and Mean-Field Variational Inference (MFVI) of neural network parameters. We show that both these methods have interpretations as optimizing notions of flatness, and when using the reparametrisation trick, they both boil down to calculating the gradient at a perturbed version of the current mean parameter. This thinking motivates our study of algorithms that combine or interpolate between SAM and MFVI. We evaluate the proposed variational algorithms on several benchmark datasets, and compare their performance to variants of SAM. Taking a broader perspective, our work suggests that SAM-like updates can be used as a drop-in replacement for the reparametrisation trick.},
  langid = {english},
  language = {en}
}

@article{UnderstandingRoleLayer2023,
  title = {Understanding the {{Role}} of {{Layer Normalization}} in {{Label-Skewed Federated Learning}}},
  year = {2023},
  month = oct,
  journal = {Transactions on Machine Learning Research},
  urldate = {2023-12-16},
  abstract = {Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in FL. Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes.},
  langid = {english},
  language = {en}
}

@book{vanleeuwenAristotelianMechanicsText2016,
  title = {The {{Aristotelian Mechanics}}: Text and Diagrams},
  shorttitle = {The {{Aristotelian Mechanics}}},
  author = {Van Leeuwen, Joyce},
  year = {2016},
  series = {Boston Studies in the Philosophy and History of Science},
  edition = {1. Auflage},
  number = {316},
  publisher = {Springer},
  address = {Cham Heidelberg New York Dordrecht London},
  isbn = {978-3-319-25925-3 978-3-319-25923-9},
  langid = {english},
  language = {eng}
}

@book{vapnikNatureStatisticalLearning2010,
  title = {The Nature of Statistical Learning Theory},
  author = {Vapnik, Vladimir Naumovich},
  year = {2010},
  series = {Statistics for Engineering and Information Science},
  edition = {Second edition},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-1-4419-3160-3},
  langid = {english},
  language = {eng}
}

@inproceedings{vapnikPrinciplesRiskMinimization1991,
  title = {Principles of {{Risk Minimization}} for {{Learning Theory}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vapnik, V.},
  year = {1991},
  volume = {4},
  pages = {831--838},
  publisher = {Morgan-Kaufmann},
  urldate = {2023-11-29},
  abstract = {Learning is posed as a problem of function estimation, for which two princi(cid:173) ples of solution are considered: empirical risk minimization and structural  risk minimization. These two principles are applied to two different state(cid:173) ments of the function estimation problem: global and local. Systematic  improvements in prediction power are illustrated in application to zip-code  recognition.}
}

@book{vapnikStatisticalLearningTheory1998,
  title = {Statistical Learning Theory},
  author = {Vapnik, Vladimir Naumovich},
  year = {1998},
  series = {Adaptive and Learning Systems for Signal Processing, Communications, and Control},
  publisher = {Wiley},
  address = {New York},
  isbn = {978-0-471-03003-4},
  lccn = {Q325.7 .V38 1998},
  keywords = {Computational learning theory}
}

@inproceedings{vermaFairnessDefinitionsExplained2018,
  title = {Fairness Definitions Explained},
  booktitle = {Proceedings of the {{International Workshop}} on {{Software Fairness}}},
  author = {Verma, Sahil and Rubin, Julia},
  year = {2018},
  month = may,
  pages = {1--7},
  publisher = {ACM},
  address = {Gothenburg Sweden},
  urldate = {2024-01-04},
  abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
  isbn = {978-1-4503-5746-3},
  langid = {english},
  language = {en}
}

@incollection{vidalOptimizationLandscapeNeural2022,
  title = {Optimization {{Landscape}} of {{Neural Networks}}},
  booktitle = {Mathematical {{Aspects}} of {{Deep Learning}}},
  author = {Vidal, Ren{\'e} and Zhu, Zhihui and Haeffele, Benjamin D.},
  editor = {Kutyniok, Gitta and Grohs, Philipp},
  year = {2022},
  pages = {200--228},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  urldate = {2024-06-21},
  abstract = {This chapter summarizes recent advances on the analysis of the optimization landscape of neural network training. We first review classical results for linear networks trained with a squared loss and without regularization. Such results show that under certain conditions on the input-output data spurious local minima are guaranteed not to exist, i.e. critical points are either saddle points or global minima. Moreover, the globally optimal weights can be found by factorizing certain matrices obtained from the input-output covariance matrices.We then review recent results for deep networks with parallel structure, positively homogeneous network mapping and regularization, and trained with a convex loss. Such results show that the non-convex objective on theweights can be lower-bounded by a convex objective on the network mapping. Moreover, when the network is sufficiently wide, local minima of the non-convex objective that satisfy a certain condition yield global minima of both the non-convex and convex objectives, and that there is always a non-increasing path to a global minimizer from any initialization.},
  isbn = {978-1-316-51678-2},
  keywords = {deep learning,non-convex optimization}
}

@misc{vijverbergTestingIIAHausmanMcfadden2011,
  type = {{{SSRN Scholarly Paper}}},
  title = {Testing for {{IIA}} with the {{Hausman-Mcfadden Test}}},
  author = {Vijverberg, Wim P. M.},
  year = {2011},
  month = jul,
  number = {1882845},
  address = {Rochester, NY},
  urldate = {2023-11-25},
  abstract = {The Independence of Irrelevant Alternatives assumption inherent in multinomial logit models is most frequently tested with a Hausman-McFadden test. As is confirmed by many findings in the literature, this test sometimes produces negative outcomes, in contradiction of its asymptotic {$\chi^2$} distribution. This problem is caused by the use of an improper variance matrix and may lead to an invalid statistical inference even when the test value is positive. With a correct specification of the variance, the sampling distribution for small samples is indeed close to a {$\chi^2$} distribution.},
  langid = {english},
  language = {en},
  keywords = {Hausman-McFadden test,IIA assumption,multinomial logit}
}

@article{voronkova1DimensionalTopologicalInvariants2023,
  title = {1-{{Dimensional Topological Invariants}} to {{Estimate Loss Surface Non-Convexity}}},
  author = {Voronkova, D. S. and Barannikov, S. A. and Burnaev, E. V.},
  year = {2023},
  month = dec,
  journal = {Doklady Mathematics},
  volume = {108},
  number = {2},
  pages = {S325-S332},
  issn = {1531-8362},
  urldate = {2024-05-28},
  abstract = {We utilize the framework of topological data analysis to examine the geometry of loss landscape. With the use of topology and Morse theory, we propose to analyse 1-dimensional topological invariants as a measure of loss function non-convexity up to arbitrary re-parametrization. The proposed approach uses optimization of 2-dimensional simplices in network weights space and allows to conduct both qualitative and quantitative evaluation of loss landscape to gain insights into behavior and optimization of neural networks. We provide geometrical interpretation of the topological invariants and describe the algorithm for their computation. We expect that the proposed approach can complement the existing tools for analysis of loss landscape and shed light on unresolved issues in the field of deep learning.},
  langid = {english},
  language = {en}
}

@article{vucinichCurrentStateChallenges2023,
  title = {The {{Current State}} and {{Challenges}} of {{Fairness}} in {{Federated Learning}}},
  author = {Vucinich, Sean and Zhu, Qiang},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {80903--80914},
  issn = {2169-3536},
  urldate = {2024-02-11},
  abstract = {The proliferation of artificial intelligence systems and their reliance on massive datasets have led to a renewed demand on privacy of data. Both the large data processing need and its associated data privacy demand have led to the development of techniques such as Federated Learning, a distributed machine learning technique with privacy preservation built-in. Within Federated Learning, as with other machine learning based techniques, the concern and challenges of ensuring that the decisions being made are fair and equitable to all users is paramount. This paper presents an up-to-date review of the motivations, concepts, characteristics, challenges, and techniques/methods related to fairness in Federated Learning reported in the literature. It also highlights open challenges and future research directions in evaluating and enforcing fairness in Federated Learning systems.},
  keywords = {algorithmic fairness,Data models,Data privacy,Deep learning,fairness evaluation,fairness measure,fairness of data,fairness of system,Federated learning,group fairness,individual fairness,Information integrity,Machine learning,Surveys,Training}
}

@article{wangAccuracyWhatData1996,
  title = {Beyond {{Accuracy}}: {{What Data Quality Means}} to {{Data Consumers}}},
  shorttitle = {Beyond {{Accuracy}}},
  author = {Wang, Richard Y. and Strong, Diane M.},
  year = {1996},
  journal = {Journal of Management Information Systems},
  volume = {12},
  number = {4},
  eprint = {40398176},
  eprinttype = {jstor},
  pages = {5--33},
  publisher = {Taylor \& Francis, Ltd.},
  issn = {0742-1222},
  urldate = {2023-11-25},
  abstract = {Poor data quality (DQ) can have substantial social and economic impacts. Although firms are improving data quality with practical approaches and tools, their improvement efforts tend to focus narrowly on accuracy. We believe that data consumers have a much broader data quality conceptualization than IS professionals realize. The purpose of this paper is to develop a framework that captures the aspects of data quality that are important to data consumers. A two-stage survey and a two-phase sorting study were conducted to develop a hierarchical framework for organizing data quality dimensions. This framework captures dimensions of data quality that are important to data consumers. Intrinsic DQ denotes that data have quality in their own right. Contextual DQ highlights the requirement that data quality must be considered within the context of the task at hand. Representational DQ and accessibility DQ emphasize the importance of the role of systems. These findings are consistent with our understanding that high-quality data should be intrinsically good, contextually appropriate for the task, clearly represented, and accessible to the data consumer. Our framework has been used effectively in industry and government. Using this framework, IS managers were able to better understand and meet their data consumers' data quality needs. The salient feature of this research study is that quality attributes of data are collected from data consumers instead of being defined theoretically or based on researchers' experience. Although exploratory, this research provides a basis for future studies that measure data quality along the dimensions of this framework.}
}

@inproceedings{wangBatchNormalizationDamages2023,
  title = {Batch {{Normalization Damages Federated Learning}} on {{NON-IID Data}}: {{Analysis}} and {{Remedy}}},
  shorttitle = {Batch {{Normalization Damages Federated Learning}} on {{NON-IID Data}}},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Yanmeng and Shi, Qingjiang and Chang, Tsung-Hui},
  year = {2023},
  month = jun,
  pages = {1--5},
  issn = {2379-190X},
  urldate = {2023-12-16},
  abstract = {Batch normalization (BN) has been widely used for accelerating the training of deep neural networks. However, recent findings show that, in the federated learning (FL) scenarios, BN can damage the learning performance when the clients have non-i.i.d. data. While several FL schemes have been proposed to address this issue, they still suffer a significant performance loss compared to the centralized scheme. In addition, none of them have explained how the BN impacts the FL convergence analytically. In this paper, we present the first convergence analysis to show that the mismatched local and global statistical parameters due to non-i.i.d data cause gradient deviation and it leads the algorithm to converge to a biased solution with a slower rate. To remedy this, we further present a new FL algorithm, called FedTAN, based on an iterative layer-wise parameter aggregation procedure. Experiment results are presented to show the superiority of FedTAN.}
}

@article{wangCollaborativeMachineLearning2023,
  title = {Collaborative {{Machine Learning}}: {{Schemes}}, {{Robustness}}, and {{Privacy}}},
  shorttitle = {Collaborative {{Machine Learning}}},
  author = {Wang, Junbo and Pal, Amitangshu and Yang, Qinglin and Kant, Krishna and Zhu, Kaiming and Guo, Song},
  year = {2023},
  month = dec,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {12},
  pages = {9625--9642},
  issn = {2162-2388},
  urldate = {2024-03-13},
  abstract = {Distributed machine learning (ML) was originally introduced to solve a complex ML problem in a parallel way for more efficient usage of computation resources. In recent years, such learning has been extended to satisfy other objectives, namely, performing learning in situ on the training data at multiple locations and keeping the training datasets private while still allowing sharing of the model. However, these objectives have led to considerable research on the vulnerabilities of distributed learning both in terms of privacy concerns of the training data and the robustness of the learned overall model due to bad or maliciously crafted training data. This article provides a comprehensive survey of various privacy, security, and robustness issues in distributed ML.},
  keywords = {Collaborative learning,Collaborative work,Data models,Distributed computing,distributed learning,federated learning,Federated learning,privacy,Privacy,robustness,Robustness,Support vector machines,Training}
}

@article{wangConceptDriftBasedCheckpointRestart2023,
  title = {Concept {{Drift-Based Checkpoint-Restart}} for {{Edge Services Rejuvenation}}},
  author = {Wang, Lei and Liu, Jiyuan and He, Qiang},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Services Computing},
  volume = {16},
  number = {3},
  pages = {1713--1725},
  issn = {1939-1374},
  urldate = {2023-11-27},
  abstract = {As a nascent technique, mobile edge computing (MEC) is mushrooming with a broad application prospect. By transferring abundant computing and storage resources from cloud to edge servers close to users, it allows services to be hosted on edge servers, which greatly reduces service latency. However, due to environmental dynamics, the edge services' reliability fluctuates in real-time. When the reliability of an edge service degrades severely, it may be suffering an anomaly, which can seriously impact its real-time performance and users' quality of experience. To ensure the real-time performance of edge services, this article presents CDCrest, a concept drift-based checkpoint restart approach for edge service rejuvenation. CDCrest employs L-Detection, a concept drift-based approach to detect edge services' runtime reliability anomalies. L-Detection leverages a sliding window and locality sensitive hashing (LSH)-based sampling to extract features from an edge service's real-time and historic reliability data streams. Then, it calculates the real-time distribution change degree (DCD) based on jensen-shannon (JS) divergence to infer whether the edge service is suffering a reliability anomaly. Once an anomaly for an edge service is identified, CDCrest employs a checkpoint restart mechanism to ensure the rapid rejuvenation of the edge service. Extensive experiments conducted based on a popular real-world dataset demonstrate the effectiveness and efficiency of CDCrest against the state-of-the-art approaches.}
}

@inproceedings{wangDiminishingEmpiricalRisk2022,
  title = {Diminishing {{Empirical Risk Minimization}} for {{Unsupervised Anomaly Detection}}},
  booktitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Wang, Shaoshen and Liu, Yanbin and Chen, Ling and Zhang, Chengqi},
  year = {2022},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  urldate = {2023-12-03},
  abstract = {Unsupervised anomaly detection (AD) is a challenging task in realistic applications. Recently, there is an increasing trend to detect anomalies with deep neural networks (DNN). However, most popular deep AD detectors cannot protect the network from learning contaminated information brought by anomalous data, resulting in unsatisfactory detection performance and overfitting issues. In this work, we identify one reason that hinders most existing DNN-based anomaly detection methods from performing is the wide adoption of the Empirical Risk Minimization (ERM). ERM assumes that the performance of an algorithm on an unknown distribution can be approximated by averaging losses on the known training set. This averaging scheme thus ignores the distinctions between normal and anomalous instances. To break through the limitations of ERM, we propose a novel Diminishing Empirical Risk Minimization (DERM) framework. Specifically, DERM adaptively adjusts the impact of individual losses through a well-devised aggregation strategy. Theoretically, our proposed DERM can directly modify the gradient contribution of each individual loss in the optimization process to suppress the influence of outliers, leading to a robust anomaly detector. Empirically, DERM outperformed the state-of-the-art on the unsupervised AD benchmark consisting of 18 datasets.}
}

@book{wangEdgeAIConvergence2020,
  title = {Edge {{AI}}: {{Convergence}} of {{Edge Computing}} and {{Artificial Intelligence}}},
  shorttitle = {Edge {{AI}}},
  author = {Wang, Xiaofei and Han, Yiwen and Leung, Victor C. M. and Niyato, Dusit and Yan, Xueqiang and Chen, Xu},
  year = {2020},
  publisher = {Springer},
  address = {Singapore},
  urldate = {2023-07-23},
  isbn = {9789811561856 9789811561863},
  langid = {english},
  language = {english},
  keywords = {Artificial intelligence,Deep learning,Edge AI,Edge computing,Edge DL,Edge intelligence,End-Edge-Cloud computing,Fog computing,Intelligent edge,Machine learning}
}

@inproceedings{wangEdgeLearningBasedEfficient2023,
  title = {Edge {{Learning-Based Efficient Data Imputation}} of {{Water Quality}}},
  booktitle = {7th {{International Conference}} on {{Computing}}, {{Control}} and {{Industrial Engineering}} ({{CCIE}} 2023)},
  author = {Wang, Yongsheng and Chen, Zhen and Liu, Limin and Gao, Jing and Liu, Guangwen and Xu, Zhiwei},
  editor = {S. Shmaliy, Yuriy and Nayyar, Anand},
  year = {2023},
  series = {Lecture {{Notes}} in {{Electrical Engineering}}},
  pages = {437--448},
  publisher = {Springer Nature},
  address = {Singapore},
  abstract = {Water quality monitoring is critical for developing effective surface water management strategies, whereas data preprocessing is the foundation for monitoring water quality. Water quality monitoring usually suffers from abnormal and missing values in the collected data of surface water quality, and thus an efficient data imputation method for water quality data is highly desired. To handle these challenges, we propose an edge learning-based data imputation method. By using VAE as the generator of WGAN-LP, the performance of value imputation of water quality is generalized and improved. In addition, we demonstrate the performance of the proposed method on two real-world datasets, collected from Wuliangsu Lake and Taihu Lake in Inner Mongolia. Compared with state of the art, the enhanced imputation accuarcy of water quality is achieved.},
  isbn = {978-981-9927-30-2},
  langid = {english},
  language = {en},
  keywords = {Data imputation,Edge learning,VAE,Water quality,WGAN-LP}
}

@article{wangEfficientSchedulingFederated2021,
  title = {Towards {{Efficient Scheduling}} of {{Federated Mobile Devices Under Computational}} and {{Statistical Heterogeneity}}},
  author = {Wang, Cong and Yang, Yuanyuan and Zhou, Pengzhan},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {32},
  number = {2},
  pages = {394--410},
  issn = {1558-2183},
  urldate = {2024-04-17},
  abstract = {Originated from distributed learning, federated learning enables privacy-preserved collaboration on a new abstracted level by sharing the model parameters only. While the current research mainly focuses on optimizing learning algorithms and minimizing communication overhead left by distributed learning, there is still a considerable gap when it comes to the real implementation on mobile devices. In this article, we start with an empirical experiment to demonstrate computation heterogeneity is a more pronounced bottleneck than communication on the current generation of battery-powered mobile devices, and the existing methods are haunted by mobile stragglers. Further, non-identically distributed data across the mobile users makes the selection of participants critical to the accuracy and convergence. To tackle the computational and statistical heterogeneity, we utilize data as a tuning knob and propose two efficient polynomial-time algorithms to schedule different workloads on various mobile devices, when data is identically or non-identically distributed. For identically distributed data, we combine partitioning and linear bottleneck assignment to achieve near-optimal training time without accuracy loss. For non-identically distributed data, we convert it into an average cost minimization problem and propose a greedy algorithm to find a reasonable balance between computation time and accuracy. We also establish an offline profiler to quantify the runtime behavior of different devices, which serves as the input to the scheduling algorithms. We conduct extensive experiments on a mobile testbed with two datasets and up to 20 devices. Compared with the common benchmarks, the proposed algorithms achieve 2-100{\texttimes} speedup epoch-wise, 2--7 percent accuracy gain and boost the convergence rate by more than 100 percent on CIFAR10.},
  keywords = {Computational modeling,Convergence,Distributed databases,Federated learning,Mobile handsets,non-IID data,on-device deep learning,scheduling optimization,Servers,Task analysis,Training}
}

@inproceedings{wangInferringClassRepresentatives2019,
  title = {Beyond {{Inferring Class Representatives}}: {{User-Level Privacy Leakage From Federated Learning}}},
  shorttitle = {Beyond {{Inferring Class Representatives}}},
  booktitle = {{{IEEE INFOCOM}} 2019 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Wang, Zhibo and Song, Mengkai and Zhang, Zhifei and Song, Yang and Wang, Qian and Qi, Hairong},
  year = {2019},
  month = apr,
  pages = {2512--2520},
  issn = {2641-9874},
  urldate = {2024-03-12},
  abstract = {Federated learning, i.e., a mobile edge computing framework for deep learning, is a recent advance in privacy-preserving machine learning, where the model is trained in a decentralized manner by the clients, i.e., data curators, preventing the server from directly accessing those private data from the clients. This learning mechanism significantly challenges the attack from the server side. Although the state-of-the-art attacking techniques that incorporated the advance of Generative adversarial networks (GANs) could construct class representatives of the global data distribution among all clients, it is still challenging to distinguishably attack a specific client (i.e., user-level privacy leakage), which is a stronger privacy threat to precisely recover the private data from a specific client. This paper gives the first attempt to explore user-level privacy leakage against the federated learning by the attack from a malicious server. We propose a framework incorporating GAN with a multi-task discriminator, which simultaneously discriminates category, reality, and client identity of input samples. The novel discrimination on client identity enables the generator to recover user specified private data. Unlike existing works that tend to interfere the training process of the federated learning, the proposed method works ``invisibly'' on the server side. The experimental results demonstrate the effectiveness of the proposed attacking approach and the superior to the state-of-the-art.},
  keywords = {Computational modeling,Data models,Data privacy,Gallium nitride,Privacy,Servers,Training}
}

@article{wangMissingValueFilling2022,
  title = {Missing {{Value Filling Based}} on the {{Collaboration}} of {{Cloud}} and {{Edge}} in {{Artificial Intelligence}} of {{Things}}},
  author = {Wang, Tian and Ke, Haoxiong and Jolfaei, Alireza and Wen, Sheng and Haghighi, Mohammad Sayad and Huang, Shuqiang},
  year = {2022},
  month = aug,
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {18},
  number = {8},
  pages = {5394--5402},
  issn = {1941-0050},
  urldate = {2023-11-16},
  abstract = {With the development of 5G technology and Internet of Things, all kinds of real life data are collected and recorded by a large number of sensors. It is of great significance to mine and analyze the hidden information in the data for applications like future prediction. However, due to interferences or instability of collection equipment, collected sensory data are often incomplete, and this incompleteness hinders the in-depth analysis of data in the cloud. Therefore, processing around missing values is significant. Relying on cloud machine learning methods is not enough to deal with the problem of missing data in the Artificial Intelligence of Things (AIoT) environment, however, edge computing provides a promising solution. In this article, gated recurrent units filling is employed at the edge nodes. A mobile edge node can not only find the historical information of the current missing data node but also acquire the data of the nodes adjacent to the missing data node. These ensure that the missing data are restored to the maximum extent at the source. The experimental results show that the missing value filling based on edge computing not only outperforms other filling methods in quality but also greatly reduces the energy consumption in AIoT.}
}

@inproceedings{wangNonIIDDataRebalancing2021,
  title = {Non-{{IID}} Data Re-Balancing at {{IoT}} Edge with Peer-to-Peer Federated Learning for Anomaly Detection},
  booktitle = {Proceedings of the 14th {{ACM Conference}} on {{Security}} and {{Privacy}} in {{Wireless}} and {{Mobile Networks}}},
  author = {Wang, Han and {Mu{\~n}oz-Gonz{\'a}lez}, Luis and Eklund, David and Raza, Shahid},
  year = {2021},
  month = jun,
  series = {{{WiSec}} '21},
  pages = {153--163},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-11-16},
  abstract = {The increase of the computational power in edge devices has enabled the penetration of distributed machine learning technologies such as federated learning, which allows to build collaborative models performing the training locally in the edge devices, improving the efficiency and the privacy for training of machine learning models, as the data remains in the edge devices. However, in some IoT networks the connectivity between devices and system components can be limited, which prevents the use of federated learning, as it requires a central node to orchestrate the training of the model. To sidestep this, peer-to-peer learning appears as a promising solution, as it does not require such an orchestrator. On the other side, the security challenges in IoT deployments have fostered the use of machine learning for attack and anomaly detection. In these problems, under supervised learning approaches, the training datasets are typically imbalanced, i.e. the number of anomalies is very small compared to the number of benign data points, which requires the use of re-balancing techniques to improve the algorithms' performance. In this paper, we propose a novel peer-to-peer algorithm,P2PK-SMOTE, to train supervised anomaly detection machine learning models in non-IID scenarios, including mechanisms to locally re-balance the training datasets via synthetic generation of data points from the minority class. To improve the performance in non-IID scenarios, we also include a mechanism for sharing a small fraction of synthetic data from the minority class across devices, aiming to reduce the risk of data de-identification. Our experimental evaluation in real datasets for IoT anomaly detection across a different set of scenarios validates the benefits of our proposed approach.},
  isbn = {978-1-4503-8349-3},
  keywords = {anomaly detection,federated learning,imbalanced data,non-IID data}
}

@inproceedings{wangOptimizingFederatedLearning2020,
  title = {Optimizing {{Federated Learning}} on {{Non-IID Data}} with {{Reinforcement Learning}}},
  booktitle = {{{IEEE INFOCOM}} 2020 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Wang, Hao and Kaplan, Zakhary and Niu, Di and Li, Baochun},
  year = {2020},
  month = jul,
  pages = {1698--1707},
  issn = {2641-9874},
  urldate = {2024-04-17},
  abstract = {The widespread deployment of machine learning applications in ubiquitous environments has sparked interests in exploiting the vast amount of data stored on mobile devices. To preserve data privacy, Federated Learning has been proposed to learn a shared model by performing distributed training locally on participating devices and aggregating the local models into a global one. However, due to the limited network connectivity of mobile devices, it is not practical for federated learning to perform model updates and aggregation on all participating devices in parallel. Besides, data samples across all devices are usually not independent and identically distributed (IID), posing additional challenges to the convergence and speed of federated learning. In this paper, we propose Favor, an experience-driven control framework that intelligently chooses the client devices to participate in each round of federated learning to counterbalance the bias introduced by non-IID data and to speed up convergence. Through both empirical and mathematical analysis, we observe an implicit connection between the distribution of training data on a device and the model weights trained based on those data, which enables us to profile the data distribution on that device based on its uploaded model weights. We then propose a mechanism based on deep Q-learning that learns to select a subset of devices in each communication round to maximize a reward that encourages the increase of validation accuracy and penalizes the use of more communication rounds. With extensive experiments performed in PyTorch, we show that the number of communication rounds required in federated learning can be reduced by up to 49\% on the MNIST dataset, 23\% on FashionMNIST, and 42\% on CIFAR-10, as compared to the Federated Averaging algorithm.},
  keywords = {Computational modeling,Convergence,Data models,Mobile handsets,Performance evaluation,Servers,Training}
}

@article{wangPoisoningAssistedPropertyInference2023,
  title = {Poisoning-{{Assisted Property Inference Attack Against Federated Learning}}},
  author = {Wang, Zhibo and Huang, Yuting and Song, Mengkai and Wu, Libing and Xue, Feng and Ren, Kui},
  year = {2023},
  month = jul,
  journal = {IEEE Transactions on Dependable and Secure Computing},
  volume = {20},
  number = {4},
  pages = {3328--3340},
  issn = {1941-0018},
  urldate = {2024-03-11},
  abstract = {Federated learning (FL) has emerged as an ideal privacy-preserving learning technique which can train a global model in a collaborative way while preserving the private data in the local. However, recent advances have demonstrated that FL is still vulnerable to inference attacks, such as reconstruction attack and membership inference. Among these attacks, the property inference attack, aiming to infer properties of the training data that are irrelevant with the learning objective, has not received too much attention while resulting in severe privacy leakage. Existing property inference attack approaches either cannot achieve satisfactory performance when the global model has converged or under dynamic FL where participants can drop in and drop out freely. In this paper, we propose a novel poisoning-assisted property inference attack (PAPI-attack) against FL. The key insight is that there exists underlying discriminative ability in the periodic model updates, which reflects the change of the data distribution, especially the occurrence of the sensitive property. Thus, a binary attack model can be constructed by a malicious participant for inferring the unintended information. More importantly, we present a property-specific poisoning mechanism by modifying the label of training data from the adversary to distort the decision boundary of shared (global) model in FL. Consequently, benign participants are induced to disclose more information about the sensitive property. Extensive experiments on real-world datasets demonstrate that PAPI-attack outperforms the state-of-the-art property inference attacks against FL.},
  keywords = {Collaborative work,Data models,Federated learning,Hidden Markov models,Privacy,privacy protection,property inference attack,Servers,Training,Training data}
}

@inproceedings{wangReconstructingTrainingData2023,
  title = {Reconstructing {{Training Data}} from {{Model Gradient}}, {{Provably}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Wang, Zihan and Lee, Jason and Lei, Qi},
  year = {2023},
  month = apr,
  pages = {6595--6612},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-02-27},
  abstract = {Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: Even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild assumptions: with shallow or deep neural networks and wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential  severe threats to privacy, especially in federated learning.},
  langid = {english},
  language = {en}
}

@article{wangWhyBatchNormalization2023,
  title = {Why {{Batch Normalization Damage Federated Learning}} on {{Non-IID Data}}?},
  author = {Wang, Yanmeng and Shi, Qingjiang and Chang, Tsung-Hui},
  year = {2023},
  month = nov,
  journal = {IEEE transactions on neural networks and learning systems},
  volume = {PP},
  issn = {2162-2388},
  abstract = {As a promising distributed learning paradigm, federated learning (FL) involves training deep neural network (DNN) models at the network edge while protecting the privacy of the edge clients. To train a large-scale DNN model, batch normalization (BN) has been regarded as a simple and effective means to accelerate the training and improve the generalization capability. However, recent findings indicate that BN can significantly impair the performance of FL in the presence of non-i.i.d. data. While several FL algorithms have been proposed to address this issue, their performance still falls significantly when compared to the centralized scheme. Furthermore, none of them have provided a theoretical explanation of how the BN damages the FL convergence. In this article, we present the first convergence analysis to show that under the non-i.i.d. data, the mismatch between the local and global statistical parameters in BN causes the gradient deviation between the local and global models, which, as a result, slows down and biases the FL convergence. In view of this, we develop a new FL algorithm that is tailored to BN, called FedTAN, which is capable of achieving robust FL performance under a variety of data distributions via iterative layer-wise parameter aggregation. Comprehensive experimental results demonstrate the superiority of the proposed FedTAN over existing baselines for training BN-based DNN models.},
  langid = {english},
  language = {eng},
  pmid = {37910415}
}

@inproceedings{wanShieldingFederatedLearning2022,
  title = {Shielding {{Federated Learning}}: {{Robust Aggregation}} with {{Adaptive Client Selection}}},
  shorttitle = {Shielding {{Federated Learning}}},
  booktitle = {Thirty-{{First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Wan, Wei and Hu, Shengshan and Lu, Jianrong and Zhang, Leo Yu and Jin, Hai and He, Yuanyuan},
  year = {2022},
  month = jul,
  volume = {1},
  pages = {753--760},
  issn = {1045-0823},
  urldate = {2024-04-07},
  abstract = {Electronic proceedings of IJCAI 2022},
  langid = {english},
  language = {en}
}

@misc{wanUnlockingPowerOpen2023,
  title = {Unlocking the {{Power}} of {{Open Set}} : {{A New Perspective}} for {{Open-set Noisy Label Learning}}},
  shorttitle = {Unlocking the {{Power}} of {{Open Set}}},
  author = {Wan, Wenhai and Wang, Xinrui and Xie, Mingkun and Huang, Shengjun and Chen, Songcan and Li, Shaoyuan},
  year = {2023},
  month = may,
  number = {arXiv:2305.04203},
  eprint = {2305.04203},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-03},
  abstract = {Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the seperation among known classes. Motivated by the phenomenon, in this paper, we propose a novel two-step contrastive learning method called CECL, which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some open-set examples into closed-set classes to enhance performance while treating others as delimiters to improve representative ability. Extensive experiments on synthetic and real-world datasets with diverse label noise demonstrate that CECL can outperform state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{weertsFairlearnAssessingImproving2023,
  title = {Fairlearn: {{Assessing}} and {{Improving Fairness}} of {{AI Systems}}},
  shorttitle = {Fairlearn},
  author = {Weerts, Hilde and Dud{\'i}k, Miroslav and Edgar, Richard and Jalali, Adrin and Lutz, Roman and Madaio, Michael},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {257},
  pages = {1--8},
  issn = {1533-7928},
  urldate = {2024-01-06},
  abstract = {Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named fairlearn, supports evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.}
}

@inproceedings{weinbergerInformationTheoreticDeterminationMisspecified2022,
  title = {On {{Information-Theoretic Determination}} of {{Misspecified Rates}} of {{Convergence}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  author = {Weinberger, Nir and Feder, Meir},
  year = {2022},
  month = jun,
  pages = {1695--1700},
  issn = {2157-8117},
  urldate = {2024-05-07},
  abstract = {We consider the problem of learning a model from given data samples in which the predictor's quality is measured by the log loss. We focus on the misspecified setting, in which the true model generating the data is chosen from a set different from the possible models that can be chosen by the learner. We establish minimax expected regret upper and lower bounds in terms of properly defined projected covering and packing entropies, and show their relation to M-projection geometric properties. We exemplify the bounds in a few settings.},
  keywords = {Convergence,Data models,Entropy,Information theory,Loss measurement,Predictive models}
}

@inproceedings{wenSharpnessMinimizationAlgorithms2024,
  title = {Sharpness {{Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wen, Kaiyue and Li, Zhiyuan},
  year = {2024},
  abstract = {Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize poorly, and (3) perhaps most strikingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. This calls for the search for other explanations for the generalization of over-parameterized neural networks.},
  langid = {english},
  language = {en}
}

@article{wenzelAssayingOutOfDistributionGeneralization2022,
  title = {Assaying {{Out-Of-Distribution Generalization}} in {{Transfer Learning}}},
  author = {Wenzel, Florian and Dittadi, Andrea and Gehler, Peter and {Simon-Gabriel}, Carl-Johann and Horn, Max and Zietlow, Dominik and Kernert, David and Russell, Chris and Brox, Thomas and Schiele, Bernt and Sch{\"o}lkopf, Bernhard and Locatello, Francesco},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {7181--7198},
  urldate = {2023-09-05},
  langid = {english},
  language = {english}
}

@article{wernerdevargasImbalancedDataPreprocessing2023,
  title = {Imbalanced Data Preprocessing Techniques for Machine Learning: A Systematic Mapping Study},
  shorttitle = {Imbalanced Data Preprocessing Techniques for Machine Learning},
  author = {{Werner~de~Vargas}, Vitor and Schneider~Aranda, Jorge Arthur and {dos Santos~Costa}, Ricardo and {da Silva~Pereira}, Paulo Ricardo and Vict{\'o}ria~Barbosa, Jorge Luis},
  year = {2023},
  month = jan,
  journal = {Knowledge and Information Systems},
  volume = {65},
  number = {1},
  pages = {31--57},
  issn = {0219-3116},
  urldate = {2024-01-30},
  abstract = {Machine Learning (ML) algorithms have been increasingly replacing people in several application domains---in which the majority suffer from data imbalance. In order to solve this problem, published studies implement data preprocessing techniques, cost-sensitive and ensemble learning. These solutions reduce the naturally occurring bias towards the majority sample through ML. This study uses a systematic mapping methodology to assess 9927 papers related to sampling techniques for ML in imbalanced data applications from 7 digital libraries. A filtering process selected 35 representative papers from various domains, such as health, finance, and engineering. As a result of a thorough quantitative analysis of these papers, this study proposes two taxonomies---illustrating sampling techniques and ML models. The results indicate that oversampling and classical ML are the most common preprocessing techniques and models, respectively. However, solutions with neural networks and ensemble ML models have the best performance---with potentially better results through hybrid sampling techniques. Finally, none of the 35 works apply simulation-based synthetic oversampling, indicating a path for future preprocessing solutions.},
  langid = {english},
  language = {en},
  keywords = {Imbalanced data,Machine learning,Preprocessing techniques,Sampling,Systematic mapping study}
}

@article{whangDataCollectionQuality2023,
  title = {Data Collection and Quality Challenges in Deep Learning: A Data-Centric {{AI}} Perspective},
  shorttitle = {Data Collection and Quality Challenges in Deep Learning},
  author = {Whang, Steven Euijong and Roh, Yuji and Song, Hwanjun and Lee, Jae-Gil},
  year = {2023},
  month = jul,
  journal = {The VLDB Journal},
  volume = {32},
  number = {4},
  pages = {791--813},
  issn = {0949-877X},
  urldate = {2023-12-01},
  abstract = {Data-centric AI is at the center of a fundamental shift in software engineering where machine learning becomes the new software, powered by big data and computing infrastructure. Here, software engineering needs to be re-thought where data become a first-class citizen on par with code. One striking observation is that a significant portion of the machine learning process is spent on data preparation. Without good data, even the best machine learning algorithms cannot perform well. As a result, data-centric AI practices are now becoming mainstream. Unfortunately, many datasets in the real world are small, dirty, biased, and even poisoned. In this survey, we study the research landscape for data collection and data quality primarily for deep learning applications. Data collection is important because there is lesser need for feature engineering for recent deep learning approaches, but instead more need for large amounts of data. For data quality, we study data validation, cleaning, and integration techniques. Even if the data cannot be fully cleaned, we can still cope with imperfect data during model training using robust model training techniques. In addition, while bias and fairness have been less studied in traditional data management research, these issues become essential topics in modern machine learning applications. We thus study fairness measures and unfairness mitigation techniques that can be applied before, during, or after model training. We believe that the data management community is well poised to solve these problems.},
  langid = {english},
  language = {en},
  keywords = {Data collection,Data quality,Data-centric AI,Deep learning}
}

@misc{wolinskiAdaptingNewtonsMethod2024,
  title = {Adapting {{Newton}}'s {{Method}} to {{Neural Networks}} through a {{Summary}} of {{Higher-Order Derivatives}}},
  author = {Wolinski, Pierre},
  year = {2024},
  month = feb,
  number = {arXiv:2312.03885},
  eprint = {2312.03885},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {We consider a gradient-based optimization method applied to a function \${\textbackslash}mathcal\{L\}\$ of a vector of variables \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$, in the case where \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$ is represented as a tuple of tensors \$({\textbackslash}mathbf\{T\}\_1, {\textbackslash}cdots, {\textbackslash}mathbf\{T\}\_S)\$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on \${\textbackslash}mathcal\{L\}\$, especially about the interactions between the tensors \${\textbackslash}mathbf\{T\}\_s\$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$ into tensors \$({\textbackslash}mathbf\{T\}\_1, {\textbackslash}cdots, {\textbackslash}mathbf\{T\}\_S)\$, in such a way that it requires neither the computation of the Hessian of \${\textbackslash}mathcal\{L\}\$ according to \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$, nor any approximation of it. The key part consists in computing a smaller matrix interpretable as a "Hessian according to the partition", which can be computed exactly and efficiently. In contrast to many existing practical second-order methods used in neural networks, which perform a diagonal or block-diagonal approximation of the Hessian or its inverse, the method we propose does not neglect interactions between layers. Finally, we can tune the coarseness of the partition to recover well-known optimization methods: the coarsest case corresponds to Cauchy's steepest descent method, the finest case corresponds to the usual Newton's method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@article{wuByzantineResilientDecentralizedStochastic2023,
  title = {Byzantine-{{Resilient Decentralized Stochastic Optimization With Robust Aggregation Rules}}},
  author = {Wu, Zhaoxian and Chen, Tianyi and Ling, Qing},
  year = {2023},
  journal = {IEEE Transactions on Signal Processing},
  volume = {71},
  pages = {3179--3195},
  issn = {1941-0476},
  urldate = {2024-04-07},
  abstract = {This article focuses on decentralized stochastic optimization in the presence of Byzantine attacks. During the optimization process, an unknown number of malfunctioning or malicious workers, termed as Byzantine workers, disobey the algorithmic protocol and send arbitrarily wrong messages to their neighbors. Even though various Byzantine-resilient algorithms have been developed for distributed stochastic optimization with a central server, we show that there are two major issues in the existing robust aggregation rules when being applied to the decentralized scenario: disagreement and non-doubly stochastic virtual mixing matrix. This article provides comprehensive analysis that discloses the negative effects of these two issues, and gives guidelines of designing favorable Byzantine-resilient decentralized stochastic optimization algorithms. Under these guidelines, we propose iterative outlier scissor (IOS), an iterative filtering-based robust aggregation rule with provable performance guarantees. Numerical experiments demonstrate the effectiveness of IOS. The code of simulation implementation is available at github.com/Zhaoxian-Wu/IOS.},
  keywords = {Byzantine attacks,Decentralized network,Guidelines,Optimization,Random variables,robust aggregation rule,Servers,Signal processing algorithms,stochastic optimization,Stochastic processes,Training}
}

@misc{wuDissectingHessianUnderstanding2022,
  title = {Dissecting {{Hessian}}: {{Understanding Common Structure}} of {{Hessian}} in {{Neural Networks}}},
  shorttitle = {Dissecting {{Hessian}}},
  author = {Wu, Yikai and Zhu, Xingyu and Wu, Chenwei and Wang, Annie and Ge, Rong},
  year = {2022},
  month = oct,
  number = {arXiv:2010.04261},
  eprint = {2010.04261},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-06},
  abstract = {Hessian captures important properties of the deep neural network loss landscape. Previous works have observed low rank structure in the Hessians of neural networks. In this paper, we propose a decoupling conjecture that decomposes the layer-wise Hessians of a network as the Kronecker product of two smaller matrices. We can analyze the properties of these smaller matrices and prove the structure of top eigenspace random 2-layer networks. The decoupling conjecture has several other interesting implications - top eigenspaces for different models have surprisingly high overlap, and top eigenvectors form low rank matrices when they are reshaped into the same shape as the corresponding weight matrix. All of these can be verified empirically for deeper networks. Finally, we use the structure of layer-wise Hessian to get better explicit generalization bounds for neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,Statistics - Machine Learning}
}

@misc{wuDRFLMDistributionallyRobust2022,
  title = {{{DRFLM}}: {{Distributionally Robust Federated Learning}} with {{Inter-client Noise}} via {{Local Mixup}}},
  shorttitle = {{{DRFLM}}},
  author = {Wu, Bingzhe and Liang, Zhipeng and Han, Yuxuan and Bian, Yatao and Zhao, Peilin and Huang, Junzhou},
  year = {2022},
  month = apr,
  number = {arXiv:2204.07742},
  eprint = {2204.07742},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-16},
  abstract = {Recently, federated learning has emerged as a promising approach for training a global model using data from multiple organizations without leaking their raw data. Nevertheless, directly applying federated learning to real-world tasks faces two challenges: (1) heterogeneity in the data among different organizations; and (2) data noises inside individual organizations. In this paper, we propose a general framework to solve the above two challenges simultaneously. Specifically, we propose using distributionally robust optimization to mitigate the negative effects caused by data heterogeneity paradigm to sample clients based on a learnable distribution at each iteration. Additionally, we observe that this optimization paradigm is easily affected by data noises inside local clients, which has a significant performance degradation in terms of global model prediction accuracy. To solve this problem, we propose to incorporate mixup techniques into the local training process of federated learning. We further provide comprehensive theoretical analysis including robustness analysis, convergence analysis, and generalization ability. Furthermore, we conduct empirical studies across different drug discovery tasks, such as ADMET property prediction and drug-target affinity prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{wuOnePixelShortcutLearning2023,
  title = {One-{{Pixel Shortcut}}: On the {{Learning Preference}} of {{Deep Neural Networks}}},
  shorttitle = {One-{{Pixel Shortcut}}},
  author = {Wu, Shutong and Chen, Sizhe and Xie, Cihang and Huang, Xiaolin},
  year = {2023},
  month = feb,
  number = {arXiv:2205.12141},
  eprint = {2205.12141},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-16},
  abstract = {Unlearnable examples (ULEs) aim to protect data from unauthorized usage for training DNNs. Existing work adds \${\textbackslash}ell\_{\textbackslash}infty\$-bounded perturbations to the original sample so that the trained model generalizes poorly. Such perturbations, however, are easy to eliminate by adversarial training and data augmentations. In this paper, we resolve this problem from a novel perspective by perturbing only one pixel in each image. Interestingly, such a small modification could effectively degrade model accuracy to almost an untrained counterpart. Moreover, our produced {\textbackslash}emph\{One-Pixel Shortcut (OPS)\} could not be erased by adversarial training and strong augmentations. To generate OPS, we perturb in-class images at the same position to the same target value that could mostly and stably deviate from all the original images. Since such generation is only based on images, OPS needs significantly less computation cost than the previous methods using DNN generators. Based on OPS, we introduce an unlearnable dataset called CIFAR-10-S, which is indistinguishable from CIFAR-10 by humans but induces the trained model to extremely low accuracy. Even under adversarial training, a ResNet-18 trained on CIFAR-10-S has only 10.61\% accuracy, compared to 83.02\% by the existing error-minimizing method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{wuToleratingAdversarialAttacks2021,
  title = {Tolerating {{Adversarial Attacks}} and {{Byzantine Faults}} in {{Distributed Machine Learning}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Wu, Yusen and Chen, Hao and Wang, Xin and Liu, Chao and Nguyen, Phuong and Yesha, Yelena},
  year = {2021},
  month = dec,
  pages = {3380--3389},
  urldate = {2024-03-21},
  abstract = {Adversarial attacks attempt to disrupt the training, retraining, and utilizing of artificial intelligence (AI) and machine learning models in large-scale distributed machine learning systems. This causes security risks on its prediction outcome. For example, attackers attempt to poison the model by either presenting inaccurate misrepresentative data or altering the models' parameters. In addition, Byzantine faults including software, hardware, network issues occur in distributed systems which also lead to a negative impact on the prediction outcome. In this paper, we propose a novel distributed training algorithm, partial synchronous stochastic gradient descent (ParSGD), which defends adversarial attacks and/or tolerates Byzantine faults. We demonstrate the effectiveness of our algorithm under three common adversarial attacks again the ML models and a Byzantine fault during the training phase. Our results show that using ParSGD, ML models can still produce accurate predictions as if it is not being attacked nor having failures at all when almost half of the nodes are being compromised or failed. We will report the experimental evaluations of ParSGD in comparison with other algorithms.},
  keywords = {Big Data,Byzantine-resilient SGD,Data security,Distributed ML,Machine learning,Machine learning algorithms,Software algorithms,Stochastic processes,Toxicology,Training}
}

@article{xiaFABAAlgorithmFast2019,
  title = {{{FABA}}: {{An Algorithm}} for {{Fast Aggregation}} against {{Byzantine Attacks}} in {{Distributed Neural Networks}}},
  shorttitle = {{{FABA}}},
  author = {Xia, Qi and Tao, Zeyi and Hao, Zijiang and Li, Qun},
  year = {2019},
  month = aug,
  journal = {IJCAI},
  urldate = {2024-04-06},
  abstract = {Many times, training a large scale deep learning neural network on a single machine becomes more and more difficult for a complex network model. Distributed training provides an efficient solution, but Byzantine attacks may occur on participating workers. They may be compromised or suffer from hardware failures. If they upload poisonous gradients, the training will become unstable or even converge to a saddle point. In this paper, we propose FABA, a Fast Aggregation algorithm against Byzantine Attacks, which removes the outliers in the uploaded gradients and obtains gradients that are close to the true gradients. We show the convergence of our algorithm. The experiments demonstrate that our algorithm can achieve similar performance to non-Byzantine case and higher efficiency as compared to previous algorithms.},
  langid = {english},
  language = {en}
}

@article{xiaoQualityInformationAware2022,
  title = {Toward {{Quality}} of {{Information Aware Distributed Machine Learning}}},
  author = {Xiao, Houping and Wang, Shiyu},
  year = {2022},
  month = jul,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {16},
  number = {6},
  pages = {109:1--109:28},
  issn = {1556-4681},
  urldate = {2023-11-29},
  abstract = {In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers (ADMM). However, existing consensus optimization frameworks assume that every node has the same quality of information (QoI), i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this article, we propose a novel consensus optimization framework for distributed machine-learning that incorporates the crucial metric, QoI. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations, but has a tighter upper bound compared with ADMM. Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM-based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy.},
  keywords = {Distributed machine learning,quality of information}
}

@article{xiaPoisoningAttacksFederated2023,
  title = {Poisoning {{Attacks}} in {{Federated Learning}}: {{A Survey}}},
  shorttitle = {Poisoning {{Attacks}} in {{Federated Learning}}},
  author = {Xia, Geming and Chen, Jian and Yu, Chaodong and Ma, Jun},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {10708--10722},
  issn = {2169-3536},
  urldate = {2024-03-25},
  abstract = {Federated learning faces many security and privacy issues. Among them, poisoning attacks can significantly impact global models, and malicious attackers can prevent global models from converging or even manipulating the prediction results of global models. Defending against poisoning attacks is a very urgent and challenging task. However, the systematic reviews of poisoning attacks and their corresponding defense strategies from a privacy-preserving perspective still need more effort. This survey provides an in-depth and up-to-date overview of poisoning attacks and corresponding defense strategies in federated learning. We first classify the poisoning attacks according to their methods and targets. Next, we analyze the differences and connections between the various categories of poisoning attacks. In addition, we classify the defense strategies against poisoning attacks in federated learning into three categories and analyze their advantages and disadvantages. Finally, we discuss the privacy protection problem in poisoning attacks and their countermeasure and propose potential research directions from the perspective of attack and defense, respectively.},
  keywords = {Data models,defense of poisoning attacks,distributed machine learning,Distributed processing,Federated learning,Machine learning,poisoning attacks,Privacy,Security,Servers}
}

@article{xiaSurveyFederatedLearning2021,
  title = {A Survey of Federated Learning for Edge Computing: {{Research}} Problems and Solutions},
  shorttitle = {A Survey of Federated Learning for Edge Computing},
  author = {Xia, Qi and Ye, Winson and Tao, Zeyi and Wu, Jindi and Li, Qun},
  year = {2021},
  month = jun,
  journal = {High-Confidence Computing},
  volume = {1},
  number = {1},
  pages = {100008},
  issn = {2667-2952},
  urldate = {2023-10-21},
  abstract = {Federated Learning is a machine learning scheme in which a shared prediction model can be collaboratively learned by a number of distributed nodes using their locally stored data. It can provide better data privacy because training data are not transmitted to a central server. Federated learning is well suited for edge computing applications and can leverage the the computation power of edge servers and the data collected on widely dispersed edge devices. To build such an edge federated learning system, we need to tackle a number of technical challenges. In this survey, we provide a new perspective on the applications, development tools, communication efficiency, security \& privacy, migration and scheduling in edge federated learning.},
  keywords = {Edge computing,Federated learning}
}

@misc{xieGeneralizedByzantinetolerantSGD2018,
  title = {Generalized {{Byzantine-tolerant SGD}}},
  author = {Xie, Cong and Koyejo, Oluwasanmi and Gupta, Indranil},
  year = {2018},
  month = mar,
  number = {arXiv:1802.10116},
  eprint = {1802.10116},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-03-25},
  abstract = {We propose three new robust aggregation rules for distributed synchronous Stochastic Gradient Descent{\textasciitilde}(SGD) under a general Byzantine failure model. The attackers can arbitrarily manipulate the data transferred between the servers and the workers in the parameter server{\textasciitilde}(PS) architecture. We prove the Byzantine resilience properties of these aggregation rules. Empirical analysis shows that the proposed techniques outperform current approaches for realistic use cases and Byzantine attack scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Statistics - Machine Learning}
}

@inproceedings{xuDataQualityMatters2023,
  title = {Data {{Quality Matters}}: {{A Case Study}} of {{Obsolete Comment Detection}}},
  shorttitle = {Data {{Quality Matters}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Xu, Shengbin and Yao, Yuan and Xu, Feng and Gu, Tianxiao and Xu, Jingwei and Ma, Xiaoxing},
  year = {2023},
  month = may,
  pages = {781--793},
  issn = {1558-1225},
  urldate = {2023-12-01},
  abstract = {Machine learning methods have achieved great success in many software engineering tasks. However, as a data-driven paradigm, how would the data quality impact the effectiveness of these methods remains largely unexplored. In this paper, we explore this problem under the context of just-in-time obsolete comment detection. Specifically, we first conduct data cleaning on the existing benchmark dataset, and empirically observe that with only 0.22\% label corrections and even 15.0\% fewer data, the existing obsolete comment detection approaches can achieve up to 10.7\% relative accuracy improvement. To further mitigate the data quality issues, we propose an adversarial learning framework to simultaneously estimate the data quality and make the final predictions. Experimental evaluations show that this adversarial learning framework can further improve the relative accuracy by up to 18.1\% compared to the state-of-the-art method. Although our current results are from the obsolete comment detection problem, we believe that the proposed two-phase solution, which handles the data quality issues through both the data aspect and the algorithm aspect, is also generalizable and applicable to other machine learning based software engineering tasks.}
}

@article{xuFrequencyPrincipleFourier2020,
  title = {Frequency {{Principle}}: {{Fourier Analysis Sheds Light}} on {{Deep Neural Networks}}},
  shorttitle = {Frequency {{Principle}}},
  author = {Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng},
  year = {2020},
  month = jun,
  journal = {Communications in Computational Physics},
  volume = {28},
  number = {5},
  eprint = {1901.06523},
  primaryclass = {cs, stat},
  pages = {1746--1767},
  issn = {1815-2406, 1991-7120},
  urldate = {2024-05-27},
  abstract = {We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) -- DNNs often fit target functions from low to high frequencies -- on high-dimensional benchmark datasets such as MNIST/CIFAR10 and deep neural networks such as VGG16. This F-Principle of DNNs is opposite to the behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibit faster convergence for higher frequencies for various scientific computing problems. With a simple theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.},
  archiveprefix = {arXiv},
  keywords = {68Q32 68T01,Computer Science - Machine Learning,I.2.6,Statistics - Machine Learning}
}

@misc{xuReputationMechanismAll2020,
  title = {A {{Reputation Mechanism Is All You Need}}: {{Collaborative Fairness}} and {{Adversarial Robustness}} in {{Federated Learning}}},
  shorttitle = {A {{Reputation Mechanism Is All You Need}}},
  author = {Xu, Xinyi and Lyu, Lingjuan},
  year = {2020},
  month = nov,
  urldate = {2024-04-03},
  abstract = {Federated learning (FL) is an emerging practical framework for effective and scalable machine learning among multiple participants, such as end users, organizations and companies. However, most existing FL or distributed learning frameworks have not well addressed two important issues together: collaborative fairness and adversarial robustness (e.g. free-riders and malicious participants). In conventional FL, all participants receive the global model (equal rewards), which might be unfair to the high-contributing participants. Furthermore, due to the lack of a safeguard mechanism, free-riders or malicious adversaries could game the system to access the global model for free or to sabotage it. In this paper, we propose a novel Robust and Fair Federated Learning (RFFL) framework to achieve collaborative fairness and adversarial robustness simultaneously via a reputation mechanism. RFFL maintains a reputation for each participant by examining their contributions via their uploaded gradients (using vector similarity) and thus identifies non-contributing or malicious participants to be removed. Our approach differentiates itself by not requiring any auxiliary/validation dataset. Extensive experiments on benchmark datasets show that RFFL can achieve high fairness and is very robust to different types of adversaries while achieving competitive predictive accuracy.},
  langid = {english},
  language = {en}
}

@article{yanDeFLDefendingModel2023,
  title = {{{DeFL}}: {{Defending}} against {{Model Poisoning Attacks}} in {{Federated Learning}} via {{Critical Learning Periods Awareness}}},
  shorttitle = {{{DeFL}}},
  author = {Yan, Gang and Wang, Hao and Yuan, Xu and Li, Jian},
  year = {2023},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {9},
  pages = {10711--10719},
  issn = {2374-3468},
  urldate = {2024-03-02},
  abstract = {Federated learning (FL) is known to be susceptible to model poisoning attacks in which malicious clients hamper the accuracy of the global model by sending manipulated model updates to the central server during the FL training process.  Existing defenses mainly focus on Byzantine-robust FL aggregations, and largely ignore the impact of the underlying deep neural network (DNN) that is used to FL training.  Inspired by recent findings on critical learning periods (CLP) in DNNs, where small gradient errors have irrecoverable impact on the final model accuracy, we propose a new defense, called a CLP-aware defense against poisoning of FL (DeFL).  The key idea of DeFL is to measure fine-grained differences between DNN model updates via an easy-to-compute federated gradient norm vector (FGNV) metric.  Using FGNV, DeFL simultaneously detects malicious clients and identifies CLP, which in turn is leveraged to guide the adaptive removal of detected malicious clients from aggregation.  As a result, DeFL not only mitigates model poisoning attacks on the global model but also is robust to detection errors.  Our extensive experiments on three benchmark datasets demonstrate that DeFL produces significant performance gain over conventional defenses against state-of-the-art model poisoning attacks.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {ML: Distributed Machine Learning & Federated Learning}
}

@article{yangClientSelectionFederated2022,
  title = {Client {{Selection}} for {{Federated Learning With Label Noise}}},
  author = {Yang, Miao and Qian, Hua and Wang, Ximin and Zhou, Yong and Zhu, Hongbin},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {71},
  number = {2},
  pages = {2193--2197},
  issn = {1939-9359},
  urldate = {2024-04-12},
  abstract = {Federated learning (FL) unleashes the full potential of training a global statistical model collaboratively from edge clients. In wireless FL, for the scarcity of spectrum, only a fraction of clients are capable to participate in the FL training in each round. On the other hand, the performance of FL suffers from the label noise, which naturally exists in the dataset of each client. To alleviate the label noise issue, prior works proposed several client selection algorithms, where the privacy of raw data might be compromised when extra information exchange was introduced. To avoid extra information exchange in FL, we propose to leverage an algorithm that measures the noise ratio of each client based on a clean validation dataset. We then propose an online client selection framework, supported by Copeland score and multi-arm bandits, which can select the client set with low noise ratios efficiently. The proposed algorithm is performed in the server side, thereby it can be implemented in the basic FL framework seamlessly. Simulation results demonstrate that our proposed algorithm spends less training time while guaranteeing the required accuracy compared to other baseline algorithms.},
  keywords = {client selection,Computational modeling,Data models,FL,label noise,Noise measurement,Privacy,relative feedback,Servers,Sorting,Training}
}

@inproceedings{yangTaxonomizingLocalGlobal2021,
  title = {Taxonomizing Local versus Global Structure in Neural Network Loss Landscapes},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yang, Yaoqing and Hodgkinson, Liam and Theisen, Ryan and Zou, Joe and Gonzalez, Joseph E and Ramchandran, Kannan and Mahoney, Michael W},
  year = {2021},
  volume = {34},
  pages = {18722--18733},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-07},
  abstract = {Viewing neural network models in terms of their loss landscapes has a long history in the statistical mechanics approach to learning, and in recent years it has received attention within machine learning proper. Among other things, local metrics (such as the smoothness of the loss landscape) have been shown to correlate with global properties of the model (such as good generalization performance). Here, we perform a detailed empirical analysis of the loss landscape structure of thousands of neural network models, systematically varying learning tasks, model architectures, and/or quantity/quality of data. By considering a range of metrics that attempt to capture different aspects of the loss landscape, we demonstrate that the best test accuracy is obtained when: the loss landscape is globally well-connected; ensembles of trained models are more similar to each other; and models converge to locally smooth regions. We also show that globally poorly-connected landscapes can arise when models are small or when they are trained to lower quality data; and that, if the loss landscape is globally poorly-connected, then training to zero loss can actually lead to worse test accuracy. Our detailed empirical results shed light on phases of learning (and consequent double descent behavior), fundamental versus incidental determinants of good generalization, the role of load-like and temperature-like parameters in the learning process, different influences on the loss landscape from model and data, and the relationships between local and global metrics, all topics of recent interest.}
}

@phdthesis{yaoDeepLearningNoisy2019,
  type = {Thesis},
  title = {Deep Learning with Noisy Supervision},
  author = {Yao, Jiangchao},
  year = {2019},
  urldate = {2024-01-23},
  abstract = {Central to many state-of-the-art classification systems via deep learning is sufficient accurate annotations for training. This is almost the bottleneck of all machine learning algorithms deployed with deep neural networks. The dilemma behind such a phenomenon is essentially the trade-off between the low expensive model design and the low expensive sample collection. For practical purpossummaryes to alleviate this issue, learning with noisy supervision is a critical solution in the Big Data era, since the noisily annotated data on the social websites and Amazon Mechanical Turk platforms can be easily acquired. Therefore, in this dissertation, we explore to solve the fundamental problems when training deep neural networks with noisy supervision.  Our first work is to introduce the low expensive noise structure information to overcome the decoupling bias issue existed learning with noise transition. We study the noise effect via a variable whose structure is implicitly aligned by the provided structure knowledge. Specifically, a Bayesian lower bound is deduced as the objective and it naturally degenerates to previous transition models in the case that there is no structure information available. Furthermore, a generative adversarial implementation is given to stably inject the structure information when training deep neural networks. The experimental results show the consistently improvement in the different simulated noises and the real-world scenario.  Our second work targets to substitute the previous ill-posed stochastic approximation to the noise transition with a rigorous stochastic reallocation regarding the confusion matrix. This work discovers the reason that causes the unstable issue in modeling the noise effect by a neural Softmax layer and introduces a Latent Class-Conditional Noise model to overcome it. In addition, a computational effective dynamic label regression method is deduced for optimization, which stochastic trains the deep neural network and safeguards the noise transition estimation. The proposed method achieves the state-of-the-art results on two toy datasets and two large real-world datasets.  The last work aims to alleviate the difficulty that the ideal assumption on the accurate noise transition is usually not fulfilled and the noise could still pollute the classifier in the back-propagation. We specially introduce a quality embedding factor to apportion the reasoning in the backpropagation, yielding a quality-augmented class-conditional noise model. On the network implementation, we elaborately design a contrastive-additive layer to infer the latent variable and deduce a stochastic optimization via reparameterization tricks. The results on a noisy web dataset and a noisy crowdsourcing dataset confirm the superiority of our model in the accuracy and interpretability.},
  copyright = {info:eu-repo/semantics/openAccess},
  langid = {australian},
  language = {en\_AU},
  annotation = {Accepted: 2019-09-17T04:28:18Z}
}

@inproceedings{yaoPyHessianNeuralNetworks2020,
  title = {{{PyHessian}}: {{Neural Networks Through}} the {{Lens}} of the {{Hessian}}},
  shorttitle = {{{PyHessian}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
  year = {2020},
  month = dec,
  pages = {581--590},
  urldate = {2024-05-19},
  abstract = {We present PYHESSIAN, a new scalable framework that enables fast computation of Hessian (i.e., second-order derivative) information for deep neural networks. PYHESSIAN enables fast computations of the top Hessian eigenvalues, the Hessian trace, and the full Hessian eigenvalue/spectral density; it supports distributed-memory execution on cloud/supercomputer systems; and it is available as open source [1]. This general framework can be used to analyze neural network models, including the topology of the loss landscape (i.e., curvature information) to gain insight into the behavior of different models/optimizers. As an example, we analyze the effect of residual connections and Batch Normalization layers on the trainability of neural networks. One recent claim, based on simpler first-order analysis, is that residual connections and Batch Normalization make the loss landscape "smoother," thus making it easier for Stochastic Gradient Descent to converge to a good solution. Our second-order analysis, easily enabled by PYHESSIAN, shows new finer-scale insights, demonstrating that while conventional wisdom is sometimes validated, in other cases it is simply incorrect. In particular, we find that Batch Normalization does not necessarily make the loss landscape smoother, especially for shallow networks.},
  keywords = {Analytical models,Artificial neural networks,Big Data,Eigenvalues and eigenfunctions,Electrostatic discharges,Lenses,Training}
}

@incollection{yiHomomorphicEncryption2014,
  title = {Homomorphic {{Encryption}}},
  booktitle = {Homomorphic {{Encryption}} and {{Applications}}},
  author = {Yi, Xun and Paulet, Russell and Bertino, Elisa},
  editor = {Yi, Xun and Paulet, Russell and Bertino, Elisa},
  year = {2014},
  pages = {27--46},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-04-11},
  abstract = {Homomorphic encryption is a form of encryption which allows specific types of computations to be carried out on ciphertexts and generate an encrypted result which, when decrypted, matches the result of operations performed on the plaintexts. This is a desirable feature in modern communication system architectures. RSA is the first public-key encryption scheme with a homomorphic property. However, for security, RSA has to pad a message with random bits before encryption to achieve semantic security. The padding results in RSA losing the homomorphic property. To avoid padding messages, many public-key encryption schemes with various homomorphic properties have been proposed in last three decades. In this chapter, we introduce basic homomorphic encryption techniques. It begins with a formal definition of homomorphic encryption, followed by some well-known homomorphic encryption schemes.},
  isbn = {978-3-319-12229-8},
  langid = {english},
  language = {en},
  keywords = {Encryption Scheme,Homomorphic Encryption,Homomorphic Property,Quadratic Residue,Semantic Security}
}

@inproceedings{yinByzantineRobustDistributedLearning2018,
  title = {Byzantine-{{Robust Distributed Learning}}: {{Towards Optimal Statistical Rates}}},
  shorttitle = {Byzantine-{{Robust Distributed Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  year = {2018},
  month = jul,
  pages = {5650--5659},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-06},
  abstract = {In this paper, we develop distributed optimization algorithms that are provably robust against Byzantine failures---arbitrary and potentially adversarial behavior, in distributed computing systems, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for all of strongly convex, non-strongly convex, and smooth non-convex population loss functions. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.},
  langid = {english},
  language = {en}
}

@article{yinComprehensiveSurveyPrivacypreserving2021,
  title = {A {{Comprehensive Survey}} of {{Privacy-preserving Federated Learning}}: {{A Taxonomy}}, {{Review}}, and {{Future Directions}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Privacy-preserving Federated Learning}}},
  author = {Yin, Xuefei and Zhu, Yanming and Hu, Jiankun},
  year = {2021},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {131:1--131:36},
  issn = {0360-0300},
  urldate = {2024-02-11},
  abstract = {The past four years have witnessed the rapid development of federated learning (FL). However, new privacy concerns have also emerged during the aggregation of the distributed intermediate results. The emerging privacy-preserving FL (PPFL) has been heralded as a solution to generic privacy-preserving machine learning. However, the challenge of protecting data privacy while maintaining the data utility through machine learning still remains. In this article, we present a comprehensive and systematic survey on the PPFL based on our proposed 5W-scenario-based taxonomy. We analyze the privacy leakage risks in the FL from five aspects, summarize existing methods, and identify future research directions.},
  keywords = {anonymization techniques,cryptographic encryption,data privacy,federated transfer learning,horizontal federated learning,perturbation techniques,Privacy-preserving federated learning,vertical federated learning}
}

@inproceedings{yoshidaDataDependencePlateauPhenomenon2019,
  title = {Data-{{Dependence}} of {{Plateau Phenomenon}} in {{Learning}} with {{Neural Network}} --- {{Statistical Mechanical Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yoshida, Yuki and Okada, Masato},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-11},
  abstract = {The plateau phenomenon, wherein the loss value stops decreasing during the process of learning, has been reported by various researchers. The phenomenon is actively inspected in the 1990s and found to be due to the fundamental hierarchical structure of neural network models. Then the phenomenon has been thought as inevitable. However, the phenomenon seldom occurs in the context of recent deep learning. There is a gap between theory and reality. In this paper, using statistical mechanical formulation, we clarified the relationship between the plateau phenomenon and the statistical property of the data learned. It is shown that the data whose covariance has small and dispersed eigenvalues tend to make the plateau phenomenon inconspicuous.}
}

@inproceedings{yoshidaHybridFLWirelessNetworks2020,
  title = {Hybrid-{{FL}} for {{Wireless Networks}}: {{Cooperative Learning Mechanism Using Non-IID Data}}},
  shorttitle = {Hybrid-{{FL}} for {{Wireless Networks}}},
  booktitle = {{{ICC}} 2020 - 2020 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  author = {Yoshida, Naoya and Nishio, Takayuki and Morikura, Masahiro and Yamamoto, Koji and Yonetani, Ryo},
  year = {2020},
  month = jun,
  pages = {1--7},
  issn = {1938-1883},
  urldate = {2024-04-17},
  abstract = {This paper proposes a cooperative mechanism for mitigating the performance degradation due to non-independent and-identically-distributed (non-IID) data in collaborative machine learning (ML), namely federated learning (FL), which trains an ML model using the rich data and computational resources of mobile clients without gathering their data to central systems. The data of mobile clients is typically non-IID owing to diversity among mobile clients' interests and usage, and FL with non-IID data could degrade the model performance. Therefore, to mitigate the degradation induced by non-IID data, we assume that a limited number (e.g., less than 1\%) of clients allow their data to be uploaded to a server, and we propose a hybrid learning mechanism referred to as Hybrid-FL, wherein the server updates the model using the data gathered from the clients and aggregates the model with the models trained by clients. The HybridFL solves both client- and data-selection problems via heuristic algorithms, which try to select the optimal sets of clients who train models with their own data, clients who upload their data to the server, and data uploaded to the server. The algorithms increase the number of clients participating in FL and make more data gather in the server IID, thereby improving the prediction accuracy of the aggregated model. Evaluations, which consist of network simulations and ML experiments, demonstrate that the proposed scheme achieves a 13.5\% higher classification accuracy than those of the previously proposed schemes for the non-IID case.},
  keywords = {Computational modeling,Data models,Distributed databases,Protocols,Servers,Task analysis,Training}
}

@inproceedings{youHandlingMissingData2020,
  title = {Handling {{Missing Data}} with {{Graph Representation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {You, Jiaxuan and Ma, Xiaobai and Ding, Yi and Kochenderfer, Mykel J and Leskovec, Jure},
  year = {2020},
  volume = {33},
  pages = {19075--19087},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-09-07},
  abstract = {Machine learning with missing data has been approached in many different ways, including feature imputation where missing feature values are estimated based on observed values and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label predictions often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph Neural Networks. Experimental results on nine benchmark datasets show that GRAPE yields 20\% lower mean absolute error for imputation tasks and 10\% lower for label prediction tasks, compared with existing state-of-the-art methods.}
}

@inproceedings{yueNeuralTangentKernel2022,
  title = {Neural {{Tangent Kernel Empowered Federated Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Yue, Kai and Jin, Richeng and Pilgrim, Ryan and Wong, Chau-Wai and Baron, Dror and Dai, Huaiyu},
  year = {2022},
  month = jun,
  pages = {25783--25803},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-12},
  abstract = {Federated learning (FL) is a privacy-preserving paradigm where multiple participants jointly solve a machine learning problem without sharing raw data. Unlike traditional distributed learning, a unique characteristic of FL is statistical heterogeneity, namely, data distributions across participants are different from each other. Meanwhile, recent advances in the interpretation of neural networks have seen a wide use of neural tangent kernels (NTKs) for convergence analyses. In this paper, we propose a novel FL paradigm empowered by the NTK framework. The paradigm addresses the challenge of statistical heterogeneity by transmitting update data that are more expressive than those of the conventional FL paradigms. Specifically, sample-wise Jacobian matrices, rather than model weights/gradients, are uploaded by participants. The server then constructs an empirical kernel matrix to update a global model without explicitly performing gradient descent. We further develop a variant with improved communication efficiency and enhanced privacy. Numerical results show that the proposed paradigm can achieve the same accuracy while reducing the number of communication rounds by an order of magnitude compared to federated averaging.},
  langid = {english},
  language = {en}
}

@inproceedings{yueSharpnessAwareMinimizationRevisited2023,
  title = {Sharpness-{{Aware Minimization Revisited}}: {{Weighted Sharpness}} as a {{Regularization Term}}},
  shorttitle = {Sharpness-{{Aware Minimization Revisited}}},
  booktitle = {Proceedings of the 29th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Yue, Yun and Jiang, Jiadi and Ye, Zhiling and Gao, Ning and Liu, Yongchao and Zhang, Ke},
  year = {2023},
  month = aug,
  series = {{{KDD}} '23},
  pages = {3185--3194},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2024-05-11},
  abstract = {Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at this link https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.},
  isbn = {9798400701030},
  keywords = {optimization,regularization,sharpness-aware minimization,wsam}
}

@article{yuRegularizationPenaltyOptimization2022,
  title = {Regularization {{Penalty Optimization}} for {{Addressing Data Quality Variance}} in {{OoD Algorithms}}},
  author = {Yu, Runpeng and Zhu, Hong and Li, Kaican and Hong, Lanqing and Zhang, Rui and Ye, Nanyang and Huang, Shao-Lun and He, Xiuqiang},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {8},
  pages = {8945--8953},
  issn = {2374-3468},
  urldate = {2023-11-28},
  abstract = {Due to the poor generalization performance of traditional empirical risk minimization (ERM) in the case of distributional shift, Out-of-Distribution (OoD) generalization algorithms receive increasing attention. However, OoD generalization algorithms overlook the great variance in the quality of training data, which significantly compromises the accuracy of these methods. In this paper, we theoretically reveal the relationship between training data quality and algorithm performance, and analyze the optimal regularization scheme for Lipschitz regularized invariant risk minimization. A novel algorithm is proposed based on the theoretical results to alleviate the influence of low quality data at both the sample level and the domain level. The experiments on both the regression and classification benchmarks validate the effectiveness of our method with statistical significance.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {Machine Learning (ML)}
}

@article{yuvalSemisupervisedEmpiricalRisk2022,
  title = {Semi-Supervised Empirical Risk Minimization: {{Using}} Unlabeled Data to Improve Prediction},
  shorttitle = {Semi-Supervised Empirical Risk Minimization},
  author = {Yuval, Oren and Rosset, Saharon},
  year = {2022},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {16},
  number = {1},
  pages = {1434--1460},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  urldate = {2023-11-28},
  abstract = {We present a general methodology for using unlabeled data to design semi supervised learning (SSL) variants of the Empirical Risk Minimization (ERM) learning process. Focusing on generalized linear regression, we analyze of the effectiveness of our SSL approach in improving prediction performance. The key ideas are carefully considering the null model as a competitor, and utilizing the unlabeled data to determine signal-noise combinations where SSL outperforms both supervised learning and the null model. We then use SSL in an adaptive manner based on estimation of the signal and noise. In the special case of linear regression with Gaussian covariates, we prove that the non-adaptive SSL version is in fact not capable of improving on both the supervised estimator and the null model simultaneously, beyond a negligible O(1/n) term. On the other hand, the adaptive model presented in this work, can achieve a substantial improvement over both competitors simultaneously, under a variety of settings. This is shown empirically through extensive simulations, and extended to other scenarios, such as non-Gaussian covariates, misspecified linear regression, or generalized linear regression with non-linear link functions.},
  keywords = {generalized linear model,predictive modeling,semi-supervised regression}
}

@inproceedings{zafarFairnessConstraintsMechanisms2017,
  title = {Fairness {{Constraints}}: {{Mechanisms}} for {{Fair Classification}}},
  shorttitle = {Fairness {{Constraints}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  year = {2017},
  month = apr,
  pages = {962--970},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-01-04},
  abstract = {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.},
  langid = {english},
  language = {en}
}

@article{zengNoiseUsefulExploiting2021,
  title = {Noise {{Is Useful}}: {{Exploiting Data Diversity}} for {{Edge Intelligence}}},
  shorttitle = {Noise {{Is Useful}}},
  author = {Zeng, Zhi and Liu, Yuan and Tang, Weijun and Chen, Fangjiong},
  year = {2021},
  month = may,
  journal = {IEEE Wireless Communications Letters},
  volume = {10},
  number = {5},
  pages = {957--961},
  issn = {2162-2345},
  urldate = {2024-01-16},
  abstract = {Edge intelligence requires to fast access distributed data samples generated by edge devices. The challenge is using limited radio resource to acquire massive data samples for training machine learning models at edge server. In this letter, we propose a new communication-efficient edge intelligence scheme where the most useful data samples are selected to train the model. Here the usefulness or values of data samples is measured by data diversity which is defined as the difference between data samples. We derive a close-form expression of data diversity that combines data informativeness and channel quality. Then a joint data-and-channel diversity aware multiuser scheduling algorithm is proposed. We find that noise is useful for enhancing data diversity under some conditions.}
}

@inproceedings{zhangBatchCryptEfficientHomomorphic2020,
  title = {\{\vphantom\}{{BatchCrypt}}\vphantom\{\}: {{Efficient Homomorphic Encryption}} for \{\vphantom\}{{Cross-Silo}}\vphantom\{\} {{Federated Learning}}},
  shorttitle = {\{\vphantom\}{{BatchCrypt}}\vphantom\{\}},
  booktitle = {2020 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 20)},
  author = {Zhang, Chengliang and Li, Suyi and Xia, Junzhe and Wang, Wei and Yan, Feng and Liu, Yang},
  year = {2020},
  pages = {493--506},
  urldate = {2024-04-11},
  isbn = {978-1-939133-14-4},
  langid = {english},
  language = {en}
}

@inproceedings{zhangDeepLearningElastic2015,
  title = {Deep Learning with {{Elastic Averaging SGD}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Sixin and Choromanska, Anna E and LeCun, Yann},
  year = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-09-25},
  abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.}
}

@article{zhangFedPDFederatedLearning2021,
  title = {{{FedPD}}: {{A Federated Learning Framework With Adaptivity}} to {{Non-IID Data}}},
  shorttitle = {{{FedPD}}},
  author = {Zhang, Xinwei and Hong, Mingyi and Dhople, Sairaj and Yin, Wotao and Liu, Yang},
  year = {2021},
  journal = {IEEE Transactions on Signal Processing},
  volume = {69},
  pages = {6055--6070},
  issn = {1941-0476},
  urldate = {2024-04-17},
  abstract = {Federated Learning (FL) is popular for communication-efficient learning from distributed data. To utilize data at different clients without moving them to the cloud, algorithms such as the Federated Averaging (FedAvg) have adopted a computation then aggregation model, in which multiple local updates are performed using local data before aggregation. These algorithms fail to work when faced with practical challenges, e.g., the local data being non-identically independently distributed. In this paper, we first characterize the behavior of the FedAvg algorithm, and show that without strong and unrealistic assumptions on the problem structure, it can behave erratically. Aiming at designing FL algorithms that are provably fast and require as few assumptions as possible, we propose a new algorithm design strategy from the primal-dual optimization perspective. Our strategy yields algorithms that can deal with non-convex objective functions, achieves the best possible optimization and communication complexity (in a well-defined sense), and accommodates full-batch and mini-batch local computation models. Importantly, the proposed algorithms are communication efficient, in that the communication effort can be reduced when the level of heterogeneity among the local data also reduces. In the extreme case where the local data becomes homogeneous, only {\textbackslash}mathcal O(1) communication is required among the agents. To the best of our knowledge, this is the first algorithmic framework for FL that achieves all the above properties.},
  keywords = {Complexity theory,Computational modeling,convergence analysis,data heterogeneity,Data models,Distributed algorithms,Distributed databases,federated learning,machine learning algorithms,Servers,Signal processing algorithms}
}

@inproceedings{zhangFedSensFederatedLearning2021,
  title = {{{FedSens}}: {{A Federated Learning Approach}} for {{Smart Health Sensing}} with {{Class Imbalance}} in {{Resource Constrained Edge Computing}}},
  shorttitle = {{{FedSens}}},
  booktitle = {{{IEEE INFOCOM}} 2021 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Zhang, Daniel Yue and Kou, Ziyi and Wang, Dong},
  year = {2021},
  month = may,
  pages = {1--10},
  issn = {2641-9874},
  urldate = {2023-11-16},
  abstract = {The advance of mobile sensing and edge computing has brought new opportunities for abnormal health detection (AHD) systems where edge devices such as smartphones and wearable sensors are used to collect people's health information and provide early alerts for abnormal health conditions such as stroke and depression. The recent development of federated learning (FL) allows participants to collaboratively train powerful AHD models while keeping their health data private to local devices. This paper targets at addressing a critical challenge of adapting FL to train AHD models, where the participants' health data is highly imbalanced and contains biased class distributions. Existing FL solutions fail to address the class imbalance issue due to the strict privacy requirements of participants as well as the heterogeneous resource constraints of their edge devices. In this work, we propose FedSens, a new FL framework dedicated to address the class imbalance problem in AHD applications with explicit considerations of participant privacy and device resource constraints. We evaluate FedSens using a real-world edge computing testbed on two real-world AHD applications. The results show that FedSens can significantly improve the accuracy of AHD models in the presence of severe class imbalance with low energy cost to the edge devices.}
}

@article{zhangGameTheoreticFederatedLearning2023,
  title = {A {{Game-Theoretic Federated Learning Framework}} for {{Data Quality Improvement}}},
  author = {Zhang, Lefeng and Zhu, Tianqing and Xiong, Ping and Zhou, Wanlei and Yu, Philip S.},
  year = {2023},
  month = nov,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {11},
  pages = {10952--10966},
  issn = {1558-2191},
  urldate = {2023-12-01},
  abstract = {Federated learning is a promising distributed machine learning paradigm that has been playing a significant role in privacy-preserving machine learning tasks. However, alongside all its achievements, the framework has limitations. First, traditional frameworks assume that all clients want to improve model accuracy and so participation is voluntary. However, in reality, clients usually want to be appropriately compensated for the data and resources they will need to commit to the training process before contributing. Second, today's frameworks allow clients to perturb their parameter updates locally, which introduces a great deal of noise to the trained model and can seriously impact model accuracy. To address these concerns, we have developed a private reward game that incentivizes clients to contribute high-quality data to the training process. The game converges to a Nash equilibrium under the guarantee of joint differential privacy, and each client maximizes their reward following an equilibrium strategy. The noise injected into the model is reduced by introducing a centralized differential privacy model that aggregates the parameters and compensates clients via a data trading market. Experimental simulations show the rationales behind and effectiveness of the proposed game approach. Additionally, we present comparisons between different training models to demonstrate the performance of the proposed approach in real-world scenarios.}
}

@inproceedings{zhangGeneralizedCrossEntropy2018,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Zhilu and Sabuncu, Mert},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-13},
  abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and large-scale datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.}
}

@misc{zhangLossSpikeTraining2023,
  title = {Loss {{Spike}} in {{Training Neural Networks}}},
  author = {Zhang, Zhongwang and Xu, Zhi-Qin John},
  year = {2023},
  month = may,
  number = {arXiv:2305.12133},
  eprint = {2305.12133},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-13},
  abstract = {In this work, we study the mechanism underlying loss spikes observed during neural network training. When the training enters a region, which has a smaller-loss-assharper (SLAS) structure, the training becomes unstable and loss exponentially increases once it is too sharp, i.e., the rapid ascent of the loss spike. The training becomes stable when it finds a flat region. The deviation in the first eigen direction (with maximum eigenvalue of the loss Hessian ({$\lambda$}max) is found to be dominated by low-frequency. Since low-frequency is captured very fast (frequency principle), the rapid descent is then observed. Inspired by our analysis of loss spikes, we revisit the link between {$\lambda$}max flatness and generalization. For real datasets, low-frequency is often dominant and well-captured by both the training data and the test data. Then, a solution with good generalization and a solution with bad generalization can both learn low-frequency well, thus, they have little difference in the sharpest direction. Therefore, although {$\lambda$}max can indicate the sharpness of the loss landscape, deviation in its corresponding eigen direction is not responsible for the generalization difference. We also find that loss spikes can facilitate condensation, i.e., input weights evolve towards the same, which may be the underlying mechanism for why the loss spike improves generalization, rather than simply controlling the value of {$\lambda$}max.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning}
}

@article{zhangNoiseawareLocalModel2023,
  title = {Noise-Aware {{Local Model Training Mechanism}} for {{Federated Learning}}},
  author = {Zhang, Jinghui and Lv, Dingyang and Dai, Qiangsheng and Xin, Fa and Dong, Fang},
  year = {2023},
  month = jun,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {14},
  number = {4},
  pages = {65:1--65:22},
  issn = {2157-6904},
  urldate = {2024-04-12},
  abstract = {As a new paradigm in training intelligent models, federated learning is widely used to train a global model without requiring local data to be uploaded from end devices. However, there are often mislabeled samples (i.e., noisy samples) in the dataset, which will cause the model update to deviate from the correct direction during the training process, thus reducing the convergence accuracy of the global model. Existing works employ noisy label correction techniques to reduce the impact of noisy samples on model updates by correcting labels; however, such methods necessitate the use of prior knowledge and additional communication costs, which cannot be directly applied to federated learning due to data privacy concerns and limited communication resources. Therefore, this paper proposes a noise-aware local model training method that corrects the noisy labels directly at the end device under the constraints of federated learning. By constructing a label correction model, a joint optimization problem is formally defined for optimizing both the label correction model and the client-side local training model (e.g., classification model). As a solution to this optimization problem, we propose a robustness training algorithm using label correction, along with a cross-validation data sampling algorithm that updates both models simultaneously. It is verified through experiments that the mechanism can effectively improve the model convergence accuracy on noisy datasets in federated learning scenarios.},
  keywords = {Federated learning,label correction,meta learning,mislabeled samples,noise-aware}
}

@inproceedings{zhangPEFLPrivacyEnhancedFederated2019,
  title = {{{PEFL}}: {{A Privacy-Enhanced Federated Learning Scheme}} for {{Big Data Analytics}}},
  shorttitle = {{{PEFL}}},
  author = {Zhang, Jiale and Chen, Bing and Yu, Shui and Deng, Hai},
  year = {2019},
  month = dec,
  pages = {1--6}
}

@article{zhangPoisonGANGenerativePoisoning2021,
  title = {{{PoisonGAN}}: {{Generative Poisoning Attacks Against Federated Learning}} in {{Edge Computing Systems}}},
  shorttitle = {{{PoisonGAN}}},
  author = {Zhang, Jiale and Chen, Bing and Cheng, Xiang and Binh, Huynh Thi Thanh and Yu, Shui},
  year = {2021},
  month = mar,
  journal = {IEEE Internet of Things Journal},
  volume = {8},
  number = {5},
  pages = {3310--3322},
  issn = {2327-4662},
  urldate = {2024-03-26},
  abstract = {Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker's well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data\_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data\_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning.},
  keywords = {Backdoor attack,Computational modeling,Data models,federated learning,Gallium nitride,generative adversarial nets,label flipping,Machine learning,poisoning attacks,Servers,Training,Training data}
}

@inproceedings{zhangPoisoningAttackFederated2019,
  title = {Poisoning {{Attack}} in {{Federated Learning}} Using {{Generative Adversarial Nets}}},
  booktitle = {2019 18th {{IEEE International Conference On Trust}}, {{Security And Privacy In Computing And Communications}}/13th {{IEEE International Conference On Big Data Science And Engineering}} ({{TrustCom}}/{{BigDataSE}})},
  author = {Zhang, Jiale and Chen, Junjun and Wu, Di and Chen, Bing and Yu, Shui},
  year = {2019},
  month = aug,
  pages = {374--380},
  issn = {2324-9013},
  urldate = {2024-03-26},
  abstract = {Federated learning is a novel distributed learning framework, where the deep learning model is trained in a collaborative manner among thousands of participants. The shares between server and participants are only model parameters, which prevent the server from direct access to the private training data. However, we notice that the federated learning architecture is vulnerable to an active attack from insider participants, called poisoning attack, where the attacker can act as a benign participant in federated learning to upload the poisoned update to the server so that he can easily affect the performance of the global model. In this work, we study and evaluate a poisoning attack in federated learning system based on generative adversarial nets (GAN). That is, an attacker first acts as a benign participant and stealthily trains a GAN to mimic prototypical samples of the other participants' training set which does not belong to the attacker. Then these generated samples will be fully controlled by the attacker to generate the poisoning updates, and the global model will be compromised by the attacker with uploading the scaled poisoning updates to the server. In our evaluation, we show that the attacker in our construction can successfully generate samples of other benign participants using GAN and the global model performs more than 80\% accuracy on both poisoning tasks and main tasks.},
  keywords = {Computational modeling,Data models,Federated learning poisoning attack generative adversarial nets security privacy,Gallium nitride,Servers,Task analysis,Training,Training data}
}

@article{zhangUnderstandingDeepLearning2021,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2021},
  month = feb,
  journal = {Communications of the ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  issn = {0001-0782},
  urldate = {2024-04-11},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.}
}

@inproceedings{zhangUnderstandingDeepLearning2022,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2022},
  month = jul,
  urldate = {2024-04-11},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  langid = {english},
  language = {en}
}

@misc{zhangUnifiedGroupFairness2022,
  title = {Unified {{Group Fairness}} on {{Federated Learning}}},
  author = {Zhang, Fengda and Kuang, Kun and Liu, Yuxuan and Chen, Long and Wu, Chao and Wu, Fei and Lu, Jiaxun and Shao, Yunfeng and Xiao, Jun},
  year = {2022},
  month = feb,
  number = {arXiv:2111.04986},
  eprint = {2111.04986},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-31},
  abstract = {Federated learning (FL) has emerged as an important machine learning paradigm where a global model is trained based on the private data from distributed clients. However, most of existing FL algorithms cannot guarantee the performance fairness towards different groups because of data distribution shift over groups. In this paper, we formulate the problem of unified group fairness on FL, where the groups can be formed by clients (including existing clients and newly added clients) and sensitive attribute(s). To solve this problem, we first propose a general fair federated framework. Then we construct a unified group fairness risk from the view of federated uncertainty set with theoretical analyses to guarantee unified group fairness on FL. We also develop an efficient federated optimization algorithm named Federated Mirror Descent Ascent with Momentum Acceleration (FMDA-M) with convergence guarantee. We validate the advantages of the FMDA-M algorithm with various kinds of distribution shift settings in experiments, and the results show that FMDA-M algorithm outperforms the existing fair FL algorithms on unified group fairness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,DRO}
}

@misc{zhangWhyFlatnessDoes2021,
  title = {Why Flatness Does and Does Not Correlate with Generalization for Deep Neural Networks},
  author = {Zhang, Shuofeng and Reid, Isaac and P{\'e}rez, Guillermo Valle and Louis, Ard},
  year = {2021},
  month = jun,
  number = {arXiv:2103.06219},
  eprint = {2103.06219},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-21},
  abstract = {The intuition that local flatness of the loss landscape is correlated with better generalization for deep neural networks (DNNs) has been explored for decades, spawning many different flatness measures. Recently, this link with generalization has been called into question by a demonstration that many measures of flatness are vulnerable to parameter re-scaling which arbitrarily changes their value without changing neural network outputs. Here we show that, in addition, some popular variants of SGD such as Adam and Entropy-SGD, can also break the flatness-generalization correlation. As an alternative to flatness measures, we use a function based picture and propose using the log of Bayesian prior upon initialization, \${\textbackslash}log P(f)\$, as a predictor of the generalization when a DNN converges on function \$f\$ after training to zero error. The prior is directly proportional to the Bayesian posterior for functions that give zero error on a test set. For the case of image classification, we show that \${\textbackslash}log P(f)\$ is a significantly more robust predictor of generalization than flatness measures are. Whilst local flatness measures fail under parameter re-scaling, the prior/posterior, which is global quantity, remains invariant under re-scaling. Moreover, the correlation with generalization as a function of data complexity remains good for different variants of SGD.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{zhaoFederatedLearningNonIID2018,
  title = {Federated {{Learning}} with {{Non-IID Data}}},
  author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  year = {2018},
  eprint = {1806.00582},
  primaryclass = {cs, stat},
  urldate = {2024-03-08},
  abstract = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55\% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30\% for the CIFAR-10 dataset with only 5\% globally shared data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{zhaoLocalDifferentialPrivacyBased2021,
  title = {Local {{Differential Privacy-Based Federated Learning}} for {{Internet}} of {{Things}}},
  author = {Zhao, Yang and Zhao, Jun and Yang, Mengmeng and Wang, Teng and Wang, Ning and Lyu, Lingjuan and Niyato, Dusit and Lam, Kwok-Yan},
  year = {2021},
  month = jun,
  journal = {IEEE Internet of Things Journal},
  volume = {8},
  number = {11},
  pages = {8836--8853},
  issn = {2327-4662},
  urldate = {2024-03-19},
  abstract = {The Internet of Vehicles (IoV) is a promising branch of the Internet of Things. IoV simulates a large variety of crowdsourcing applications, such as Waze, Uber, and Amazon Mechanical Turk, etc. Users of these applications report the real-time traffic information to the cloud server which trains a machine learning model based on traffic information reported by users for intelligent traffic management. However, crowdsourcing application owners can easily infer users' location information, traffic information, motor vehicle information, environmental information, etc., which raises severe sensitive personal information privacy concerns of the users. In addition, as the number of vehicles increases, the frequent communication between vehicles and the cloud server incurs unexpected amount of communication cost. To avoid the privacy threat and reduce the communication cost, in this article, we propose to integrate federated learning and local differential privacy (LDP) to facilitate the crowdsourcing applications to achieve the machine learning model. Specifically, we propose four LDP mechanisms to perturb gradients generated by vehicles. The proposed Three-Outputs mechanism introduces three different output possibilities to deliver a high accuracy when the privacy budget is small. The output possibilities of Three-Outputs can be encoded with two bits to reduce the communication cost. Besides, to maximize the performance when the privacy budget is large, an optimal piecewise mechanism (PM-OPT) is proposed. We further propose a suboptimal mechanism (PM-SUB) with a simple formula and comparable utility to PM-OPT. Then, we build a novel hybrid mechanism by combining Three-Outputs and PM-SUB. Finally, an LDP-FedSGD algorithm is proposed to coordinate the cloud server and vehicles to train the model collaboratively. Extensive experimental results on real-world data sets validate that our proposed algorithms are capable of protecting privacy while guaranteeing utility.},
  keywords = {Cloud computing,Crowdsourcing,Differential privacy,Federated learning,Internet of Things,local differential privacy,Privacy,Servers,Software algorithms}
}

@misc{zhaoSymmetriesFlatMinima2023,
  title = {Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow},
  author = {Zhao, Bo and Ganev, Iordan and Walters, Robin and Yu, Rose and Dehmamy, Nima},
  year = {2023},
  month = mar,
  number = {arXiv:2210.17216},
  eprint = {2210.17216},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-08-26},
  abstract = {Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part of the global minimum. By relating conserved quantities to convergence rate and sharpness of the minimum, we provide insights on how initialization impacts convergence and generalizability.},
  archiveprefix = {arXiv},
  langid = {english},
  language = {en},
  keywords = {Computer Science - Machine Learning,Mathematics - Representation Theory}
}

@article{zhongStudyRealworldMicrograph2021,
  title = {A Study of Real-World Micrograph Data Quality and Machine Learning Model Robustness},
  author = {Zhong, Xiaoting and Gallagher, Brian and Eves, Keenan and Robertson, Emily and Mundhenk, T. Nathan and Han, T. Yong-Jin},
  year = {2021},
  month = oct,
  journal = {npj Computational Materials},
  volume = {7},
  number = {1},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {2057-3960},
  urldate = {2023-11-12},
  abstract = {Machine-learning (ML) techniques hold the potential of enabling efficient quantitative micrograph analysis, but the robustness of ML models with respect to real-world micrograph quality variations has not been carefully evaluated. We collected thousands of scanning electron microscopy (SEM) micrographs for molecular solid materials, in which image pixel intensities vary due to both the microstructure content and microscope instrument conditions. We then built ML models to predict the ultimate compressive strength (UCS) of consolidated molecular solids, by encoding micrographs with different image feature descriptors and training a random forest regressor, and by training an end-to-end deep-learning (DL) model. Results show that instrument-induced pixel intensity signals can affect ML model predictions in a consistently negative way. As a remedy, we explored intensity normalization techniques. It is seen that intensity normalization helps to improve micrograph data quality and ML model robustness, but microscope-induced intensity variations can be difficult to eliminate.},
  copyright = {2021 The Author(s)},
  langid = {english},
  language = {en},
  keywords = {Techniques and instrumentation,Theory and computation}
}

@misc{zhouCriticalPointsNeural2017,
  title = {Critical {{Points}} of {{Neural Networks}}: {{Analytical Forms}} and {{Landscape Properties}}},
  shorttitle = {Critical {{Points}} of {{Neural Networks}}},
  author = {Zhou, Yi and Liang, Yingbin},
  year = {2017},
  month = oct,
  number = {arXiv:1710.11205},
  eprint = {1710.11205},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-13},
  abstract = {Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide full (necessary and sufficient) characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for various neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of these neural networks. One particular conclusion is that: The loss function of linear networks has no spurious local minimum, while the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{zhouEdgeIntelligencePaving2019,
  title = {Edge {{Intelligence}}: {{Paving}} the {{Last Mile}} of {{Artificial Intelligence With Edge Computing}}},
  shorttitle = {Edge {{Intelligence}}},
  author = {Zhou, Zhi and Chen, Xu and Li, En and Zeng, Liekang and Luo, Ke and Zhang, Junshan},
  year = {2019},
  month = aug,
  journal = {Proceedings of the IEEE},
  volume = {107},
  number = {8},
  pages = {1738--1762},
  issn = {0018-9219, 1558-2256},
  urldate = {2023-10-04},
  langid = {english},
  language = {en},
  keywords = {figure}
}

@article{zhouFastAdaBeliefImprovingConvergence2023,
  title = {{{FastAdaBelief}}: {{Improving Convergence Rate}} for {{Belief-Based Adaptive Optimizers}} by {{Exploiting Strong Convexity}}},
  shorttitle = {{{FastAdaBelief}}},
  author = {Zhou, Yangfan and Huang, Kaizhu and Cheng, Cheng and Wang, Xuguang and Hussain, Amir and Liu, Xin},
  year = {2023},
  month = sep,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {9},
  pages = {6515--6529},
  issn = {2162-2388},
  urldate = {2024-05-07},
  abstract = {AdaBelief, one of the current best optimizers, demonstrates superior generalization ability over the popular Adam algorithm by viewing the exponential moving average of observed gradients. AdaBelief is theoretically appealing in which it has a data-dependent O({\textbackslash}sqrt T) regret bound when objective functions are convex, where T is a time horizon. It remains, however, an open problem whether the convergence rate can be further improved without sacrificing its generalization ability. To this end, we make the first attempt in this work and design a novel optimization algorithm called FastAdaBelief that aims to exploit its strong convexity in order to achieve an even faster convergence rate. In particular, by adjusting the step size that better considers strong convexity and prevents fluctuation, our proposed FastAdaBelief demonstrates excellent generalization ability and superior convergence. As an important theoretical contribution, we prove that FastAdaBelief attains a data-dependent O({\l}og T) regret bound, which is substantially lower than AdaBelief in strongly convex cases. On the empirical side, we validate our theoretical analysis with extensive experiments in scenarios of strong convexity and nonconvexity using three popular baseline models. Experimental results are very encouraging: FastAdaBelief converges the quickest in comparison to all mainstream algorithms while maintaining an excellent generalization ability, in cases of both strong convexity or nonconvexity. FastAdaBelief is, thus, posited as a new benchmark model for the research community.},
  keywords = {Adaptive learning rate,Benchmark testing,Convergence,Deep learning,image classification,Learning systems,online learning,Optimization,optimization algorithm,stochastic gradient descent,strong convexity,Task analysis,Training}
}

@article{zhouFederatedLabelNoiseLearning2024,
  title = {Federated {{Label-Noise Learning}} with {{Local Diversity Product Regularization}}},
  author = {Zhou, Xiaochen and Wang, Xudong},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {15},
  pages = {17141--17149},
  issn = {2374-3468},
  urldate = {2024-04-12},
  abstract = {Training data in federated learning (FL) frameworks can have label noise, since they must be stored and annotated on clients' devices.  If trained over such corrupted data, the models learn the wrong knowledge of label noise, which highly degrades their performance.   Although several FL schemes are designed to combat label noise, they suffer performance degradation when the clients' devices only have limited local training samples. To this end, a new scheme called federated label-noise learning (FedLNL) is developed in this paper. The key problem of FedLNL is how to estimate a noise transition matrix (NTM) accurately in the case of limited local training samples. If a gradient-based update method is used to update the local NTM on each client's device, it can generate too large gradients for the local NTM, causing a high estimation error of the local NTM. To tackle this issue, an alternating update method for the local NTM and the local classifier is designed in FedLNL, where the local NTM is updated by a Bayesian inference-based update method. Such an alternating update method makes the loss function of existing NTM-based schemes not applicable to FedLNL. To enable federated optimization of FedLNL, a new regularizer on the parameters of the classifier called local diversity product regularizer is designed for the loss function of FedLNL.  The results show that FedLNL improves the test accuracy of a trained model by up to 25.98\%, compared with the state-of-the-art FL schemes that tackle label-noise issues.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  language = {en},
  keywords = {ML: Adversarial Learning & Robustness}
}

@inproceedings{zhouManifoldProjectionAdversarial2020,
  title = {Manifold {{Projection}} for {{Adversarial Defense}} on {{Face Recognition}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Zhou, Jianli and Liang, Chao and Chen, Jun},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  pages = {288--305},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {Although deep convolutional neural network based face recognition system has achieved remarkable success, it is susceptible to adversarial images: carefully constructed imperceptible perturbations can easily mislead deep neural networks. A recent study has shown that in addition to regular off-manifold adversarial images, there are also adversarial images on the manifold. In this paper, we propose Adversarial Variational AutoEncoder (A-VAE), a novel framework to tackle both types of attacks. We hypothesize that both off-manifold and on-manifold attacks move the image away from the high probability region of image manifold. We utilize variational autoencoder (VAE) to estimate the lower bound of the log-likelihood of image and explore to project the input images back into the high probability regions of image manifold again. At inference time, our model synthesizes multiple similar realizations of a given image by random sampling, then the nearest neighbor of the given image is selected as the final input of the face recognition model. As a preprocessing operation, our method is attack-agnostic and can adapt to a wide range of resolutions. The experimental results on LFW demonstrate that our method achieves state-of-the-art defense success rate against conventional off-manifold attacks such as FGSM, PGD, and C\&W under both grey-box and white-box settings, and even on-manifold attack.},
  isbn = {978-3-030-58577-8},
  langid = {english},
  language = {en},
  keywords = {Adversarial defense,Face recognition}
}

@inproceedings{zhouModelAgnosticSample2022,
  title = {Model {{Agnostic Sample Reweighting}} for {{Out-of-Distribution Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Zhou, Xiao and Lin, Yong and Pi, Renjie and Zhang, Weizhong and Xu, Renzhe and Cui, Peng and Zhang, Tong},
  year = {2022},
  month = jun,
  pages = {27203--27221},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-23},
  abstract = {Distributionally robust optimization (DRO) and invariant risk minimization (IRM) are two popular methods proposed to improve out-of-distribution (OOD) generalization performance of machine learning models. While effective for small models, it has been observed that these methods can be vulnerable to overfitting with large overparameterized models. This work proposes a principled method, Model Agnostic samPLe rEweighting (MAPLE), to effectively address OOD problem, especially in overparameterized scenarios. Our key idea is to find an effective reweighting of the training samples so that the standard empirical risk minimization training of a large model on the weighted training data leads to superior OOD generalization performance. The overfitting issue is addressed by considering a bilevel formulation to search for the sample reweighting, in which the generalization complexity depends on the search space of sample weights instead of the model size. We present theoretical analysis in linear case to prove the insensitivity of MAPLE to model size, and empirically verify its superiority in surpassing state-of-the-art methods by a large margin.},
  langid = {english},
  language = {en}
}

@inproceedings{zhuDeepLeakageGradients2019,
  title = {Deep {{Leakage}} from {{Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhu, Ligeng and Liu, Zhijian and Han, Song},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-27},
  abstract = {Passing gradient is a widely used scheme in  modern multi-node learning system (e.g, distributed training, collaborative learning). In a long time, people used to believe that gradients are safe to share: i.e, the training set will not be leaked by gradient sharing.  However, in this paper, we show that we can obtain the private training set from the publicly shared gradients.  The leaking only takes few gradient steps to process and can obtain the original training set instead of look-alike alternatives.  We name this leakage as {\textbackslash}textit\{deep leakage from gradient\}  and practically validate the effectiveness of our algorithm on both computer vision and natural language processing tasks. We empirically show that our attack is much stronger than previous approaches and thereby and raise people's awareness to rethink the gradients' safety. We also discuss some possible strategies to defend this deep leakage.}
}

@incollection{zhuDeepLeakageGradients2020,
  title = {Deep {{Leakage}} from {{Gradients}}},
  booktitle = {Federated {{Learning}}: {{Privacy}} and {{Incentive}}},
  author = {Zhu, Ligeng and Han, Song},
  editor = {Yang, Qiang and Fan, Lixin and Yu, Han},
  year = {2020},
  pages = {17--31},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-03-17},
  abstract = {Exchanging model updates is a widely used method in the modern federated learning system. For a long time, people believed that gradients are safe to share: i.e., the gradients are less informative than the training data. However, there is information hidden in the gradients. Moreover, it is even possible to reconstruct the private training data from the publicly shared gradients. This chapter discusses techniques that reveal information hidden in gradients and validate the effectiveness on common deep learning tasks. It is important to raise people's awareness to rethink the gradient's safety. Several possible defense strategies have also been discussed to prevent such privacy leakage.},
  isbn = {978-3-030-63076-8},
  langid = {english},
  language = {en},
  keywords = {Federated Learning,Gradients' safety,Privacy leakage}
}

@inproceedings{zhuEliminatingClassNoise2003,
  title = {Eliminating {{Class Noise}} in {{Large Datasets}}},
  booktitle = {Proceedings, {{Twentieth International Conference}} on {{Machine Learning}}},
  author = {Zhu, X. and Wu, X. and Chen, Q.},
  year = {2003},
  volume = {2},
  pages = {920--927},
  abstract = {This paper presents a new approach for identifying and eliminating mislabeled instances in large or distributed datasets. We first partition a dataset into subsets, each of which is small enough to be processed by an induction algorithm at one time. We construct good rules from each subset, and use the good rules to evaluate the whole dataset. For a given instance I k, two error count variables are used to count the number of times it has been identified as noise by all subsets. The instance with higher error values will have a higher probability of being a mislabeled example. Two threshold schemes, majority and non-objection, are used to identify the noise. Experimental results and comparative studies from real-world datasets are reported to evaluate the effectiveness and efficiency of the proposed approach.},
  isbn = {978-1-57735-189-4},
  langid = {english},
  language = {English}
}

@article{zhuFederatedLearningNonIID2021,
  title = {Federated Learning on Non-{{IID}} Data: {{A}} Survey},
  shorttitle = {Federated Learning on Non-{{IID}} Data},
  author = {Zhu, Hangyu and Xu, Jinjin and Liu, Shiqing and Jin, Yaochu},
  year = {2021},
  month = nov,
  journal = {Neurocomputing},
  volume = {465},
  pages = {371--390},
  issn = {0925-2312},
  urldate = {2024-03-08},
  abstract = {Federated learning is an emerging distributed machine learning framework for privacy preservation. However, models trained in federated learning usually have worse performance than those trained in the standard centralized learning mode, especially when the training data are not independent and identically distributed (Non-IID) on the local devices. In this survey, we provide a detailed analysis of the influence of Non-IID data on both parametric and non-parametric machine learning models in both horizontal and vertical federated learning. In addition, current research work on handling challenges of Non-IID data in federated learning are reviewed, and both advantages and disadvantages of these approaches are discussed. Finally, we suggest several future research directions before concluding the paper.},
  keywords = {Federated learning,Machine learning,Non-IID data,Privacy preservation}
}

@phdthesis{ziminLearningDependentData2018,
  type = {Thesis},
  title = {Learning from Dependent Data},
  author = {Zimin, Alexander},
  year = {2018},
  issn = {2663-337X},
  urldate = {2024-04-14},
  abstract = {The most common assumption made in statistical learning theory is the assumption of the independent and identically distributed (i.i.d.) data. While being very convenient mathematically, it is often very clearly violated in practice. This disparity between the machine learning theory and applications underlies a growing demand in the development of algorithms that learn from dependent data and theory that can provide generalization guarantees similar to the independent situations. This thesis is dedicated to two variants of dependencies that can arise in practice. One is a dependence on the level of samples in a single learning task. Another dependency type arises in the multi-task setting when the tasks are dependent on each other even though the data for them can be i.i.d. In both cases we model the data (samples or tasks) as stochastic processes and introduce new algorithms for both settings that take into account and exploit the resulting dependencies. We prove the theoretical guarantees on the performance of the introduced algorithms under different evaluation criteria and, in addition, we compliment the theoretical study by the empirical one, where we evaluate some of the algorithms on two real world datasets to highlight their practical applicability.},
  langid = {english},
  language = {eng},
  school = {Institute of Science and Technology Austria}
}

@inproceedings{ziyinWhatShapesLoss2022,
  title = {What Shapes the Loss Landscape of Self Supervised Learning?},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Ziyin, Liu and Lubana, Ekdeep Singh and Ueda, Masahito and Tanaka, Hidenori},
  year = {2022},
  month = sep,
  urldate = {2024-05-07},
  abstract = {Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.},
  langid = {english},
  language = {en}
}

@inproceedings{zophNeuralArchitectureSearch2022,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zoph, Barret and Le, Quoc},
  year = {2022},
  month = jul,
  urldate = {2024-04-20},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  langid = {english},
  language = {en}
}

@article{zouGeneralizationPerformanceERM2009,
  title = {The Generalization Performance of {{ERM}} Algorithm with~Strongly Mixing Observations},
  author = {Zou, Bin and Li, Luoqing and Xu, Zongben},
  year = {2009},
  month = jun,
  journal = {Machine Learning},
  volume = {75},
  number = {3},
  pages = {275--295},
  issn = {1573-0565},
  urldate = {2024-04-14},
  abstract = {The generalization performance is the main concern of machine learning theoretical research. The previous main bounds describing the generalization ability of the Empirical Risk Minimization (ERM) algorithm are based on independent and identically distributed (i.i.d.) samples. In order to study the generalization performance of the ERM algorithm with dependent observations, we first establish the exponential bound on the rate of relative uniform convergence of the ERM algorithm with exponentially strongly mixing observations, and then we obtain the generalization bounds and prove that the ERM algorithm with exponentially strongly mixing observations is consistent. The main results obtained in this paper not only extend the previously known results for i.i.d. observations to the case of exponentially strongly mixing observations, but also improve the previous results for strongly mixing samples. Because the ERM algorithm is usually very time-consuming and overfitting may happen when the complexity of the hypothesis space is high, as an application of our main results we also explore a new strategy to implement the ERM algorithm in high complexity hypothesis space.},
  langid = {english},
  language = {en},
  keywords = {ERM principle,Exponentially strongly mixing,Generalization performance,Relative uniform convergence}
}

@book{zouMetalearningTheoryAlgorithms2023,
  title = {Meta-Learning: Theory, Algorithms and Applications},
  shorttitle = {Meta-Learning},
  editor = {Zou, Lan},
  year = {2023},
  series = {The {{Elsevier}} and {{Miccai Society}} Book Series},
  publisher = {Academic Press, an imprint of Elsevier},
  address = {London ; San Diego},
  isbn = {978-0-323-89931-4},
  lccn = {Q335 .M48 2023},
  keywords = {Artificial intelligence,Neural networks (Computer science)},
  annotation = {OCLC: on1338682415}
}
